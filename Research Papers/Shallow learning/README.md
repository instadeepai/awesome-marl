This is a collection of older papers on the topic of learning in multi-agent systems. We sort papers by publication date and survey subtopic. Any additions to this repo are welcome.

### Convergent Learning

<details> <summary> <a href="http://www.dklevine.com/archive/refs4415.pdf"> Learning Mixed Equilibria </a> by Drew Fundenberg, David M. Kreps. Journal of Games and Economic Behvaior, 1993.  </summary> We study learning processes for finite strategic-form games, in which players use the history of past play to forecast play in the current period. In a generalization of fictitious play, we assume only that players asymptotically choose best responses to the historical frequencies of opponents′ past play. This implies that if the stage-game strategies converge, the limit is a Nash equilibrium. In the basic model, plays seems unlikely to converge to a mixed-strategy equilibrium, but such convergence is natural when the stage game is perturbed in the manner of Harsanyi′s purification theorem.  <br> - </details>

<details> <summary> <a href="http://www.eecs.harvard.edu/cs286r/courses/spring06/papers/kalailehrer93.pdf"> Rational learning leads to nash equilibrium </a>by Ehud Kalai, Ehud Lehrer. Econometrica, 1993. <a href="link">  </a> </summary> Each of n players, in an infinitely repeated game, starts with subjective beliefs about his opponents' strategies. If the individual beliefs are compatible with the true strategies chosen, then Bayesian updating will lead in the long run to accurate prediction of the future play of the game. It follows that individual players, who know their own payoff matrices and choose strategies to maximize their expected utility, must eventually play according to a Nash equilibrium of the repeated game. An immediate corollary is that, when playing a Harsanyi-Nash equilibrium of a repeated game of incomplete information about opponents' payoff matrices, players will eventually play a Nash equilibrium of the real game, as if they had complete information <br> - </details>

<details> <summary> <a href="https://cs.brown.edu/~mlittman/papers/ml96-generalized.pdf"> A generalized reinforcement-learning model: Convergence and applications </a>by Michael L. Littman, C. Szepesvari. ICML, 1996. <a href="link">  </a> </summary> Reinforcement learning is the process by which an autonomous agent uses its experience interacting with an environment to improve its behavior. The Markov decision process (MDP) model is a popular way of formalizing the reinforcement learning problem but it is by no means the only way. In this paper we show how many of the important theoretical results concerning reinforcement learning in MDPs extend to a generalized MDP model that includes MDPs two-player games and MDPs under a worst-case optimality criterion as special cases. The basis of this extension is a stochastic approximation theorem that reduces asynchronous convergence to synchronous con <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf"> The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems </a> by Caroline Claus, Craig Boutilier. AAAI, 1998.  </summary> Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.  <br> - </details>

<details> <summary> <a href="https://econweb.ucsd.edu/~jandreon/Econ264/papers/Erev%20Roth%20AER%201998.pdf"> Predicting how people play games: reinforcement leaning in experimental games with unique, mixed strategy equilibria </a> by Ido Erev, Alvin E. Roth. The American Economic Review, 1998. </summary> We examine learning in all experiments we could locate involving 100 periods or more of games with a unique equilibrium in mixed strategies, and in a new experiment. We study both the ex post ("best fit") descriptive power of learning models, and their ex ante predictive power, by simulating each experiment using parameters estimated from the other experiments. Even a one-parameter reinforcement learning model robustly outperforms the equilibrium predictions. Predictive power is improved by adding "forgetting" and "experimentation," or by allowing greater rationality as in probabilistic fictitious play. Implications for developing a low-rationality, cognitive game theory are discussed. <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~mmv/papers/01ijcai-mike.pdf"> Rational and Convergent Learning in Stochastic Games </a>by Michael Bowling, Manuela Veloso. 2001. </summary> This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we briefly overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm, WoLF policy hillclimbing, that is based on a simple principle: “learn quickly while losing, slowly while winning.” The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.  <br> - </details>

<details> <summary> <a href="https://dspace.mit.edu/bitstream/handle/1721.1/3688/CS023.pdf?sequence=2&isAllowed=y"> Playing is believing: the role of beliefs in multi-agent learning </a> by Yu-Han Chang, Leslie Pack Kaelbling. NeurIPS, 2001. </summary> We propose a new classification for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classification, we review the optimality of existing algorithms and discuss some insights that can be gained. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the long-run against fair opponents. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/3-540-44795-4_33.pdf"> Social agents playing a periodical policy </a>by Ann Now´e, Johan Parent, and Katja Verbeeck. ECML, 2001. <a href="link">  </a> </summary> Coordination is an important issue in multiagent systems. Within the stochastic game framework this problem translates to policy learning in a joint action space. This technique however suffers some important drawbacks like the assumption of the existence of a unique Nash equilibrium and synchronicity, the need for central control, the cost of communication, etc. Moreover in general sum games it is not always clear which policies should be learned. Playing pure Nash equilibria is often unfair to at least one of the players, while playing a mixed strategy doesn’t give any guarantee for coordination and usually results in a sub-optimal payoff for all agents. In this work we show the usefulness of periodical policies, which arise as a side effect of the fairness conditions used by the agents. We are interested in games which assume competition between the players, but where the overall performance can only be as good as the performance of the poorest player. Players are social distributed reinforcement learners, who have to learn to equalize their payoff. Our approach is illustrated on synchronous one-step games as well as on asynchronous job scheduling games. <br> - </details>

<details> <summary> <a href="https://papers.nips.cc/paper/2002/file/0d73a25092e5c1c9769a9f3255caa65a-Paper.pdf"> Efficient learning equilibrium </a>by Ronen I. Brafman, Moshe Tennenholtz. NeurIPS, 2002. <a href="link">  </a> </summary> We introduce efficient learning equilibrium (ELE), a normative approach to learning in noncooperative settings. In ELE, the learning algorithms themselves are required to be in equilibrium. In addition, the learning algorithms must arrive at a desired value after polynomial time, and a deviation from the prescribed ELE becomes irrational after polynomial time. We prove the existence of an ELE (where the desired value is the expected payoff in a Nash equilibrium) and of a Pareto-ELE (where the objective is the maximization of social surplus) in repeated games with perfect monitoring. We also show that an ELE does not always exist in the imperfect monitoring case. Finally, we discuss the extension of these results to general-sum stochastic games <br> - </details>

<br/>

### General-sum games

<details> <summary> <a href="https://webdocs.cs.ualberta.ca/~bowling/papers/00icml.pdf"> Convergence Problems of General-Sum Multiagent Reinforcement Learning </a>by Michael Bowling. ICML, 2000. </summary> Stochastic games are a generalization of MDPs to multiple agents, and can be used as a framework for investigating multiagent learning. Hu and Wellman (1998) recently proposed a multiagent Q-learning method for general-sum stochastic games. In addition to describing the algorithm, they provide a proof that the method will converge to a Nash equilibrium for the game under specified conditions. The convergence depends on a lemma stating that the iteration used by this method is a contraction mapping. Unfortunately the proof is incomplete. In this paper we present a counterexample and flaw to the lemma’s proof. We also introduce strengthened assumptions under which the lemma holds, and examine how this affects the classes of games to which the theoretical result can be applied  <br> - </details>

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/littman01a.pdf"> Friend-or-foe Q-learning in general-sum games </a>by Michael L. Littman. ICML, 2001. <a href="link">  </a> </summary> This paper describes an approach to reinforcement learning in multiagent general-sum games in which a learner is told to treat each other agent as either a friend" or foe". This Q-learning-style algorithm provides strong convergence guarantees compared to an existing Nash-equilibrium-based learning rule. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-02/SS02-02-012.pdf"> Correlated-Q Learning </a>by Amy Greenwald, Keith Hall, Roberto Serrano. NeurIPS workshop on multi-agent learning, 2002. <a href="link">  </a> </summary> Bowling named two desiderata for multiagent learning algorithms: rationality and convergence. This paper introduces correlated-Q learning, a natural generalization of Nash-Q and FF-Q that satisfies these criteria. Nash-Q satisfies rationality, but in general it does not converge. FF-Q satisfies convergence, but in general it is not rational. Correlated-Q satisfies rationality by construction. This papers demonstrates the empirical convergence of correlated-Q on a standard testbed of general-sum Markov games. <br> - </details>

<details> <summary> <a href="https://www.jmlr.org/papers/volume4/hu03a/hu03a.pdf"> Nash Q-Learning for General-Sum Stochastic Games </a>by Junling Hu, Michael Wellman. JMLR, 2003. <a href="link">  </a> </summary> We extend Q-learning to a noncooperative multiagent context, using the framework of general-sum stochastic games. A learning agent maintains Q-functions over joint actions, and performs updates based on assuming Nash equilibrium behavior over the current Q-values. This learning protocol provably converges given certain restrictions on the stage games (defined by Q-values) that arise during learning. Experiments with a pair of two-player grid games suggest that such restrictions on the game structure are not necessarily required. Stage games encountered during learning in both grid environments violate the conditions. However, learning consistently converges in the first grid game, which has a unique equilibrium Q-function, but sometimes fails to converge in the second, which has three different equilibrium Q-functions. In a comparison of offline learning performance in both games, we find agents are more likely to reach a joint optimal path with Nash Q-learning than with a single-agent Q-learning method. When at least one agent adopts Nash Q-learning, the performance of both agents is better than using single-agent Q-learning. We have also implemented an online version of Nash Q-learning that balances exploration with exploitation, yielding improved performance. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Sandip-Sen-2/publication/2831062_Towards_a_Pareto-optimal_solution_in_general-sum_games/links/54dbfb250cf28d3de65e2dc8/Towards-a-Pareto-optimal-solution-in-general-sum-games.pdf"> Towards a Pareto Optimal Solution in general-sum games </a>by Sandip Sen, Stephane Airiau, Rajatish Mukherjee. AAMAS, 2003. <a href="link">  </a> </summary> Multiagent learning literature has investigated iterated two-player games to develop mechanisms that allow agents to learn to converge on Nash Equilibrium strategy profiles. Such equilibrium configuration implies that there is no motivation for one player to change its strategy if the other does not. Often, in general sum games, a higher payoff can be obtained by both players if one chooses not to respond optimally to the other player. By developing mutual trust, agents can avoid iterated best responses that will lead to a lesser payoff Nash Equilibrium. In this paper we work with agents who select actions based on expected utility calculations that incorporates the observed frequencies of the actions of the opponent(s). We augment this stochastically-greedy agents with an interesting action revelation strategy that involves strategic revealing of one's action to avoid worst-case, pessimistic moves. We argue that in certain situations, such apparently risky revealing can indeed produce better payoff than a non-revealing approach. In particular, it is possible to obtain Pareto-optimal solutions that dominate Nash Equilibrium. We present results over a large number of randomly generated payoff matrices of varying sizes and compare the payoffs of strategically revealing learners to payoffs at Nash equilibrium. <br> - </details>

<br/>

### Repeated games

<details> <summary> <a href="http://www-stat.wharton.upenn.edu/~steele/Resources/Projects/SequenceProject/Hannan.pdf"> Approximation to bayes risk in repeated plays </a>by James Hannan. Contributions to the Theory of Games, 1959. <a href="link">  </a> </summary> This paper is concerned with the development of a dynamic theoryof decision under uncertainty. The results obtained are directly applicableto the development of a dynamic theory of games in which at least one play­er is, at each stage, fully informed on the joint empirical distribution ofthe past choices of strategies of the rest. Since the decision problem canbe Imbedded in a sufficiently unspecified game theoretic model, the paperis written in the language and notation of the general two person game, in which, however, player  I’s motivation is completely unspecified. <br> - </details>

<details> <summary> <a href="https://scholars.huji.ac.il/sites/default/files/abrahamn/files/bounded.pdf"> Bounded complexity justifies cooperation in finitely repeated prisoner’s dilemma </a>by Abraham Neyman. Economic Letters, 1985. <a href="link">  </a> </summary> Cooperation in the finitely repeated prisoner's dilemma is justified, without departure from strict utility maximization or complete information, but under the assumption that there are bounds (possibly very large) to the complexity of the strategies that the players may use. <br> - </details>

<details> <summary> <a href="http://www.econ.ucla.edu/workingpapers/wp735.pdf"> Noncomputable strategies and discounted repeated games </a>by John H. Nachbar, William R. Zame. Economic Theory, 1996. <a href="link">  </a> </summary> A number of authors have used formal models of computation to capture the idea of “bounded rationality” in repeated games. Most of this literature has used computability by a finite automaton as the standard. A conceptual difficulty with this standard is that the decision problem is not “closed.” That is, for every strategy implementable by an automaton, there is some best response implementable by an automaton, but there may not exist any algorithm forfinding such a best response that can be implemented by an automaton. However, such algorithms can always be implemented by a Turing machine, the most powerful formal model of computation. In this paper, we investigate whether the decision problem can be closed by adopting Turing machines as the standard of computability. The answer we offer is negative. Indeed, for a large class of discounted repeated games (including the repeated Prisoner's Dilemma) there exist strategies implementable by a Turing machine for whichno best response is implementable by a Turing machine. <br> - </details>

<details> <summary> <a href="https://www.cs.utexas.edu/users/pstone/Papers/bib2html-links/threats-ATAL2001.pdf"> Implicit negotiation in repeated games </a>by Peter Stone and Michael L. Littman. ATAL, 2001. <a href="link">  </a> </summary> In business-related interactions such as the on-going high-stakes FCC spectrum auctions, explicit communication among participants is regarded as collusion, and is therefore illegal. In this paper, we consider the possibility of autonomous agents engaging in implicit negotiation via their tacit interactions. In repeated general-sum games, our testbed for studying this type of interaction, an agent using a ``best response'' strategy maximizes its own payoff assuming its behavior has no effect on its opponent. This notion of best response requires some degree of learning to determine the fixed opponent behavior. Against an unchanging opponent, the best-response agent performs optimally, and can be thought of as a ``follower, '' since it adapts to its opponent. However, pairing two best-response agents in a repeated game can result in suboptimal behavior. We demonstrate this suboptimality in several different games using variants of Q-learning as an example of a best-response strategy. We then examine two ``leader'' strategies that induce better performance from opponent followers via stubbornness and threats. These tactics are forms of implicit negotiation in that they aim to achieve a mutually beneficial outcome without using explicit communication outside of the game. <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~mmv/papers/01ijcai-mike.pdf"> Sophisticated EWA Learning and Strategic Teaching in Repeated Games </a>by Colin F. Camerer, Teck-Hua Ho, Juin-Kuan Chong. Journal of Economic Theory, 2002. <a href="https://www.Summary.so/instadeep/Multiagent-Learning-Basics-Challenges-and-Prospect-21cb7b4294b84a4188cafd184a3deed8">   </a> </summary> Most learning models assume players are adaptive (i.e., they respond only to their own previous experience and ignore others' payo® information) and behavior is not sensitive to the way in which players are matched. Empirical evidence suggests otherwise. In this paper, we extend our adaptive experienceweighted attraction (EWA) learning model to capture sophisticated learning and strategic teaching in repeated games. The generalized model assumes there is a mixture of adaptive learners and sophisticated players. An adaptive learner adjusts his behavior the EWA way. A sophisticated player rationally best-responds to her forecasts of all other behaviors. A sophisticated player can be either myopic or farsighted. A farsighted player develops multiple-period rather than single-period forecasts of others' behaviors and chooses to `teach' the other players by choosing a strategy scenario that gives her the highest discounted net present value. We estimate the model using data from p-beauty contests and repeated trust games with incomplete information. The generalized model is better than the adaptive EWA model in describing and predicting behavior. Including teaching also allows an empirical learning-based approach to reputation formation which predicts better than a quantal-response extension of the standard typebased approach. <br> - </details>

<br/>

### Opponent modelling

<details> <summary> <a href="http://strategicreasoning.org/wp-content/uploads/2010/03/csr01.pdf"> Learning about other agents in a dynamic multiagent system </a>by Junling Hu, Michael Wellman. Journal of Cognitive Systems Research, 2001. <a href="link">  </a> </summary> We analyze the problem of learning about other agents in a class of dynamic multiagent systems, where performance of the primary agent depends on behavior of the others. We consider an online version of the problem, where agents must learn models of the others in the course of continual interactions. Various levels of recursive models are implemented in a simulated double auction market. Our experiments show learning agents on average outperform non-learning agents who do not use information about others. Among learning agents, those with minimum recursion assumption generally perform better than the agents with more complicated, though often wrong assumptions. <br> - </details>

<br/>

### Decision theory

<details> <summary> <a href="https://www.ijcai.org/Proceedings/91-1/Papers/011.pdf"> A decision-theoretic approach to coordinating multiagent interactions </a>by Piotr J. Gmytrasiewicz, Edmund H. Durfee, David K. Weh. IJCAI, 1991. <a href="https://www.Summary.so/instadeep/Multiagent-Learning-Basics-Challenges-and-Prospect-21cb7b4294b84a4188cafd184a3deed8">  </a> </summary> We describe a decision-theoretic method that an autonomous agent can use to model multiagent situations and behave rationally based on its model. Our approach, which we call the Recursive Modeling Method, explicitly accounts for the recursive nature of multiagent reasoning. Our method lets an agent recursively model another agent's decisions based on probabilistic views of how that agent perceives the multiagent situation, which in turn are derived from hypothesizing how that other agent perceives the initial agent's possible decisions, and so on. Further, we show how the possibility of multiple interactions can affect the decisions of agents, allowing cooperative behavior to emerge as a rational choice of selfish agents that otherwise might behave uncooperatively <br> - </details>

<br/>

### Extensive form games

<details> <summary> <a href="https://www.tau.ac.il/~samet/papers/learning-to-play.pdf">  Learning to play games in extensive form by valuation </a>by Phillipe Jehiel, Dov Samet. NAJ Economics, 2001. <a href="link">  </a> </summary> Game theoretic models of learning which are based on the strategic form of the game cannot explain learning in games with large extensive form. We study learning in such games by using valuation of moves. A valuation for a player is a numeric assessment of her moves that purports to reflect their desirability. We consider a myopic player, who chooses moves with the highest valuation. Each time the game is played, the player revises her valuation by assigning the payoff obtained in the play to each of the moves she has made. We show for a repeated win–lose game that if the player has a winning strategy in the stage game, there is almost surely a time after which she always wins. When a player has more than two payoffs, a more elaborate learning procedure is required. We consider one that associates with each move the average payoff in the rounds in which this move was made. When all players adopt this learning procedure, with some perturbations, then, with probability 1 there is a time after which strategies that are close to subgame perfect equilibrium are played. A single player who adopts this procedure can guarantee only her individually rational payoff <br> - </details>

<br/>

### Theoretical frameworks for MARL

<details> <summary> <a href="https://courses.cs.duke.edu/spring07/cps296.3/littman94markov.pdf"> Markov games as a framework for multiagent reinforcement learning  </a>by Michael L. Littman. ICML, 1994. <a href="link">  </a> </summary> In the Markov decision process(MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic. <br> - </details>

<details> <summary> <a href="https://www.lirmm.fr/~jq/Cours/3cycle/module/HuWellman98icml.pdf"> Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm </a>by Junling Hu, Michael P. Wellman. ICML, 1998. <a href="link">  </a> </summary> In this paper, we adopt general-sum stochastic games as a framework for multiagent reinforcement learning. Our work extends previous work by Littman on zero-sum stochastic games to a broader framework. We design a multiagent Q-learning method under this framework, and prove that it converges to a Nash equilibrium under specified conditions. This algorithm is useful for finding the optimal strategy when there exists a unique Nash equilibrium in the game. When there exist multiple Nash equilibria in the game, this algorithm should be combined with other learning techniques to find optimal strategies. <br> - </details>

<details> <summary> <a href="https://www.semanticscholar.org/paper/Multiagent-Reinforcement-Learning-in-Stochastic-Hu-Wellman/7ce14dbb9add4d9656746703babd00d8f765b22a"> Multiagent reinforcement learning in stochastic games </a>by Hu, J., Wellman, M.P. Cambridge University Press, 1999. <a href="link">  </a> </summary> We adopt stochastic games as a general framework for dynamic noncooperative systems. This framework provides a way of describing the dynamic interactions of agents in terms of individuals' Markov decision processes. By studying this framework, we go beyond the common practice in the study of learning in games, which primarily focus on repeated games or extensive-form games. For stochastic games with incomplete information, we design a multiagent reinforcement learning method which allows agents to learn Nash equilibrium strategies. We show in both theory and experiments that this algorithm converges. From the viewpoint of machine learning research, our work helps to establish the theoretical foundation for applying reinforcement learning, originally deened for single-agent systems, to multiagent systems. <br> - </details>

<br/>

### Incomplete information games

<details> <summary> <a href="http://www.ma.huji.ac.il/~zamir/papers/22_IJGT85.pdf"> Formulation of bayesian analysis for games with incomplete information </a>by J-F. Mertens, S. Zamir. International Journal of Game Theory, 1985. <a href="link">  </a> </summary> A formal model is given of Harsanyi's infinite hierarchies of beliefs. It is shown that the model closes with some Bayesian game with incomplete information, and that any such game can be approximated by one with a finite number of states of world. <br> - </details>

<br/>

### Bounded rationality

<details> <summary> <a href="https://dl.acm.org/doi/pdf/10.1145/195058.195445"> On complexity as bounded rationality  </a>by C.H. Papadimitriou, M. Yannakakis. STOC, 1994. <a href="link">  </a> </summary> It has been hoped that computational approaches can help resolve some well-known paradoxes in game theory. We prove that tf the repeated prisoner’s dilemma M played by finite automata with less than exponentially (in the number of rounds) many states, then cooperation can be achieved an equilibrium (while with exponentially many states, defection is the only equilibrium). We furthermore prove a generalization to arbitrary games and Pareto optimal points. Finally, we present a general model of polynomially computable games, and characterize in terms of fami!iar complexity classes ranging from NP to NEXP the natural problems that arise in relation with such games. <br> - </details>

<details> <summary> <a href="https://mitpress.mit.edu/9780262681001/modeling-bounded-rationality/"> Modeling Bounded Rationality </a>by Ariel Rubinstein. MIT Press,
1998. <a href="link">  </a> </summary> The notion of bounded rationality was initiated in the 1950s by Herbert Simon; only recently has it influenced mainstream economics. In this book, Ariel Rubinstein defines models of bounded rationality as those in which elements of the process of choice are explicitly embedded. The book focuses on the challenges of modeling bounded rationality, rather than on substantial economic implications. In the first part of the book, the author considers the modeling of choice. After discussing some psychological findings, he proceeds to the modeling of procedural rationality, knowledge, memory, the choice of what to know, and group decisions.In the second part, he discusses the fundamental difficulties of modeling bounded rationality in games. He begins with the modeling of a game with procedural rational players and then surveys repeated games with complexity considerations. He ends with a discussion of computability constraints in games. The final chapter includes a critique by Herbert Simon of the author's methodology and the author's response. <br> - </details>

<br/>

### Coordination

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1994/AAAI94-065.pdf"> Learning to coordinate without sharing information </a>by Sandip Sen, Mahendra Sekaran, John Hale. National Conference on Artificial Intelligence, 1994. <a href="link">  </a> </summary> Researchers in the field of Distributed Artificial Intelligence (DAI) have been developing efficient mechanisms to coordinate the activities of multiple autonomous agents. The need for coordination arises because agents have to share resources and expertise required to achieve their goals. Previous work in the area includes using sophisticated information exchange protocols, investigating heuristics for negotiation, and developing formal models of possibilities of conflict and cooperation among agent interests. In order to handle the changing requirements of continuous and dynamic environments, we propose learning as a means to provide additional possibilities for effective coordination. We use reinforcement learning techniques on a block pushing problem to show that agents can learn complimentary policies to follow a desired path without any knowledge about each other. We theoretically analyze and experimentally verify the effects of learning rate on system convergence, and demonstrate benefits of using learned coordination knowledge on similar problems. Reinforcement learning based coordination can be achieved in both cooperative and non-cooperative domains, and in domains with noisy communication channels and other stochastic characteristics that present a formidable challenge to using other coordination schemes. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/publication/226319923_A_Distributed_Approach_for_Coordination_of_Traffic_Signal_Agents"> A game-theoretic approach to coordination of traffic signal agents </a>by Bazzan A. L. C. PhD thesis, 1997. <a href="link">  </a> </summary> Innovative control strategies are needed to cope with the increasing urban traffic chaos. In most cases, the currently used strategies are based on a central traffic-responsive control system which can be demanding to implement and maintain. Therefore, a functional and spatial decentralization is desired. For this purpose, distributed artificial intelligence and multi-agent systems have come out with a series of techniques which allow coordination and cooperation. However, in many cases these are reached by means of communication and centrally controlled coordination processes, giving little room for decentralized management. Consequently, there is a lack of decision-support tools at managerial level (traffic control centers) capable of dealing with decentralized policies of control and actually profiting from them. In the present work a coordination concept is used, which overcomes some disadvantages of the existing methods. This concept makes use of techniques of evolutionary game theory: intersections in an arterial are modeled as individually-motivated agents or players taking part in a dynamic process in which not only their own local goals but also a global one has to be taken into account. The role of the traffic manager is facilitated since s/he has to deal only with tactical ones, leaving the operational issues to the agents. Thus the system ultimately provides support for the traffic manager to decide on traffic control policies. Some application in traffic scenarios are discussed in order to evaluate the feasibility of transferring the responsibility of traffic signal coordination to agents. The results show different performances of the decentralized coordination process in different scenarios (e.g. the flow of vehicles is nearly equal in both opposing directions, one direction has a clearly higher flow, etc.). Therefore, the task of the manager is facilitate once s/he recognizes the scenario and acts accordingly. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2002/AAAI02-050.pdf"> Reinforcement Learning of Coordination in Cooperative Multiagent Systems </a>by Kapetanakis S. and Kudenko D. AAAI, 2002. <a href="link">  </a> </summary> We report on an investigation of reinforcement learning techniques for the learning of coordination in cooperative multiagent systems. Specifically, we focus on a novel action selection strategy for Q-learning (Watkins 1989). The new technique is applicable to scenarios where mutual observation of actions is not possible. To date, reinforcement learning approaches for such independent agents did not guarantee convergence to the optimal joint action in scenarios with high miscoordination costs. We improve on previous results (Claus & Boutilier 1998) by demonstrating empirically that our extension causes the agents to converge almost always to the optimal joint action even in these difficult cases. <br> - </details>

details> <summary> <a href="https://core.ac.uk/download/pdf/54045748.pdf"> Coordination of independent learners in cooperative Markov games </a>by La¨etitia Matignon, Guillaume J. Laurent, Nadine Le Fort-Piat. Technical Report, 2009. <a href="link">  </a> </summary> In the framework of fully cooperative multi-agent systems, independent agents learning by reinforcement must overcome several difficulties as the coordination or the impact of exploration. The study of these issues allows first to synthesize the characteristics of existing reinforcement learning decentralized methods for independent learners in cooperative Markov games. Then, given the difficulties encountered by these approaches, we focus on two main skills: optimistic agents, which manage the coordination in deterministic environments, and the detection of the stochasticity of a game. Indeed, the key difficulty in stochastic environment is to distinguish between various causes of noise. The SOoN algorithm is so introduced, standing for “Swing between Optimistic or Neutral”, in which independent learners can adapt automatically to the environment stochasticity. Empirical results on various cooperative Markov games notably show that SOoN overcomes the main factors of non-coordination and is robust face to the exploration of other agents <br> - </details>

<details> <summary> <a href="https://link.springer.com/chapter/10.1007/3-540-36187-1_36"> Learning to Reach the Pareto Optimal Nash Equilibrium as a Team </a>by Katja Verbeeck, Ann Nowe, Tom Lenaerts, Johan Parent.  LNAI, 2002. <a href="link">  </a> </summary> Coordination is an important issue in multi-agent systems when agents want to maximize their revenue. Often coordination is achieved through communication, however communication has its price. We are interested in finding an approach where the communication between the agents is kept low, and a global optimal behavior can still be found. In this paper we report on an efficient approach that allows independent reinforcement learning agents to reach a Pareto optimal Nash equilibrium with limited communication. The communication happens at regular time steps and is basicallya signal for the agents to start an exploration phase. During each exploration phase, some agents exclude their current best action so as to give the team the opportunityto look for a possiblyb etter Nash equilibrium. This technique of reducing the action space byexclusions was onlyrecen tlyin troduced for finding periodical policies in games of conflicting interests. Here, we explore this technique in repeated common interest games with deterministic or stochastic outcomes. <br> - </details>

<br/>

### Evolutionary game theory

<details> <summary> <a href="https://www.econstor.eu/bitstream/10419/94705/1/wp407.pdf"> Nash Equilibrium and Evolution by Imitation </a>by Bjornerstedt J., and Weibull, J. The Rational Foundations of Economic Behavior, 1995. <a href="link">  </a> </summary> Nash's "mass action" interpretation of his equilibrium concept does not presume that the players know the game or are capable of sophisticated calculations. Instead, players are repeatedly and randomly drawn from large populations to play the game, one population for each player position, and base their strategy choice on observed payoffs. The present paper examines in some detail such an interpretation in a dass of population dynamics based on adaptation by way of imitation of successful behaviors. Drawing from results in evolutionary game theory, implications of dynamic stability for aggregate Nash equilibrium play are discussed.  <br> - </details>

<details> <summary> <a href="https://core.ac.uk/download/pdf/6518937.pdf"> Learning through Reinforcement and Replicator Dynamics </a>by Borgers, T., Sarin, R. Journal of Economic Theory, 1997. <a href="link">  </a> </summary> This paper considers a version of R. R. Bush and F. Mosteller's stochastic learning theory in the context of games. We show that in a continuous time limit the learning model converges to the replicator dynamics of evolutionary game theory. Thus we provide a non-biological interpretation of evolutionary game theory. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-540-30115-8_18.pdf"> Analyzing Multi-agent Reinforcement Learning Using Evolutionary Dynamics </a>by ’t Hoen, P.J., Tuyls, K. ECML, 2004. <a href="link">  </a> </summary> In this paper, we show how the dynamics of Q-learning can be visualized and analyzed from a perspective of Evolutionary Dynamics (ED). More specifically, we show how ED can be used as a model for Qlearning in stochastic games. Analysis of the evolutionary stable strategies and attractors of the derived ED from the Reinforcement Learning (RL) application then predict the desired parameters for RL in MultiAgent Systems (MASs) to achieve Nash equilibriums with high utility. Secondly, we show how the derived fine tuning of parameter settings from the ED can support application of the COllective INtelligence (COIN) framework. COIN is a proved engineering approach for learning of cooperative tasks in MASs. We show that the derived link between ED and RL predicts performance of the COIN framework and visualizes the incentives provided in COIN toward cooperative behavior. <br> - </details>

<details> <summary> <a href="http://www.fulviofrisone.com/attachments/article/412/Hofbauer%20Evolutionary%20Games%20and%20Population%20Dynamics.pdf"> Evolutionary Games and Population Dynamics </a>by Hofbauer, J., Sigmund, K. Cambridge University Press, 1998. <a href="link">  </a> </summary> Every form of behaviour is shaped by trial and error. Such stepwise adaptation can occur through individual learning or through natural selection, the basis of evolution. Since the work of Maynard Smith and others, it has been realised how game theory can model this process. Evolutionary game theory replaces the static solutions of classical game theory by a dynamical approach centred not on the concept of rational players but on the population dynamics of behavioural programmes. In this book the authors investigate the nonlinear dynamics of the self-regulation of social and economic behaviour, and of the closely related interactions between species in ecological communities. Replicator equations describe how successful strategies spread and thereby create new conditions which can alter the basis of their success, i.e. to enable us to understand the strategic and genetic foundations of the endless chronicle of invasions and extinctions which punctuate evolution. In short, evolutionary game theory describes when to escalate a conflict, how to elicit cooperation, why to expect a balance of the sexes, and how to understand natural selection in mathematical terms. <br> - </details>

<details> <summary> <a href="https://www.reed.edu/biology/courses/BIO342/2012_syllabus/2012_readings/smith_1976_games.pdf"> Evolution and the Theory of Games </a>by J. Maynard Smith. American Scientist, 1976. <a href="link">  </a> </summary> n situations characterized by conflict of interest, the best strategy to adopt depends on what others are doing. <br> - </details>

<details> <summary> <a href="http://etherplan.com/the-logic-of-animal-conflict.pdf"> The logic of animal conflict </a>by Maynard Smith, J., Price, G.R. Nature, 1973. <a href="link">  </a> </summary> Conflicts between animals of the same species usually are of “limited war” type, not causing serious injury. This is often explained as due to group or species selection for behaviour benefiting the species rather than individuals. Game theory and computer simulation analyses show, however, that a “limited war” strategy benefits individual animals as well as the species. <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/30801544/Tuybnaic02-with-cover-page-v2.pdf?Expires=1668008453&Signature=D2CzKtWwSHhSIlxHOl7NcfNf3Be7IomezobJzZPLDBT2~3nA28zKf7xPQkQq13XFBfGgEIhx3IvzL9OHu4abVVSf9TxdFZwCaNg7JODf81a8~bBg2y9CITtTYBtmpw8gxQw9mXc4dpHBEc9dKwjLi18zC47x2e9gr4ZX3uYeRu6JflBxR6FmqwvlNzR4VxPvTv0DwgKdnALkVedwDLaGUlE7iQEd5VQgNhy8ZF-76bZ8qhGWv4FNdrFY5bjVAbJ2nz4vYcM2AAc6qNE~if9VjBARd1hkg0-3U7WLUDk2UnRzBz2rn9Z7ra75pN2MQB0VtpXAQHuh8gG5Od~MBOIfuA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> Towards a relation between learning agents and evolutionary dynamics </a>by Karl Tuyls, Tom Lenaerts, Katja Verbeeck, Sam Maes. BNAIC, 2002. <a href="link">  </a> </summary> Modeling learning agents in the context of Multi-agent Systems requires insight in the type and form of interactions with the environment and other agents in the system. Usually, these agents are modeled similar to the different players in a standard game theoretical model. In this paper we examine whether evolutionary game theory, and more specifically the replicator dynamics, is an adequate theoretical model for the study of the dynamics of reinforcement learning agents in a multi-agent system. As a first step in this direction we extend the results of [1, 9] to a more general reinforcement learning framework, i.e. Learning Automata. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Karl-Tuyls/publication/221471451_On_a_Dynamical_Analysis_of_Reinforcement_Learning_in_Games_Emergence_of_Occam%27s_Razor/links/0c9605203e5ba30157000000/On-a-Dynamical-Analysis-of-Reinforcement-Learning-in-Games-Emergence-of-Occams-Razor.pdf"> On a Dynamical Analysis of Reinforcement Learning in Games: Emergence of Occam’s Razor </a>by Karl Tuyls, Katja Verbeeck, Sam Maes. Lecture Notes in Computer Science, 2003. <a href="link">  </a> </summary> Modeling learning agents in the context of Multi-agent Systems requires an adequate understanding of their dynamic behaviour. Usually, these agents are modeled similar to the different players in a standard game theoretical model. Unfortunately traditional Game Theory is static and limited in its usefelness. Evolutionary Game Theory improves on this by providing a dynamics which describes how strategies evolve over time. In this paper, we discuss three learning models whose dynamics are related to the Replicator Dynamics(RD). We show how a classical Reinforcement Learning(RL) technique, i.e. Qlearning relates to the RD. This allows to better understand the learning process and it allows to determine how complex a RL model should be. More precisely, Occam’s Razor applies in the framework of games, i.e. the simplest model (Cross) suffices for learning equilibria. An experimental verification in all three models is presented. <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/30888627/p693-with-cover-page-v2.pdf?Expires=1668008714&Signature=c6qzlgxHi0g~03AfAJcY0D1i7Pi0TbjmgXXrMbaE-mYV52qJJLFcEMYIjkqj59wHIJyC69sEI6dMenm8neQP9IORV-tpCNigyRWlRS5b8WL4gIFLqBodbIlr10KlLaY6~zNRHY-shOzVixDraeDdIT3qCqh-Kjb~S3uSnoIRRgKSV8p5XzeW1SAnJEcXRnM1ZZY6VTWiVejZoH02f-g9Tx1LiDmvp8XI2FJK0FuMrR-iFpcFcafc44q8bSu9HxhPBW3OPll4~vT4H3U~dMyv2h-lo-vp6gmuoQLPpym~Gc1lqBQsQb8ngo0ZDEdESlUz-ayTFD8CS6cgWm17HHO~IQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> A selection-mutation model for Q-learning in multi-agent systems </a>by Karl Tuyls, Katja Verbeeck, Tom Lenaerts. AAMAS, 2003. <a href="link">  </a> </summary> Although well understood in the single-agent framework, the use of traditional reinforcement learning (RL) algorithms in multi-agent systems (MAS) is not always justified. The feedback an agent experiences in a MAS, is usually influenced by the other agents present in the system. Multi agent environments are therefore non-stationary and convergence and optimality guarantees of RL algorithms are lost. To better understand the dynamics of traditional RL algorithms we analyze the learning process in terms of evolutionary dynamics. More specifically we show how the Replicator Dynamics (RD) can be used as a model for Q-learning in games. The dynamical equations of Q-learning are derived and illustrated by some well chosen experiments. Both reveal an interesting connection between the exploitationexploration scheme from RL and the selection-mutation mechanisms from evolutionary game theory. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-540-39857-8_38.pdf"> Extended Replicator Dynamics as a Key to Reinforcement Learning in Multi-agent Systems </a>by Karl Tuyls, Dries Heytens, Ann Nowe, Bernard Manderick.  ECML, 2003. <a href="link">  </a> </summary> Modeling learning agents in the context of Multi-agent Systems requires an adequate understanding of their dynamic behaviour. Evolutionary Game Theory provides a dynamics which describes how strategies evolve over time. B¨orgers et al. [1] and Tuyls et al. [11] have shown how classical Reinforcement Learning (RL) techniques such as Cross-learning and Q-learning relate to the Replicator Dynamics (RD). This provides a better understanding of the learning process. In this paper, we introduce an extension of the Replicator Dynamics from Evolutionary Game Theory. Based on this new dynamics, a Reinforcement Learning algorithm is developed that attains a stable Nash equilibrium for all types of games. Such an algorithm is lacking for the moment. This kind of dynamics opens an interesting perspective for introducing new Reinforcement Learning algorithms in multi-state games and MultiAgent Systems. <br> - </details>

<details> <summary> <a href="https://www.econstor.eu/bitstream/10419/95028/1/wp347.pdf"> An Introduction to Evolutionary Game Theory </a>by Weibull, Jörgen W. IFN, 1992. <a href="link">  </a> </summary> Please note that these lecture notes are incomplete and may contain typos and errors. I hope to have a more full-fledged version in a few months, and appreciate in the meantime comments and suggestions.  <br> - </details>

<details> <summary> <a href="https://www.econstor.eu/bitstream/10419/94851/1/wp487.pdf"> What have we learned from Evolutionary Game Theory so far? </a>by Weibull, Jörgen W. IFN, 1997. <a href="link">  </a> </summary> Evolutionary theorizing has a long tradition in economics. Only recently has this approach been brought into the framework of noncooperative game theory. Evolutionary game theory studies the robustness of strategic behavior with respect to evolutionary forces in the context of games played many times in large populations of boundedly rational agents. This new strand in economic theory has lead to new predictions and opened up doors to other social sciences. The discussion will be focused on the following questions: What distinguishes the evolutionary approach from the rationalistic? What are the most important ndings in evolutionary game theory so far? What are the next challenges for evolutionary game theory in economics? <br> - </details>

<br/>

### Dispersion games

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2002/AAAI02-061.pdf"> Dispersion games: general definitions and some specific learning results </a>by T. Grenager, R. Powers, Y. Shoham. AAAI, 2002. <a href="link">  </a> </summary> Dispersion games are the generalization of the anticoordination game to arbitrary numbers of agents and actions. In these games agents prefer outcomes in which the agents are maximally dispersed over the set of possible actions. This class of games models a large number of natural problems, including load balancing in computer science, niche selection in economics, and division of roles within a team in robotics. Our work consists of two main contributions. First, we formally define and characterize some interesting classes of dispersion games. Second, we present several learning strategies that agents can use in these games, including traditional learning rules from game theory and artificial intelligence, as well as some special purpose strategies. We then evaluate analytically and empirically the performance of each of these strategies. <br> - </details>

<br/>

### No-regret learning

<details> <summary> <a href="http://static.cs.brown.edu/people/amy/papers/icml.pdf"> On no-regret learning, fictitious play, and nash equilibrium </a>by Jafari, C., Greenwald, A., Gondek, D., Ercal, G. ICML, 2001. <a href="link">  </a> </summary> This paper addresses the question what is the outcome of multi-agent learning via no-regret algorithms in repeated games? Specifically, can the outcome of no-regret learning be characterized by traditional game-theoretic solution concepts, such as Nash equilibrium? The conclusion of this study is that no-regret learning is reminiscent of fictitious play: play converges to Nash equilibrium in dominancesolvable, constant-sum, and generalsum 2  2 games, but cycles exponentially in the Shapley game. Notably, however, the information required of fictitious play far exceeds that of noregret learning. <br> - </details>

<br/>

### Theses

<details> <summary> <a href="https://aaltodoc.aalto.fi/bitstream/handle/123456789/2486/isbn9512273594.pdf?sequence=1&isAllowed=y"> Multiagent Reinforcement Learning in Markov Games: Asymmetric and Symmetric approaches </a>by Ville Kononen. PhD dissertation, 2004. <a href="link">  </a> </summary> Modern computing systems are distributed, large, and heterogeneous. Computers, other information processing devices and humans are very tightly connected with each other and therefore it would be preferable to handle these entities more as agents than stand-alone systems. One of the goals of artificial intelligence is to understand interactions between entities, whether they are artificial or natural, and to suggest how to make good decisions while taking other decision makers into account. In this thesis, these interactions between intelligent and rational agents are modeled with Markov games and the emphasis is on adaptation and learning in multiagent systems. Markov games are a general mathematical tool for modeling interactions between multiple agents. The model is very general, for example common board games are special instances of Markov games, and particularly interesting because it forms an intersection of two distinct research disciplines: machine learning and game theory. Markov games extend Markov decision processes, a well-known tool for modeling single-agent problems, to multiagent domains. On the other hand, Markov games can be seen as a dynamic extension to strategic form games, which are standard models in traditional game theory. From the computer science perspective, Markov games provide a flexible and efficient way to describe different social interactions between intelligent agents. This thesis studies different aspects of learning in Markov games. From the machine learning perspective, the focus is on a very general learning model, i.e. reinforcement learning, in which the goal is to maximize the long-time performance of the learning agent. The thesis introduces an asymmetric learning model that is computationally efficient in multiagent systems and enables the construction of different agent hierarchies. In multiagent reinforcement learning systems based on Markov games, the space and computational requirements grow very quickly with the number of learning agents and the size of the problem instance. Therefore, it is necessary to use function approximators, such as neural networks, to model agents in many real-world applications. In this thesis, various numeric learning methods are proposed for multiagent learning problems. The proposed methods are tested with small but non-trivial example problems from different research areas including artificial robot navigation, simplified soccer game, and automated pricing models for intelligent agents. The thesis also contains an extensive literature survey on multiagent reinforcement learning and various methods based on Markov games. Additionally, game-theoretic methods and methods originated from computer science for multiagent learning and decision making are compared. <br> - </details>

<details> <summary> <a href="http://reports-archive.adm.cs.cmu.edu/anon/1998/CMU-CS-98-187.pdf"> Layered Learning in Multi-Agent Systems </a>by Peter Stone. PhD thesis, 1998. <a href="link">  </a> </summary> Multi-agent systems in complex, real-time domains require agents to act effectively both autonomously and as part of a team. This dissertation addresses multi-agent systems consisting of teams of autonomous agents acting in real-time, noisy, collaborative, and adversarial environments. Because of the inherent complexity of this type of multi-agent system, this thesis investigates the use of machine learning within multi-agent systems. The dissertation makes four main  contributions to the fields of Machine Learning and Multi-Agent Systems. First, the thesis defines a team member agent architecture within which a exible teamstructure is presented, allowing agents to decompose the task space into exible roles and allowing them to smoothly switch roles while acting. Team organization is achieved by the introduction of a locker-room agreement as a collection of conventions followed by all team members. It defines agent roles, team formations, and pre-compiled multi-agent plans. In addition, the team member agent architecture includes a communication paradigm for domains with single-channel, low-bandwidth, unreliable communication. The communication paradigm facilitates team coordination while being robust to lost messages and active interference from opponents. Second, the thesis introduces layered learning, a general-purpose machine learning paradigm for complex domains in which learning a mapping directly from agents' sensors to their actuators is intractable. Given a hierarchical task decomposition, layered learning allows for learning at each level of the hierarchy, with learning at each level directly affecting learning at the next higher level. Third, the thesis introduces a new multi-agent reinforcement learning algorithm, namely team-partitioned, opaque-transition reinforcement learning (TPOT-RL). TPOT-RL is designed for domains in which agents cannot necessarily observe the state changes when other team members act. It exploits local, action-dependent features to aggressively generalize its input representation for learning and partitions the task among the agents, allowing them to simultaneously learn collaborative policies by observing the long-term effects of their actions. Fourth, the thesis contributes a fully functioning multi-agent system that incorporates learning in a real-time, noisy domain with teammates and adversaries. Detailed algorithmic descriptions of the agents' behaviors as well as their source code are included in the thesis. Empirical results validate all four contributions within the simulated robotic soccer domain. The generality of the contributions is verified by applying them to the real robotic soccer, and network routing domains. Ultimately, this dissertation demonstrates that by learning portions of their cognitive processes, selectively communicating, and coordinating their behaviors via common knowledge, a group of independent agents can work towards a common goal in a complex, real-time, noisy, collaborative, and adversarial environment. <br> - </details>

<br/>

### Books

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/neumann44a.pdf"> Theory of Games and Economic Behaviour </a>by von Neumann, J., Morgenstern, O. Princeton University Press, 1944. <a href="link">  </a> </summary> This is the classic work upon which modern-day game theory is based. What began more than sixty years ago as a modest proposal that a mathematician and an economist write a short paper together blossomed, in 1944, when Princeton University Press published Theory of Games and Economic Behavior. In it, John von Neumann and Oskar Morgenstern conceived a groundbreaking mathematical theory of economic and social organization, based on a theory of games of strategy. Not only would this revolutionize economics, but the entirely new field of scientific inquiry it yielded--game theory--has since been widely used to analyze a host of real-world phenomena from arms races to optimal policy choices of presidential candidates, from vaccination policy to major league baseball salary negotiations. And it is today established throughout both the social sciences and a wide range of other sciences. <br> - </details>

<details> <summary> <a href="https://arielrubinstein.tau.ac.il/books/GT.pdf"> A Course in Game Theory </a>by Martin J. Osborne and Ariel Rubinstein.  MIT Press, 1994. <a href="link">  </a> </summary> A Course in Game Theory presents the main ideas of game theory at a level suitable for graduate students and advanced undergraduates, emphasizing the theory's foundations and interpretations of its basic concepts. The authors provide precise definitions and full proofs of results, sacrificing generalities and limiting the scope of the material in order to do so. The text is organized in four parts: strategic games, extensive games with perfect information, extensive games with imperfect information, and coalitional games. It includes over 100 exercises. <br> - </details>

### Biology inspired

<details> <summary> <a href="https://watermark.silverchair.com/282794.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsYwggLCBgkqhkiG9w0BBwagggKzMIICrwIBADCCAqgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMBRcVXHaCrhh9DPsJAgEQgIICecvIjI-AyHPqDEtoqxG45etk8Kr3qZnjlgYcF9HmV5Zgk3mbn6FJ-OzHZWLMlN-KdRwXGkIjDVCEOB8A-qFzdZz484BYJeQqM-d1usHshJkVd8nuz2mGVsBsS1PT9YPlw68yuUWF1UzLK41IyXZmZk4Pu58zMpQSken2DXuBZzC5R0btcbkHVhC_PuRR2Empi2m-xjW8dxWV5R3sbNiDnQQFLjm7BZZXTEE1qV1GvUCW0DdIDhcWJpa-LP6NZ1G0Evo4nlcVC4h1ZNzgizSe5oKTU3hJjeltMD35akait35q5EiPh6xMFqhykSJAAu-krMio05VNfHqAZ-n9vrW5g2W6z38eSBMCPmOx4VaLh9-vCyxHTIosjdbrsKISJ1F2t1AZitrUPniIOLr07aPZBVQHT1lxegrDKEQ-CzvB5Zg7cyVgyBRHzYKswHkCNij_3teAAB7Wi8DSmJWVKOovllkEuXayDtwEy8cktVZsgCGunE9Mz1wZKB0Pj6UGiccWtbGTS63nTNBgFh4ZntPAChV6x3olsXbzqcn0snIVvDtUzS_ziOyIGSD1WrHcZCQUFcWiZXPg059UQHKxunkIJCoZTqEhq5Aicg0MBOG7RPfBMIMv4GZtEPtNRuo3Ax1kFbdmlN2_FwnbSAtMYnh5an1Z3SjVeWkKCko2FBP6yigjm2vOIJBs0Q1s3sCNAFIoa7fSIH2jgmykzWsFQdRe-0ke7GHPs0UhzzDIYogT569IVE7FUeyY3lAHMFTcH3k8XlSCtnCu84J3nXoghqfHArjcDNZnTEg9mxu5W17_tnJzHhuU-viP7hzQu0gFaej7yaaCLRKkClTkuQ"> Evolution of biological information </a>by Thomas D. Schneider. Nucleic Acids Research, 2000. <a href="link">  </a> </summary> How do genetic systems gain information by evolutionary processes? Answering this question precisely requires a robust, quantitative measure of information. Fortunately, 50 years ago Claude Shannon defined information as a decrease in the uncertainty of a receiver. For molecular systems, uncertainty is closely related to entropy and hence has clear connections to the Second Law of Thermodynamics. These aspects of information theory have allowed the development of a straightforward and practical method of measuring information in genetic control systems. Here this method is used to observe information gain in the binding sites for an artificial ‘protein’ in a computer simulation of evolution. The simulation begins with zero information and, as in naturally occurring genetic systems, the information measured in the fully evolved binding sites is close to that needed to locate the sites in the genome. The transition is rapid, demonstrating that information gain can occur by punctuated equilibrium. <br> - </details>

<br/>

### Collective Intelligence

<details> <summary> <a href="https://proceedings.neurips.cc/paper/1998/file/5129a5ddcd0dcd755232baa04c231698-Paper.pdf"> USING COLLECTIVE INTELLIGENCE TO ROUTE INTERNET TRAFFIC </a>by David H. Wolpert, Kagan Turner, Jeremy Frank. NeurIPS, 1998. <a href="link">  </a> </summary> A COllective INtelligence (COIN) is a set of interacting reinforcement learning (RL) algorithms designed in an automated fashion so that their collective behavior optimizes a global utility function. We summarize the theory of COINs, then present experiments using that theory to design COINs to control internet traffic routing. These experiments indicate that COINs outperform all previously investigated RL-based, shortest path routing algorithms.  <br> - </details>

<details> <summary> <a href="link"> GENERAL PRINCIPLES OF LEARNING-BASED MULTI-AGENT SYSTEMS </a>by David H. Wolpert, Kevin R. Wheeler, Kagan Turner. International Conference on Autonomous Agents, 1999. <a href="link">  </a> </summary> We consider the problem of how to design large decentralized multi-agent systems (MAS’s) in an automated fashion, with little or no hand-tuning. Our approach has each agent run a reinforcement learning algorithm. This converts the problem into one of how to automatically set/update the reward functions for each of the agents so that the global goal is achieved. In particular we don not want the agents to “work at cross-purposes” as far as the global goal is concerned. We use the term artificial COllective INtelligence (COIN) to refer to systems that embody solutions to this problem. In this paper we present a summary of a mathematical framework for COINS. We then investigate the real-world applicability of the core concepts of that framework via two computer experiments: we show that our COINS perform near optimally in a difficult variant of Arthur’s bar problem [l] (and in psrtitular avoid the tragedy of the commons for that problem), and we also illustrate optimal performance for our COINS in the leader-follower problem.  <br> - </details>

<br/>

<!-- <details> <summary> <a href="link"> title </a>by authors. Conference, year. <a href="link">  </a> </summary> abstract <br> - </details> -->
