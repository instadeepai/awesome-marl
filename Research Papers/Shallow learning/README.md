This is a collection of older papers on the topic of learning in multi-agent systems. We sort papers by publication date and survey subtopic. Any additions to this repo are welcome.

### Auctions, Bidding and Negotiation

<details> <summary> <a href="http://59.90.80.165:8080/jspui/bitstream/123456789/64/1/downloadme%20%282%29.pdf"> Bargaining and Markets </a>by Martin J. Osborne, Ariel Rubinstein. Academic Press, 1990. <a href="link">  </a> </summary> The formal theory of bargaining originated with John Nash's work in the early 1950s. This book discusses two recent developments in this theory. The first uses the tool of extensive games to construct theories of bargaining in which time is modeled explicitly. The second applies the theory of bargaining to the study of decentralized markets. Rather than surveying the field, the authors present a select number of models, each of which illustrates a key point. In addition, they give detailed proofs throughout the book. It uses a small number of models, rather than a survey of the field, to illustrate key points, and includes detailed proofs given as explanations for the models. The text has been class-tested in a semester-long graduate course. <br> - </details>

<details> <summary> <a href="https://www.jair.org/index.php/jair/article/view/10106/23927"> A Market-Oriented Programming Environment and its Application to Distributed Multicommodity Flow Problems </a>by M. P. Wellman. JAIR, 1993. <a href="link">  </a> </summary> Market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem solving, we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy. WALRAS provides basic constructs for defining computational market structures, and protocols for deriving their corresponding price equilibria. In a particular realization of this approach for a form of multicommodity flow problem, we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation, and that the behavior of the system can be meaningfully analyzed in economic terms. <br> - </details>

<details> <summary> <a href="https://authors.library.caltech.edu/80874/1/2118073.pdf">  A Note on Sequential Auctions </a>by DAN BERNHARDT, DAVID SCOONES. American Economic Review, 1994. <a href="link">  </a> </summary> This note explores multiobject, sequential, private-value auctions. <br> - </details>

<details> <summary> <a href="http://www2.econ.iastate.edu/tesfatsi/binmore1.pdf"> Applying Game Theory to Automated Negotiation </a>by Ken Binmore and Nir Vulkan. Workshop on Economics, Game Theory and the Internet, 1997. <a href="link">  </a> </summary> With existing technology, it is already possible for personal agents to schedule meetings for their users, to write the small print of an agreement, and for agents to search the Internet for the cheapest price. But serious negotiation cranks the difficulty of the problem up several notches. In this paper, we review what game theory has to offer in the light of experience gained in programming automated agents within the ADEPT (Advance Decision Environment for Process Tasks) project, which is currently being used by British Telecom for some purposes <br> - </details>

<details> <summary> <a href="https://digital.csic.es/bitstream/10261/160955/1/RAS24(1998)_159-82.pdf"> Negotiation Decision Functions for Autonomous Agents </a>by Peyman Faratin, Carles Sierra, Nick R Jenning. Journal of Robotics and Autonomous Systems, 1998. <a href="link">  </a> </summary>  We present a formal model of negotiation between autonomous agents The purpose of the negotiation is to reach an agreement about the provision of a service by one agent for another The model defines a range of strategies and tactics that agents can employ to generate initial offers, evaluate proposals and offer counter proposals. The model is based on computationally tractable assumptions, demonstrated in the domain of business process management and empirically evaluate <br> - </details>

<details> <summary> <a href="http://www.cs.toronto.edu/kr/papers/auctions.pdf"> Sequential Auctions for the Allocation of Resources with Complementarities </a>by Craig Boutilier, Moises Goldszmidt, Bikash Sabata. IJCAI, 1999. <a href="link">  </a> </summary> Market-based mechanisms such as auctions are being studied as an appropriate means for resource allocation in distributed and multiagent decision problems. When agents value resources in combination rather than in isolation, one generally relies on combinatorial auctions where agents bid for resource bundles, orsimultaneous auctionsfor all resources. We develop a different model, where agents bid for required resourcessequentially. This model has the advantage that it can be applied in settings where combinatorial and simultaneous models are infeasible (e.g., when resources are made available at different points in time by different parties), as well as certain benefits in settings where combinatorial models are applicable. We develop a dynamic programming model for agents to compute bidding policies based on estimated distributions over prices. We also describe how these distributions are updated to provide a learning model for bidding behavior <br> - </details>

<details> <summary> <a href="https://dl.acm.org/doi/pdf/10.1145/336992.337000"> Automated Strategy Searches in an Electronic Goods Market: Learning and Complex Price Schedules </a>by Christopher H. Brooks, Scott Fay, Rajarshi Das, Jeffrey K. MacKie-Masons,
Jeffrey Kephartt, Edmund H. Durfee. ACM-EC, 1999. <a href="link">  </a> </summary> Markets for electronic goods provide the possibility of exploring new and more complex pricing schemes, due to the flexibility of information goods and negligible marginal cost. In this paper we compare dynamic performance across price schedules of varying complexity. We provide a monopolist producer with two machine learning methods which implement a strategy that balances exploitation to maximize current profits against exploration to improve future profits. We find that the complexity of the price schedule affects both the amount of exploration necessary and the aggregate profit received by a producer. In general, simpler price schedules are more robust and give up less profit during the learning periods even though the more complex schedules have higher long-run profits. These results hold for both learning methods, even though the relative performance of the methods is quite sensitive to differences in the smoothness of the profit landscape for different price schedules. Our results have implications for automated learning and strategic pricing in non-stationary environments, which arise when the consumer population changes, individuals change their preferences, or competing firms change their strategies. <br> - </details>

<details> <summary> <a href="https://ora.ox.ac.uk/objects/uuid:f776a391-7ac0-4489-9e12-8b7db59dfc4b/download_file?safe_filename=Survey.pdf&file_format=application%2Fpdf&type_of_work=Working+paper"> Auction theory: A guide to the literature </a>by Klemperer P. Journal of economic surveys, 1999. <a href="link">  </a> </summary> This paper provides an elementary, non-technical, survey of auction theory, by introducing and describing some of the critical papers in the subject. (The most important of these are reproduced in a companion book, The Economic Theory of Auctions, Paul Klemperer (ed.), Edward Elgar (pub.), forthcoming.) We begin with the most fundamental concepts, and then introduce the basic analysis of optimal auctions, the revenue equivalence theorem, and marginal revenues. Subsequent sections addrms risk-aversion, affiliation, asymmetries, entry, collusion, multi-unit auctions, double auctions, royalties, incentive contracts, and other topics. Appendices contain technical details, some simple worked examples, and bibliographies.  <br> - </details>

<details> <summary> <a href="https://hpi.de/fileadmin/user_upload/fachgebiete/plattner/teaching/Dynamic_Pricing/kep00.pdf"> Dynamic pricing by software agents </a>by Jeffrey O. Kephart, James E. Hanson, Amy R. Greenwald. Computer Networks, 2000. <a href="link">  </a> </summary> We envision a future in which the global economy and the Internet will merge, evolving into an information economy bustling with billions of economically motivated software agents that exchange information goods and services with humans and other agents. Economic software agents will differ in important ways from their human counterparts, and these dierences may have significant beneficial or harmful effects upon the global economy. It is therefore important to consider the economic incentives and behaviors of economic software agents, and to use every available means to anticipate their collective interactions. We survey research conducted by the Information Economies group at IBM Research aimed at understanding collective interactions among agents that dynamically price information goods or services. In particular, we study the potential impact of widespread shopbot usage on prices, the price dynamics that may ensue from various mixtures of automated pricing agents (or ``pricebots''), the potential use of machine-learning algorithms to improve profits, and more generally the interplay among learning, optimization, and dynamics in agentbased information economies. These studies illustrate both beneficial and harmful collective behaviors that can arise in such systems, suggest possible cures for some of the undesired phenomena, and raise fundamental theoretical issues, particularly in the realms of multi-agent learning and dynamic optimization. <br> - </details>

<details> <summary> <a href="https://deepblue.lib.umich.edu/bitstream/handle/2027.42/50440/DynamicBundling.pdf?sequence=1"> Pricing information bundles in a dynamic environment </a>by J. Kephart, C. Brooks, and R. Das. ACMEC, 2001. <a href="link">  </a> </summary> We explore a scenario in which a monopolist producer of information goods seeks to maximize its profits in a market where consumer demand shifts frequently and unpredictably. The producer may set an arbitrarily complex price schedule---a function that maps the set of purchased items to a price. However, lacking direct knowledge of consumer demand, it cannotcompute the optimal schedule. Instead, it attempts to optimize profits via trial and error. By means of a simple model of consumer demand and a modified version of a simple nonlinear optimization routine, we study a variety of parametrizations of the price schedule and quantify some of the relationships among learnability, complexity, and profitability. In particular, we show that fixed pricing or simple two-parameter dynamic pricing schedules are preferred when demand shifts frequently, but that dynamic pricing based on more complex schedules tends to be most profitable when demand shifts very infrequently. <br> - </details>

<details> <summary> <a href="https://www.cs.ox.ac.uk/people/michael.wooldridge/pubs/gdn2001.pdf"> Automated negotiation: prospects, methods, and challenges </a>by N. Jennings, P. Faratin, A. Lomuscio, S. Parsons, C. Sierra, and M. Wooldrigde. International Journal of Group Decision and Negotiation, 2001. <a href="link">  </a> </summary> This paper is to examine the space of negotiation opportunities for autonomous agents, to identify and evaluate some of the key techniques, and to highlight some of the major challenges for future automated negotiation research. This paper is not meant as a survy of the field of automated negotiation. Rather, the descriptions and assessments of the various approaches are generally undertaken with particular reference to work in which the authors have been involved. However, the specific issues raised should be viewed as being broadly applicable. <br> - </details>

<details> <summary> <a href="http://econ2.econ.iastate.edu/tesfatsi/ACEIntroSpecialIssue.JEDC2001.LT.pdf"> Introduction to the special issue on agent-based computational economics </a>by Leigh Tesfatsion. Journal of Economic Dynamics & Control, 2001. <a href="link">  </a> </summary> A brief overview of agent-based computational economics (ACE) is given, followed by a synopsis of the articles included in this special issue on ACE and in a companion special issue on ACE scheduled to appear in Computational Economics. <br> - </details>

<details> <summary> <a href="https://www.hpl.hp.com/techreports/2001/HPL-2001-107.pdf"> Economic Dynamics of Agents in Multiple Auctions </a>by Chris Preist, Andrew Byde, Claudio Bartolini. Autonomous Agents, 2001. <a href="link">  </a> </summary> Over the last few years, electronic auctions have become an increasingly important aspect of e-commerce, both in the business to business and business to consumer domains. As a result of this, it is often possible to find many auctions selling similar goods on the web. However, when an individual is attempting to purchase such a good, they will usually bid in one, or a small number, of such auctions. This results in two forms of inefficiency. Firstly, the individual may pay more for the good than would be expected in an ideal market. Secondly, some sellers may fail to make a sale that could take place in an ideal market. In this paper, we present an agent that is able to participate in multiple auctions for a given good, placing bids appropriately to secure the cheapest price. We present experiments to show; 1. Current auction markets on the web are inefficient, with trades taking place away from equilibrium price, and not all benefit from trade being extracted. 2. Our agent is able to exploit these inefficiencies, resulting in it making higher profits than the simple strategy of bidding in a small number of auctions. 3. As more participants use our agent, the market becomes more efficient. When all participants use the agent, all trades take place close to equilibrium price, and the market approaches ideal behaviour <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/cs/0205066.pdf"> Effectiveness of preference elicitation in combinatorial auctions </a>by Benoıt Hudson, Tuomas Sandholm. AMEC IV, 2002. <a href="link">  </a> </summary> Combinatorial auctions where agents can bid on bundles of items are desirable because they allow the agents to express complementarity and substitutability between the items. However, expressing one’s preferences can require bidding on all bundles. Selective incremental preference elicitation by the auctioneer was recently proposed to address this problem [4], but the idea was not evaluated. In this paper we show, experimentally and theoretically, that automated elicitation provides a drastic benefit. In all of the elicitation schemes under study, as the number of items for sale increases, the amount of information elicited is a vanishing fraction of the information collected in traditional “direct revelation mechanisms” where bidders reveal all their valuation information. Most of the elicitation schemes also maintain the benefit as the number of agents increases. We develop more effective elicitation policies for existing query types. We also present a new query type that takes the incremental nature of elicitation to a new level by allowing agents to give approximate answers that are refined only on an as-needed basis. In the process, we present methods for evaluating different types of elicitation policies. <br> - </details>

<details> <summary> <a href="https://epdf.tips/auction-theory1da4f2acfd65f4df0ff99f80a90282e777638.html"> Auction Theory </a>by Vijay Krishna. Academic press, 2002. <a href="link">  </a> </summary> Auction Theory, Second Edition improves upon his 2002 bestseller with a new chapter on package and position auctions as well as end-of-chapter questions and chapter notes. Complete proofs and new material about collusion complement Krishna’s ability to reveal the basic facts of each theory in a style that is clear, concise, and easy to follow. With the addition of a solutions manual and other teaching aids, the 2e continues to serve as the doorway to relevant theory for most students doing empirical work on auctions. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/256867/1/aamas02-byde.pdf"> Decision Procedures for Multiple Auctions </a>by Andrew Byde, Chris Preist, Nicholas R Jennings. AAMAS, 2002. <a href="link">  </a> </summary> This paper presents a decision theoretic framework that an autonomous agent can use to bid effectively across multiple, simultaneous auctions. Specifically, our framework enables an agent to make rational decisions about purchasing multiple goods from a series of auctions that operate different protocols (we deal with the English, Dutch, First-Price Sealed Bid and Vickrey cases). The framework is then used to characterize the optimal decision that an agent should take. Finally, we develop a practical algorithm that provides a heuristic approximation to this ideal <br> - </details>

<details> <summary> <a href="https://pdf.sciencedirectassets.com/271585/1-s2.0-S0004370200X00963/1-s2.0-S0004370202002904/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIGFP1%2Bs3XmZFMMz356aNwAmhMuGI9k1k7jf9Har%2FKjOTAiEAp31KCbp0yCHszItK5g5OpUYeeE4yIjBe9A%2FnJss4LyEq1QQIv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDAJhRmh%2FklRoMSMnWCqpBGypJb7gu7MTorfY6Hnc3oMb%2F2dFRfOlhT1Y3uEje99X%2FixlxlioaCFUcWucjZWBNb4U56DGXJwkJUQlUbm8JIv232PXklf%2B%2BYzpVtgB5jgBWizuCzEC1nGHT3jic%2BtWmsI7%2Fw6ZH8Zy4BaW34r4IlMxtIt2cBSPqk2%2B1DIEr3NRLOPV%2B0vZeDFgpU8gCgqQAFJp%2BfW5C%2FUodnBSWNkLV2nHGzL7PeXpBRpUojCqLXqC20BgFjb%2F9nsTfwxkDtrYWpzV8deMmWrBcuMJJWK2y5oBwvDHE9T0lup2DH%2BqWBkwcPXGLsyyM03tBgQjjoNw903SQagtCDDkRILKDgzUrj%2FYX4TjDRn3mSwUk9me5jMS8YB2yexIW6p5STYEMwqcJ4l7v%2BboFeS%2F1321ADrE77zx06hamEEP2uPtTKYORKGk0WyGi4cnh1JgDorV5wIMaHNYN5vBFxSVp9uEpIVABv7ZJpzj0%2FvL7k%2B47UHe68Mzjmsk1pgTuV0ccAPRy6nAlaxcR3PsgI8egXmZ4sEZ2o9xht3Yhw1P8YXmE7ZR1vDkSqWVst1mCrJ0WXq01pZSfz6%2BDYi3EcQPQr%2FEl%2FG7auzznum89Dx25zaiHw%2BhT8iJ3VmN1oAJ9KkjQbM7cseO4KpS0akFkniEDMDYfBt%2BQYNPJ07b2M6xEA98lyzwiiVCEuS%2FgXx%2Fw1UpSmAW1yCiFDPYtFloqE7LVU2gIuzZ6jxlOMYR3TYKMUgwtsf4mwY6qQHJRvMhFn1auKaTVI0w14zxYddvAr7RxGsDW1R9XvZBRm%2BYCJ%2Fhe5h82uWaGUJAwIqorYqpoE3Zke4VBJTiebdscWaNoKDWh9Na7Jhq0QEyaF9CqrkTyeYROqGe1OT2yOyoAG8%2FzOBWHnvQfFSKXfTVllBPvpTPdtlL%2Fu160kbIxjReqSMVNgkjmKdcgCVOT0guvZReygFelqRn%2FmWRSf8TUfCSGfClQ4aO&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221123T143152Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6ZQP7CUN%2F20221123%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=366f562e104e1ce07189adf5a3b637ef9d279201d7968aaea2b89f3746e10b68&hash=99f87d2a61c2835d7e9af934f74013b81ca0f23dafe0c8db633e045f298ac604&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0004370202002904&tid=spdf-ea80df82-46a9-4cd0-9633-76313cf3bffa&sid=a340001a289239432d6b3090763952e8abdegxrqb&type=client&ua=515902055c0556585506&rr=76ea9c674f613f1f"> Using similarity criteria to make issue trade-offs in automated negotiations </a>by P. Faratin, C. Sierra, N.R. Jennings. Artificial Intelligence, 2002. <a href="link">  </a> </summary> Automated negotiation is a key form of interaction in systems that are composed of multiple autonomous agents. The aim of such interactions is to reach agreements through an iterative process of making offers. The content of such proposals are, however, a function of the strategy of the agents. Here we present a strategy called the trade-off strategy where multiple negotiation decision variables are traded-off against one another (e.g., paying a higher price in order to obtain an earlier delivery date or waiting longer in order to obtain a higher quality service). Such a strategy is commonly known to increase the social welfare of agents. Yet, to date, most computational work in this area has ignored the issue of trade-offs, instead aiming to increase social welfare through mechanism design. The aim of this paper is to develop a heuristic computational model of the trade-off strategy and show that it can lead to an increased social welfare of the system. A novel linear algorithm is presented that enables software agents to make trade-offs for multi-dimensional goods for the problem of distributed resource allocation. Our algorithm is motivated by a number of real-world negotiation applications that we have developed and can operate in the presence of varying degrees of uncertainty. Moreover, we show that on average the total time used by the algorithm is linearly proportional to the number of negotiation issues under consideration. This formal analysis is complemented by an empirical evaluation that highlights the operational effectiveness of the algorithm in a range of negotiation scenarios. The algorithm itself operates by using the notion of fuzzy similarity to approximate the preference structure of the other negotiator and then uses a hill-climbing technique to explore the space of possible trade-offs for the one that is most likely to be acceptable <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1004452&casa_token=_y29arx7OywAAAAA:ut4x7dKOsZeIwPZeANENUCx4AHZJw5eqnqBX6dK8u5k-CcJXyYzv3i4qMvM_7P-Zhp_KwOK_qD3Ee8M&tag=1"> Co-evolving automata negotiate with a variety of opponents </a>by authors. CEC, 2002. <a href="link">  </a> </summary> Real-life negotiations typically involve multiple parties with (i) different preferences for the different issues and (ii) bargaining strategies which change over time. Such a dynamic environment (with imperfect information) is addressed in this paper with a multi-population evolutionary algorithm (EA). Each population represents an evolving collection of bargaining strategies in our setup. The bargaining strategies are represented by a special kind of finite automata, which require only two transitions per state. We show that such automata (with a limited complexity) are a suitable choice in a computational setting. We furthermore describe an EA which generates highly-efficient bargaining automata in the course of time. A series of computational experiments shows that co-evolving automata are able to discriminate successfully between different opponents, although they receive no explicit information about the identity or preferences of their opponents. These results are important for the further development of evolving automata for real-life (agent system) applications <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/A:1023068821218.pdf"> Negotiating Complex Contracts </a>by MARK KLEIN, PEYMAN FARATIN, HIROKI SAYAMA, YANEER BAR-YAM. Group Decision and Negotiation, 2003. <a href="link">  </a> </summary> Work to date on computational models of negotiation has focused almost exclusively on defining contracts consisting of one or a few independent issues and tractable contract spaces. Many real-world contracts, by contrast, are much more complex, consisting of multiple inter-dependent issues and intractably large contract spaces. This paper describes a simulated annealing based approach appropriate for negotiating such complex contracts that achieves near-optimal social welfares for negotiations with binary issue dependencies. <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245277&casa_token=b0X5M38ekKsAAAAA:rA1HV26lSrVBoxk78BNyYE_g08ZR8TeU--aBCB149XCEV3MgVGVKTCuBSMKtqgOQJ4DdWYIo5aTbAg&tag=1"> A Fuzzy-Logic Based Bidding Strategy for Autonomous Agents in Continuous Double Auctions </a>by Minghua He, Ho-fung Leung, Nicholas R. Jennings. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2003. <a href="link">  </a> </summary> Increasingly, many systems are being conceptualized, designed, and implemented as marketplaces in which autonomous software entities (agents) trade services. These services can be commodities in e-commerce applications or data and knowledge services in information economies. In many of these cases, there are both multiple agents that are looking to procure services and multiple agents that are looking to sell services at any one time. Such marketplaces are termed continuous double auctions (CDAs). Against this background, this paper develops new algorithms that buyer and seller agents can use to participate in CDAs. These algorithms employ heuristic fuzzy rules and fuzzy reasoning mechanisms in order to determine the best bid to make given the state of the marketplace. Moreover, we show how an agent can dynamically adjust its bidding behavior to respond effectively to changes in the supply and demand in the marketplace. We then show, by empirical evaluations, how our agents outperform four of the most prominent algorithms previously developed for CDAs (several of which have been shown to outperform human bidders in experimental studies). <br> - </details>

<details> <summary> <a href="https://pdf.sciencedirectassets.com/271585/1-s2.0-S0004370200X02639/1-s2.0-S0004370203000158/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIBP5GP7dmGqi7wAg5DR0bgqcpijcuNrCmVR5%2FJjNLEyUAiAaY2zWzg3ZHyA91biTlAPOFcqnKl%2Feq9nMY7FfR6jYfirVBAju%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMW7KnF2I7cCo00gM6KqkETOGplfimfDfqVd0lLkHb01ZxcR6F1mYpd9Q1Zcwvef0DyPMYWZHYt5YzASAlUAyMt0Msak7eJ8ipxVA%2B9FN3KFOwXiuvT%2BSfbOIuHbZdxq2karkRJK3LNWyuRjm7nRUfihqbuLhAQXnDWMfi0Y8MeqTyT8o5KqY5dQ5RzABntbg2igH19KT1r95A3MOvVwcdA5nXqhtooCcwZ8gQogVWpOyGce%2BREqS7qWqFnABSxSyQyz6fBG3WWBD229TAYdew4n45HP%2FmT3hNIz9SUZxTkEsrh7T2QU46XaqCCDq3A2qlCOLwS%2FvcA96aj7Gr9UNhjHlBbg0Rvwfvbn2kXfpe72uLbCUVrTzCE%2FXYfubVL2Y2FtOdqjZ5pWbcDv2MCWDmOFa0OdfXWDM%2BGm0I0fq2IfhOG59JM2jl7vNqElnTB6sdY0MiG6f7rOwBi6C75q4IpHAgax8C0OjG61kgY%2BBDqQGxH3mCx3m9q%2BleYKUHt7O1DPNqHN1gUui0ODRdrFyCXgIlLrnQO6L7a0BYuM3DQGyon9a1RgzgiRNMnAvVb%2FJO%2BHGx7oKjuqTUWNHGB7VIgNlhnSVtMmPsqEG1EcOCel2PF2jwu194lM3YlHqfdgawRH0pHIGmqAV26o9FhlpOJ5hz6ZQnexjrWzQ6tpAfBtDCvmSJrL0WkOQNY1yCOCPfaZuruBmkMnhYN3x14S4hZHH0u6qtjfy2rIFvl7Ix77ubwul8WnvAojCt8oKcBjqqAbUuga7WlnDbaiM370oOD9WjaWRhhY%2BdsPGONab%2FhkSTMVmEQWk6MJycF5Xz82ad%2BhCEU3k%2FPvdJDUy3IODVhfUACSQKUMwLLZUO%2FQbLIW2WGDyaWxh45brO7Lrmw0%2BltMZgPKOsbydk2I2Hmdwx%2Bi6Fi%2FEJ8xw1cUuxnN1hvJhN8MbMGJ08FYhmgQ%2B7eX0BK%2FSN3W2Yf1Z0z3RpB4EZYksaN1xW8P8Rm8f2&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221125T135532Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY7K43AS7V%2F20221125%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8c035dfdf2564023776054990ff28b4847fe04c4ed5a994dde23c527cbf2063a&hash=81d6bc119090610a45419d033c87de9f4cb2b93713f5cddf3659a379db625870&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0004370203000158&tid=spdf-588e879f-6d1e-40a5-9540-50758ec7b35a&sid=9109dace770f5647b7390367bbe152f21328gxrqb&type=client&ua=515901050057050c0401&rr=76fae1ecea430536"> BOB: Improved winner determination in combinatorial auctions and generalizations </a>by Tuomas Sandholm, Subhash Suri. Artificial Intelligence, 2003. <a href="link">  </a> </summary> Combinatorial auctions can be used to reach efficient resource and task allocations in multiagent systems where the items are complementary or substitutable. Determining the winners is NP-complete and inapproximable, but it was recently shown that optimal search algorithms do very well on average. This paper presents a more sophisticated search algorithm for optimal (and anytime) winner determination, including structural improvements that reduce search tree size, faster data structures, and optimizations at search nodes based on driving toward, identifying and solving tractable special cases. We also uncover a more general tractable special case, and design algorithms for solving it as well as for solving known tractable special cases substantially faster. We generalize combinatorial auctions to multiple units of each item, to reserve prices on singletons as well as combinations, and to combinatorial exchanges. All of these generalizations support both complementarity and substitutability of the items. Finally, we present algorithms for determining the winners in these generalizations <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/258568/1/amec03-shaheen.pdf"> Comparing Equilibria for Game-Theoretic and Evolutionary Bargaining Models </a>by Shaheen Fatima, Michael Wooldridge, Nicholas R. Jennings. AMEC, 2003. <a href="link">  </a> </summary> Game-theoretic models of bargaining are typically based on the assumption that players have perfect rationality and that they always play an equilibrium strategy. In contrast, research in experimental economics shows that in bargaining between human subjects, participants do not always play the equilibrium strategy. Such agents are said to be boundedly rational. In playing a game against a boundedly rational opponent, a player’s most effective strategy is not the equilibrium strategy, but the one that is the best reply to the opponent’s actual strategy. Against this background, this paper studies the bargaining behavior of boundedly rational agents by using genetic algorithms. Since bargaining involves players with different utility functions, we have two subpopulations – one represents the buyer, and the other represents the seller (i.e., the population is asymmetric). We study the competitive co-evolution of strategies in the two subpopulations for an incomplete information setting, and compare the results with those prescribed by game theory. Our analysis leads to two main conclusions. Firstly, our study shows that although each agent in the game-theoretic model has a strategy that is dominant at every period at which it makes a move, the stable state of the evolutionary model does not always match the game-theoretic equilibrium outcome. Secondly, as the players mutually adapt to each other’s strategy, the stable outcome depends on the initial population <br> - </details>

<details> <summary> <a href="https://www.sciencedirect.com/science/article/pii/S0004370203000419/pdf?crasolve=1&r=76f041591e3d2124&ts=1669273097150&rtype=https&vrr=UKN&redir=UKN&redir_fr=UKN&redir_arc=UKN&vhash=UKN&host=d3d3LnNjaWVuY2VkaXJlY3QuY29t&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&iv=742ebc324f7984dca1c1f6de0cf4a665&token=62646465663638356136353730393536643266623265343633336439643732323833613663623764353764643062636363383434303733323165383133613666346236383034646231666337633131643464313334363834383939623a623961643531653033613038666332653237333533346436&text=0bf166264e06189f27d06630256a1c2218ed109869cfd04cdd7b694f679a1287661a3053a5dc6c2e3e962d6cc5e7d241af10350003b03173d40bb11d07ff26b5dbc280e72cfd625493395b6015d5b5960aef862798df7827df3586b4c57b018f077057472eb1d10a023fc01fbb1b2f1def6c6823b9c802a46361b12e75ffd215e7114522d13622b262cbbb25eff773da04dbcceaff497663b6bc3280c76b93b2f4b8d8caa1b9298047813637fc08246717f7cdd9b57d6aa305647e891557004d2261c623e64078ada293c7502d9b3308d488047cd87ae0454aae5501f294ee713b0c8291db3c5694ba5c068f0a0d4346c68503cedba06b7867b8111b52c2ca6eb2ac434d4333c1471c5ac85f26a0222a65e3a3a85d99783852cb8224d2ad29dfbe0609a3a97813306531270891016352&original=3f6d64353d3661363231646162316230373731366264323864663530636261623139646336267069643d312d73322e302d53303030343337303230333030303431392d6d61696e2e706466265f76616c636b3d31"> A fuzzy constraint based model for bilateral, multi-issue negotiations in semi-competitive environments </a>by Xudong Luo, Nicholas R. Jennings, Nigel Shadbolt, Ho-fung Leung, Jimmy Ho-man Lee. Artificial Intelligence, 2003. <a href="link">  </a> </summary> This paper develops a fuzzy constraint based model for bilateral multi-issue negotiation in trading environments. In particular, we are concerned with the principled negotiation approach in which agents seek to strike a fair deal for both parties, but which, nevertheless, maximises their own payoff. Thus, there are elements of both competition and cooperation in the negotiation (hence semicompetitive environments). One of the key intuitions of the approach is that there is often more than one option that can satisfy the interests of both parties. So, if the opponent cannot accept an offer then the proponent should endeavour to find an alternative that is equally acceptable to it, but more acceptable to the opponent. That is, the agent should make a trade-off. Only if such a trade-off is not possible should the agent make a concession. Against this background, our model ensures the agents reach a deal that is fair (Pareto-optimal) for both parties if such a solution exists.Moreover, this is achieved by minimising the amount of private information that is revealed. The model uses prioritised fuzzy constraints to represent trade-offs between the different possible values of the negotiation issues and to indicate how concessions should be made when they are necessary. Also by using constraints to express negotiation proposals, the model can cover the negotiation space more efficiently since each exchange covers a region rather than a single point (which is what most existing models deal with). In addition, by incorporating the notion of a reward into our negotiation model, the agents can sometimes reach agreements that would not otherwise be possible. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/258572/1/pat-toit.pdf"> Developing a Bidding Agent for Multiple Heterogeneous Auctions </a>by P Anthony, NR Jennings. ACM TOIT, 2003. <a href="link">  </a> </summary> Due to the proliferation of online auctions, there is an increasing need to monitor and bid in multiple auctions in order to procure the best deal for the desired good. To this end, this paper reports on the development of a heuristic decision making framework that an autonomous agent can exploit to tackle the problem of bidding across multiple auctions with varying start and end times and with varying protocols (including English, Dutch and Vickrey). The framework is flexible, configurable, and enables the agent to adopt varying tactics and strategies that attempt to ensure that the desired item is delivered in a manner consistent with the user’s preferences. Given this large space of possibilities, we employ a genetic algorithm to search (offline) for effective strategies in common classes of environment. The strategies that emerge from this evolution are then codified into the agent’s reasoning behaviour so that it can select the most appropriate strategy to employ in its prevailing circumstances. The proposed framework has been implemented in a simulated marketplace environment and its effectiveness has been empirically demonstrated. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/a:1024592607487.pdf"> Multi-Issue Negotiation Processes by Evolutionary Simulation, Validation and Social Extensions </a>by ENRICO GERDING, DAVID VAN BRAGT and HAN LA POUTRE. Computational Economics, 2003. <a href="link">  </a> </summary> We describe a system for bilateral negotiations in which artificial agents are generated by an evolutionary algorithm (EA). The negotiations are governed by a finite-horizon version of the alternating-offers protocol. Several issues are negotiated simulataneously. We first analyse and validate the outcomes of the evolutionary system, using the game-theoretic subgame-perfect equilibrium as a benchmark. We then present two extensions of the negotiation model. In the first extension agents take into account the fairness of the obtained payoff. We find that when the fairness norm is consistently applied during the negotiation, agents reach symmetric outcomes which are robust and rather insensitive to the actual fairness settings. In the second extension we model a competitive market situation where agents have multiple bargaining opportunities before reaching the final agreement. Symmetric outcomes are now also obtained, even when the number of bargaining opportunities is small. We furthermore study the influence of search or negotiation costs in this game <br> - </details>

<details> <summary> <a href="http://shiftleft.com/mirrors/www.hpl.hp.com/techreports/2002/HPL-2002-321.pdf"> Applying Evolutionary Game Theory to Auction Mechanism Design </a>by Andrew Byde. ACM-EC, 2003. <a href="link">  </a> </summary> In this paper we describe an evolution-based method for evaluating auction mechanisms, and apply it to a space of mechanisms including the standard first- and second-price sealed bid auctions. We replicate results known already in the Auction Theory literature regarding the suitability of different mechanisms for different bidder environments, and extend the literature by establishing the superiority of novel mechanisms over standard mechanisms, for commonly occurring scenarios. Thus this paper simultaneously extends Auction Theory, and provides a systematic method for further such extensions. <br> - </details>

<details> <summary> <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=1046&context=sis_research"> Walverine: A walrasian trading agent </a>by Shih-Fen Cheng, Evan Leung, Kevin M. Lochner, Kevin O’Malley, Daniel M. Reeves,Julian L. Schvartzman, Michael P. Wellman. AAMAS, 2003. <a href="link">  </a> </summary> TAC-02 was the third in a series of Trading Agent Competition events fostering research in automating trading strategies by showcasing alternate approaches in an open-invitation market game. TAC presents a challenging travel-shopping scenario where agents must satisfy client preferences for complementary and substitutable goods by interacting through a variety of market types. Michigan’s entry, Walverine, bases its decisions on a competitive (Walrasian) analysis of the TAC travel economy. Using this Walrasian model, we construct a decision-theoretic formulation of the optimal bidding problem, which Walverine solves in each round of bidding for each good. Walverine’s optimal bidding approach, as well as several other features of its overall strategy, are potentially applicable in a broad class of trading environments. <br> - </details>

<details> <summary> <a href="http://iebi.nctu.edu.tw/course_old/9515492/Final_Report/9534530/9534530-3.pdf"> The Importance of Ordering in Sequential Auctions </a>by Wedad Elmaghraby. Management Science, 2003. <a href="link">  </a> </summary> To date, the largest part of literature on multi-unit auctions has assumed that there are k homogeneous objects being auctioned, where each bidder wishes to win exactly one or all of k units. These modeling assumptions have made the examination of ordering in sequential auctions inconsequential. The aim of this paper is to introduce and highlight the critical influence that ordering can have on the efficiency of an auction. We study a buyer who outsources via sequential 2nd-price auctions two heterogeneous jobs, and faces a diverse set of suppliers with capacity constraints <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-642-17045-4_14.pdf"> Bargaining with Posterior Opportunities: An Evolutionary Social Simulation </a>by E. H. Gerding, J.A. La Poutre. LNEMS, 2003. <a href="link">  </a> </summary> Negotiations have been extensively studied theoretically throughout the years. A well-known bilateral approach is the ultimatum game, where two agents negotiate on how to split a pie or a "dollar": the proposer makes an offer and responder can choose to accept or reject. In this paper a natural extension of the ultimatum game is presented, in which both agents can negotiate with other opponents in case of a disagreement. This way the basics of a compe titive market are modelled where for instance a buyer can try several sellers before making a purchase decision. The game is investigated using an evolutionary simulation. The outcomes appear to depend largely on the information available to the agents. We find that if the agents' number of future bargaining opportunities is commonly known, the proposer has the advantage. Ifthis information is held private, however, the responder can obtain a larger share of the pie. For the first case we also provide a game-theoretic analysis and compare the outcome with evolutionary results. Furthermore, the effects of search costs and allowing multiple issues to be negotiated simultaneously are investigated. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/258564/1/minghua-toit.pdf"> SouthamptonTAC: An Adaptive Autonomous
Trading Agent </a>by MINGHUA HE, NICHOLAS R. JENNINGS. ACM Transactions on Internet Technology, 2003. <a href="link">  </a> </summary> Software agents are increasingly being used to represent humans in on-line auctions. Such agents have the advantages of being able to systematically monitor a wide variety of auctions and then make rapid decisions about what bids to place in what auctions. They can do this continuously and repetitively without losing concentration. Moreover, in complex multiple auction settings, agents may need to modify their behavior in one auction depending on what is happening in another. To provide a means of evaluating and comparing (benchmarking) research methods in this area, the Trading Agent Competition (TAC) was established. This competition involves a number of agents bidding against one another in a number of related auctions (operating different protocols) to purchase travel packages for customers. Against this background, this artcle describes the design, implementation and evaluation of our adaptive autonomous trading agent, SouthamptonTAC, one of the most successful participants in TAC 2002 <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/David-Bragt/publication/225315578_Why_Agents_for_Automated_Negotiations_Should_Be_Adaptive/links/0c960525bab1154003000000/Why-Agents-for-Automated-Negotiations-Should-Be-Adaptive.pdf"> Why agents for automated negotiations should be adaptive </a>by D.D.B. van Bragt, J.A. La Poutre. Netnomics, 2003. <a href="link">  </a> </summary> We show that adaptive agents on the Internet can learn to exploit bidding agents who use a (limited) number of fixed strategies. These learning agents can be generated by adapting a special kind of finite automata with evolutionary algorithms (EAs). Our approach is especially powerful if the adaptive agent participates in frequently occurring micro-transactions, where there is sufficient opportunity for the agent to learn online from past negotiations. More in general, results presented in this paper provide a solid basis for the further development of adaptive agents for Internet applications. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/cs/0412106.pdf"> Online Learning of Aggregate Knowledge about Non-linear Preferences Applied to Negotiating Prices and Bundles </a>by D.J.A. Somefun, T.B. Klos, J.A. La Poutre. ICEC, 2004. <a href="link">  </a> </summary> In this paper, we consider a form of multi-issue negotiation where a shop negotiates both the contents and the price of bundles of goods with his customers. We present some key insights about, as well as a procedure for, locating mutually beneficial alternatives to the bundle currently under negotiation. The essence of our approach lies in combining aggregate (anonymous) knowledge of customer preferences with current data about the ongoing negotiation process. The developed procedure either works with already obtained aggregate knowledge or, in the absence of such knowledge, learns the relevant information online. We conduct computer experiments with simulated customers that have nonlinear preferences. We show how, for various types of customers, with distinct negotiation heuristics, our procedure (with and without the necessary aggregate knowledge) increases the speed with which deals are reached, as well as the number and the Pareto efficiency of the deals reached compared to a benchmark. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/ICAPS/2004/ICAPS04-030.pdf"> Price Prediction Strategies for Market-Based Scheduling </a>by Jeffrey K. MacKie-Mason, Anna Osepayshvili, Daniel M. Reeves, Michael P. Wellman. AAAI, 2004. <a href="link">  </a> </summary> In a market-based scheduling mechanism, the allocation of time-specific resources to tasks is governed by a competitive bidding process. Agents bidding for multiple, separately allocated time slots face the risk that they will succeed in obtaining only part of their requirement, incurring expenses for potentially worthless slots. We investigate the use of price prediction strategies to manage such risk. Given an uncertain price forecast, agents follow simple rules for choosing whether and on which time slots to bid. We find that employing price predictions can indeed improve performance over a straightforward baseline in some settings. Using an empirical game-theoretic methodology, we establish Nash equilibrium profiles for restricted strategy sets. This allows us to confirm the stability of price-predicting strategies, and measure overall efficiency. We further experiment with variant strategies to analyze the source of prediction’s power, demonstrate the existence of self-confirming predictions, and compare the performance of alternative prediction methods. <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1460738&casa_token=wKwgrRN8AGQAAAAA:i8kMBXhVePv1z3n_64vnE1I0aBe0-bFVpB6zgbbIVpmQCsWoc1YVc0u1xfLHlnG_x0cJHHX36FP-3KA"> An Efficient Turnkey Agent for Repeated Trading with Overall Budget and Preferences </a>by I.B. Vermeulen, D.J.A. Somefun, J.A. La Poutre. Cybernetics and Intelligent Systems, 2004. <a href="link">  </a> </summary> For various e-commerce applications autonomous agents can do the actual trading on behalf of their users. We consider an agent whu trades rcpentcdly on hchalf of his uscr, given an overall budget and prcfcrcnccs per time stcp, both specified at thc start. For many e-commerce settings such an agent has limited computationaI resources, limited prior information concerning price fluctuations, and little time for online learning. We therefore develop an efficient heuristic that requires little prior information to work well from the start, even for very roughed nonsmooth problem instanccs. Extensive computer experiments conducted for a wide variety of customer preferences show virtually no difference in performance betwcen a dynamic prugramming (IW) approach and the developed heuristic carrying out the agent’s task. The DP approach ha$, however, thc important drawback of generally being too computationally intensive. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/263434/1/marketACM_draft.pdf"> Market-based Recommendation: Agents that Compete for Consumer Attention </a>by SANDER M. BOHTE, ENRICO GERDING, HAN LA POUTRE. ACM TOIT, 2004. <a href="link">  </a> </summary> The amount of attention space available for recommending suppliers to consumers on e-commerce sites is typically limited. We present a competitive distributed recommendation mechanism based on adaptive software agents for efficiently allocating the “consumer attention space”, or banners. In the example of an electronic shopping mall, the task is delegated to the individual shops, each of which evaluates the information that is available about the consumer and his or her interests (e.g. keywords, product queries, and available parts of a profile). Shops make a monetary bid in an auction where a limited amount of “consumer attention space” for the arriving consumer is sold. Each shop is represented by a software agent that bids for each consumer. This allows shops to rapidly adapt their bidding strategy to focus on consumers interested in their offerings. For various basic and simple models for on-line consumers, shops, and profiles, we demonstrate the feasibility of our system by evolutionary simulations as in the field of agent-based computational economics (ACE). We also develop adaptive software agents that learn bidding-strategies, based on neural networks and strategy exploration heuristics. Furthermore, we address the commercial and technological advantages of this distributed market-based approach. The mechanism we describe is not limited to the example of the electronic shopping mall, but can easily be extended to other domains. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/1207.4108"> Bidding under Uncertainty: Theory and Experiments </a>by Amy Greenwald, Justin Boyan. UAI, 2004. <a href="link">  </a> </summary> This paper describes a study of agent bidding strategies, assuming combinatorial valuations for complementary and substitutable goods, in three auction environments: sequential auctions, simultaneous auctions, and the Trading Agent Competition (TAC) Classic hotel auction design, a hybrid of sequential and simultaneous auctions. The problem of bidding in sequential auctions is formulated as an MDP, and it is argued that expected marginal utility bidding is the optimal bidding policy. The problem of bidding in simultaneous auctions is formulated as a stochastic program, and it is shown by example that marginal utility bidding is not an optimal bidding policy, even in deterministic settings. Two alternative methods of approximating a solution to this stochastic program are presented: the first method, which relies on expected values, is optimal in deterministic environments; the second method, which samples the nondeterministic environment, is asymptotically optimal as the number of samples tends to infinity. Finally, experiments with these various bidding policies are described in the TAC Classic setting. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/265624/1/autombarg2004a.pdf"> Automated Bilateral Bargaining about Multiple Attributes in a One-to-Many Setting </a>by E.H. Gerding, D.J.A. Somefun, J.A. La Poutre. ICEC, 2004. <a href="link">  </a> </summary> Negotiations are an important way of reaching agreements between selfish autonomous agents. In this paper we focus on one-to-many bargaining within the context of agentmediated electronic commerce. We consider an approach where a seller agent negotiates over multiple interdependent attributes with many buyer agents in a bilateral fashion. In this setting, “fairness,” which corresponds to the notion of envy-freeness in auctions, may be an important business constraint. For the case of virtually unlimited supply (such as information goods), we present a number of one-to-many bargaining strategies for the seller agent, which take into account the fairness constraint, and consider multiple attributes simultaneously. We compare the performance of the bargaining strategies using an evolutionary simulation, especially for the case of impatient buyers. Several of the developed strategies are able to extract almost all the surplus; they utilize the fact that the setting is one-to-many, even though bargaining is bilateral. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/259560/1/aamas04duong.pdf"> Coordinating multiple concurrent negotiations </a>by Thuc Duong Nguyen, Nicholas R. Jennings. AAMAS, 2004. <a href="link">  </a> </summary> To secure good deals, an agent may engage in multiple concurrent negotiations for a particular good or service. However for this to be effective, the agent needs to carefully coordinate its negotiations. At a basic level, such coordination should ensure the agent does not procure more of the good than is needed. But to really derive benefit from such an approach, the agent needs the concurrent encounters to mutually influence one another (e.g. a good price with one opponent should enable an agent to negotiate more strongly in the other interactions). To this end, this paper presents a novel heuristic model for coordinating multiple bilateral negotiations. The model is empirically evaluated and shown to be effective and robust in a range of negotiation scenarios. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/B:AGNT.0000038029.82331.c0.pdf"> Agent-Mediated Electronic Commerce </a>by CARLES SIERRA. AAMAS, 2004. <a href="link">  </a> </summary> Electronic commerce has been one of the traditional arenas for agent technology. The complexity of these applications has been a challenge for researchers that have developed methodologies, products and systems, having in mind the specificities of trade, the interaction particularities of commerce, the strict notion of commitment and contract, and the clearly shaped conventions and norms that structure the field. In this paper I survey some key areas for agent technology which, although general, are of special importance in electronic commerce, namely, solid development methodologies, negotiation technologies and trust-building mechanisms. I give examples of systems in which I have directly participated, although I also try to refer to the work of other AgentLink Special Interest Group members over the last few years <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/265630/1/bilateralbarga.pdf"> Bilateral Bargaining in a One-to-Many Bargaining Setting </a>by Enrico Gerding, Koye Somefun, Han La Poutre. AAMAS, 2004. <a href="link">  </a> </summary> Electronic markets are becoming increasingly transparent with low search cost, strong price competition, and low margins. Automated negotiation enables a business to go beyond price competition. Through the use of autonomous agents, which negotiate on behalf of their owners, a business can obtain flexibility in prices and goods, distinguish between groups of buyers based on their preferences, and even personalize complex goods according to the demands of individual buyers without significantly increasing transaction costs. We focus here on one-to-many bargaining, where a seller agent negotiates, on behave of a seller, with many buyer agents individually in a bilateral fashion. In many cases, auctions can be used to effectively organize one-to-many bargaining. For various situations, however, auctions may not be the preferred protocol for bargainers. In situations of, for example, flexible or virtually unlimited supply, multiple issues, and/or continuous sale the appropriate auction protocol becomes, at best, much more complex. Consequently, businesses may opt for the intuitive and flexible bilateral bargaining protocol, where the seller agent negotiates bilaterally with one or more buyers simultaneously by exchanging offers and counter offers. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/265638/1/neg_bundle.pdf"> Automated Negotiation and Bundling of Information Goods </a>by Koye Somefun, Enrico Gerding, Sander Bohte, Han La Poutre. LNAI, 2004. <a href="link">  </a> </summary> In this paper, we present a novel system for selling bundles of news items. Through the system, customers bargain with the seller over the price and quality of the delivered goods. The advantage of the developed system is that it allows for a high degree of flexibility in the price, quality, and content of the offered bundles. The price, quality, and content of the delivered goods may, for example, differ based on daily dynamics and personal interest of customers. Autonomous “software agents” execute the negotiation on behalf of the users of the system. To perform the actual negotiation these agents make use of bargaining strategies. We present the novel approach of decomposing bargaining strategies into concession strategies and Pareto efficient search strategies. Additionally, we introduce the orthogonal and orthogonal-DF strategy: two Pareto search strategies. We show through computer experiments that the use of these Pareto search strategies will result in very efficient bargaining outcomes. Moreover, the system is setup such that it is actually in the best interest of the customer to have their agent adhere to this approach of disentangling the bargaining strategy. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/1207.1400.pdf"> Self-Confirming Price Prediction for Bidding in Simultaneous Ascending Auctions </a>by Anna Osepayshvili, Michael P. Wellman, Daniel M. Reeves, Jeffrey K. MacKie-Mason. UAI, 2005. <a href="link">  </a> </summary> Simultaneous ascending auctions present agents with the exposure problem: bidding to acquire a bundle risks the possibility of obtaining an undesired subset of the goods. Auction theory provides little guidance for dealing with this problem. We present a new family of decision-theoretic bidding strategies that use probabilistic predictions of final prices. We focus on selfconfirming price distribution predictions, which by definition turn out to be correct when all agents bid decision-theoretically based on them. Bidding based on these is provably not optimal in general, but our experimental evidence indicates the strategy can be quite effective compared to other known methods. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/268206/1/prima2005.pdf"> Learning the Structure of Utility Graphs Used in Multi-Issue Negotiation through Collaborative Filtering </a>by Valentin Robu, Han La Poutre. PRIMA, 2005. <a href="link">  </a> </summary> Graphical utility models represent powerful formalisms for modeling complex agent decisions involving multiple issues. In the context of negotiation, it has been shown that using utility graphs enables reaching Paretoefficient agreements with a limited number of negotiation steps, even for highdimensional negotiations involving complex complementarity/ substitutability dependencies between multiple issues. This paper considerably extends the results of Valentin et al., (2005), by proposing a method for constructing the utility graphs of buyers automatically, based on previous negotiation data. Our method is based on techniques inspired from item-based collaborative filtering, used in online recommendation algorithms. Experimental results show that our approach is able to retrieve the structure of utility graphs online, with a high degree of accuracy, even for highly non-linear settings and even if a relatively small amount of data about concluded negotiations is available. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Valentin-Robu/publication/221456988_Modeling_complex_multi-issue_negotiations_using_utility_graphs/links/004635267b150953b9000000/Modeling-complex-multi-issue-negotiations-using-utility-graphs.pdf"> Modeling Complex Multi-Issue Negotiations
Using Utility Graphs </a>by Valentin Robu, D.J.A. Somefun, J.A. La Poutre. AAMAS, 2005. <a href="link">  </a> </summary> This paper presents an agent strategy for complex bilateral negotiations over many issues with inter-dependent valuations. We use ideas inspired by graph theory and probabilistic influence networks to derive efficient heuristics for negotiations about multiple issues. Experimental results show — under relatively weak assumptions with respect to the structure of the utility functions – that the developed approach leads to Pareto-efficient outcomes. Moreover, Pareto-efficiency can be reached with few negotiation steps, because we explicitly model and utilize the underlying graphical structure of complex utility functions. Consequently, our approach is applicable to domains where reaching an efficient outcome in a limited amount of time is important. Furthermore, unlike other solutions for highdimensional negotiations, the proposed approach does not require a mediator. <br> - </details>

<details> <summary> <a href="https://deepblue.lib.umich.edu/bitstream/handle/2027.42/50434/proof-dexter-dss.pdf?sequence=1"> Exploring bidding strategies for market-based scheduling </a>by Daniel M. Reeves*, Michael P. Wellman, Jeffrey K. MacKie-Mason, Anna Osepayshvili. Decision Support Systems, 2005. <a href="link">  </a> </summary> A market-based scheduling mechanism allocates resources indexed by time to alternative uses based on the bids of participating agents. Agents are typically interested in multiple time slots of the schedulable resource, with value determined by the earliest deadline by which they can complete their corresponding tasks. Despite the strong complementarities among slots induced by such preferences, it is often infeasible to deploy a mechanism that coordinates allocation across all time slots. We explore the case of separate, simultaneous markets for individual time slots, and the strategic problem it poses for bidding agents. Investigation of the straightforward bidding policy and its variants indicates that the efficacy of particular strategies depends critically on preferences and strategies of other agents, and that the strategy space is far too complex to yield to general game-theoretic analysis. For particular environments, however, it is often possible to derive constrained equilibria through evolutionary search methods <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/263435/1/fulltext.pdf"> Efficient Methods for Automated Multi-Issue Negotiation: Negotiating over a Two-Part Tariff </a>by D.J.A. Somefun, E.H. Gerding, J.A. La Poutre. International Journal of Intelligent Systems, 2006. <a href="link">  </a> </summary> In this article, we consider the novel approach of a seller and customer negotiating bilaterally about a two-part tariff, using autonomous software agents. An advantage of this approach is that win–win opportunities can be generated while keeping the problem of preference elicitation as simple as possible. We develop bargaining strategies that software agents can use to conduct the actual bilateral negotiation on behalf of their owners. We present a decomposition of bargaining strategies into concession strategies and Pareto-efficient-search methods: Concession and Paretosearch strategies focus on the conceding and win–win aspect of bargaining, respectively. An important technical contribution of this article lies in the development of two Pareto-search methods. Computer experiments show, for various concession strategies, that the respective use of these two Pareto-search methods by the two negotiators results in very efficient bargaining outcomes while negotiators concede the amount specified by their concession strategy <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/264473/1/toit-06.pdf"> A Heuristic Bidding Strategy for Buying Multiple Goods in Multiple English Auctions </a>by MINGHUA HE, NICHOLAS R. JENNINGS, ADAM PRUGEL-BENNETT. ACM Transactions on Internet Technology, 2006. <a href="link">  </a> </summary> This paper presents the design, implementation, and evaluation of a novel bidding algorithm that a software agent can use to obtain multiple goods from multiple overlapping English auctions. Specifically, an Earliest Closest First heuristic algorithm is proposed that uses neurofuzzy techniques to predict the expected closing prices of the auctions and to adapt the agent’s bidding strategy to reflect the type of environment in which it is situated. This algorithm first identifies the set of auctions that are most likely to give the agent the best return and then, according to its attitude to risk, it bids in some other auctions that have approximately similar expected returns, but which finish earlier than those in the best return set. We show through empirical evaluation against a number of methods proposed in the multiple auction literature that our bidding strategy performs effectively and robustly in a wide range of scenarios. <br> - </details>

<details> <summary> <a href="http://pubsonline.informs.org/doi/10.1287/moor.6.1.58"> Optimal Auction Design </a> by Myerson, Roger B.. Mathematics of Operations Research, 1981. <a href="link">  </a> </summary> This paper considers the problem faced by a seller who has a single object to sell to one of several possible buyers, when the seller has imperfect information about how much the buyers might be willing to pay for the object. The seller's problem is to design an auction game which has a Nash equilibrium giving him the highest possible expected utility. Optimal auctions are derived in this paper for a wide class of auction design problems. <br> - </details>

<details> <summary> <a href="https://www.jstor.org/stable/1912346?origin=crossref"> Incentive Compatibility and the Bargaining Problem </a> by Myerson, Roger B.. Econometrica, 1979. <a href="link">  </a> </summary> Collective choice problems are studied from the Bayesian viewpoint. It is shown that the set of expected utility allocations which are feasible with incentive-compatible mechanisms is compact and convex, and includes the equilibrium allocations for all other mechanisms. The generalized Nash solution proposed by Harsanyi and Selten is then applied to this set to define a bargaining solution for Bayesian collective choice problems. <br> - </details>

<details> <summary> <a href="https://www.sciencedirect.com/science/article/pii/0022053183900480"> Efficient mechanisms for bilateral trading </a> by Myerson, Roger B; Satterthwaite, Mark A. Journal of Economic Theory, 1983. <a href="link">  </a> </summary> We consider bargaining problems between one buyer and one seller for a single object. The seller's valuation and the buyer's valuation for the object are assumed to be independent random variables, and each individual's valuation is unknown to the other. We characterize the set of allocation mechanisms that are Bayesian incentive compatible and individually rational, and show the general impossibility of ex post efficient mechanisms without outside subsidies. For a wide class of problems we show how to compute mechanisms that maximize expected total gains from trade, and mechanisms that can maximize a broker's expected profit. <br> - </details>

<details> <summary> <a href="https://academic.oup.com/mit-press-scholarship-online/book/29406/chapter/244787625"> Bidding Languages for Combinatorial Auctions </a> by Nisan, Noam. Combinatorial Auctions, 2005. <a href="link">  </a> </summary> Combinatorial auctions provide a valuable mechanism for the allocation of goods in settings where buyer valuations exhibit complex structure with respect to substitutability and complementarity. Most algorithms are designed to work with explicit bids for concrete bundles of goods. However, logical bidding languages allow the expression of complex utility functions in a natural and concise way. We introduce a new, generalized language where bids are given by propositional formulae whose subformulae can be annotated with prices. This language allows bidder utilities to be formulated more naturally and concisely than existing languages. Furthermore, we outline a general algorithmic technique for winner determination for auctions that use this bidding language. <br> - </details>

<details> <summary> <a href="https://courses.cs.duke.edu/fall11/cps296.1/ica_chapter.pdf"> Iterative combinatorial auctions: achieving economic and computational efficiency </a> by Parkes, David C.; Ungar, Lyle H.., 2001. <a href="link">  </a> </summary> ITERATIVE COMBINATORIAL AUCTIONS: ACHIEVING ECONOMIC AND COMPUTATIONAL EFFICIENCY David Christopher Parkes Supervisor: Lyle H. Ungar A fundamental problem in building open distributed systems is to design mechanisms that compute optimal system-wide solutions despite the self-interest of individual users and computational agents. Classic game-theoretic solutions are often prohibitively expensive computationally. For example, the Generalized Vickrey Auction (GVA) is an e cient and strategy-proof solution to the combinatorial allocation problem (CAP), in which agents demand bundles of items, but every agent must reveal its value for all possible bundles and the auctioneer must solve a sequence of NP-hard optimization problems to compute the outcome. I propose iBundle, an iterative combinatorial auction in which agents can bid for combinations of items and adjust their bids in response to bids from other agents. iBundle computes the e cient allocation in the CAP when agents follow myopic best-response bidding strategies, bidding for the bundle(s) that maximize their surplus taking the current prices as xed. iBundle solves problems without complete information revelation from agents and terminates in competitive equilibrium. Moreover, an agent can follow a myopic best-response strategy with approximate values on bundles, for example with lowerand upperbounds. My approach to iterative mechanism design decomposes the problem into two parts. First, I use linear programming theory to develop an e cient iterative auction under the assumption that agents will follow a myopic best-response bidding strategy. Second, I extend the approach to also compute Vickrey payments at the end of the auction. This <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2000/AAAI00-012.pdf"> Iterative Combinatorial Auctions: Theory and Practice </a> by Parkes, David C.; Ungar, Lyle H.. AAAI/IAAI, 2000. <a href="link">  </a> </summary> Combinatorial auctions, which allow agents to bid directly for bundles of resources, are necessary for optimal auction-based solutions to resource allocation problems with agents that have non-additive values for resources, such as distributed scheduling and task assignment problems. We introduce iBundle, the first iterative combinatorial auction that is optimal for a reasonable agent bidding strategy, in this case myopic best-response bidding. Its optimality is proved with a novel connection to primal-dual optimization theory. We demonstrate orders of magnitude performance improvements over the only other known optimal combinatorial auction, the Generalized Vickrey Auction. <br> - </details>

<details> <summary> <a href="http://dpennock.com/papers/pennock-ec-2004-dynamic-parimutuel.pdf"> A dynamic pari-mutuel market for hedging, wagering, and information aggregation </a> by Pennock, David M.. ACM Conference on Economics and Computation, 2004. <a href="link">  </a> </summary> I develop a new mechanism for risk allocation and information speculation called a dynamic pari-mutuel market (DPM). ADPM acts as hybrid between a pari-mutuel market and a continuous double auction (CDA), inheriting some of the advantages of both. Like a pari-mutuel market, a DPM offers infinite buy-in liquidity and zero risk for the market institution; like a CDA, a DPM cancontinuously react to new information, dynamically incorporate information into prices, and allow traders to lock in gains or limit losses by selling prior to event resolution. The trader interface can be designed to mimic the familiar double auction format with bid-ask queues, though with an addition variable called the payoff per share. The DPM price function can be viewed as an automated market maker always offering to sell at some price, and moving the price appropriately according to demand. Since the mechanism is pari-mutuel (i.e., redistributive), it is guaranteed to pay out exactly the amount of money taken in. Iexplore a number of variations on the basic DPM, analyzing the properties of each, and solving in closed form for their respective price functions. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2007/AAAI07-018.pdf"> Revenue monotonicity in combinatorial auctions </a> by Rastegari, Baharak; Condon, Anne; Leyton-Brown, Kevin. SeCO Workshops, 2007. <a href="link">  </a> </summary> Intuitively, one might expect that a seller's revenue from an auction weakly increases as the number of bidders grows, as this increases competition. However, it is known that for combinatorial auctions that use the VCG mechanism, a seller can sometimes increase revenue by dropping bidders. In this paper we investigate the extent to which this problem can occur under other dominant-strategy combinatorial auction mechanisms. Our main result is that such failures of "revenue monotonicity" are not limited to mechanisms that achieve efficient allocations. Instead, they can occur under any dominant-strategy direct mechanism that sets prices using critical values, and that always chooses an allocation that cannot be augmented to make some bidder better off, while making none worse off. <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~sandholm/oralg.aij.pdf"> Optimal Winner Determination Algorithms </a> by Sandholm, Tuomas., 2005. <a href="link">  </a> </summary> This chapter discusses the optimal winner determination algorithm, which is used for solving the general problem where bids are not restricted. It further focuses on different fundamental design dimensions of search algorithms for winner determination, which include search formulation, search strategy, decomposition techniques, random restart techniques, and caching techniques. The chapter also discusses winner determination under fully expressive bidding languages such as XOR bidding language and OR bidding language, where substitutability is exhibited using XOR-constraints between different bids. It concludes that the various techniques it has discussed are applicable in generalized combinatorial markets, which include combinatorial reverse auctions and combinatorial exchanges. <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~sandholm/elicitation.acmec01.pdf"> Preference elicitation in combinatorial auctions </a> by Conen, Wolfram; Sandholm, Tuomas. Adaptive Agents and Multi-Agent Systems, 2001. <a href="link">  </a> </summary> Combinatorial auctions (CAs) where bidders can bid on bundles of items can be very desirable market mechanisms when the items sold exhibit complementarity and/or substitutability, so the bidder's valuations for bundles are not additive. However, in a basic CA, the bidders may need to bid on exponentially many bundles, leading to difficulties in determining those valuations, undesirable information revelation, and unnecessary communication. In this paper we present a design of an auctioneer agent that uses topological structure inherent in the problem to reduce the amount of information that it needs from the bidders. An analysis tool is presented as well as data structures for storing and optimally assimilating the information received from the bidders. Using this information, the agent then narrows down the set of desirable (welfare-maximizing or Pareto-efficient) allocations, and decides which questions to ask next. Several algorithms are presented that ask the bidders for value, order, and rank information. A method is presented for making the elicitor incentive compatible. <br> - </details>

<br/>

### Bounded Rationality

<details> <summary> <a href="https://dl.acm.org/doi/pdf/10.1145/195058.195445"> On complexity as bounded rationality  </a>by C.H. Papadimitriou, M. Yannakakis. STOC, 1994. <a href="link">  </a> </summary> It has been hoped that computational approaches can help resolve some well-known paradoxes in game theory. We prove that tf the repeated prisoner’s dilemma M played by finite automata with less than exponentially (in the number of rounds) many states, then cooperation can be achieved an equilibrium (while with exponentially many states, defection is the only equilibrium). We furthermore prove a generalization to arbitrary games and Pareto optimal points. Finally, we present a general model of polynomially computable games, and characterize in terms of fami!iar complexity classes ranging from NP to NEXP the natural problems that arise in relation with such games. <br> - </details>

<details> <summary> <a href="http://sfi-edu.s3.amazonaws.com/sfi-edu/production/uploads/sfi-com/dev/uploads/filer/eb/8d/eb8d7d4d-8bc0-49ac-8feb-e73c5a141a12/94-03-014.pdf"> Inductive Reasoning, Bounded Rationality and the Bar Problem </a>by W. Brian Arthur. American Economic Review, 1994. <a href="link">  </a> </summary> This paper draws on modem psychology to argue that as humans, in economic decision contexts that are complicated or ill-defined, we use not deductive, but inductive reasoning. That is, in such contexts we induce a variety of working hypotheses or mental models, act upon the most credible, and replace hypotheses with new ones if they cease to work. Inductive reasoning leads to a rich psychological world in which an agent's hypotheses or mental models compete for survival against each other, in an environment formed by other other agents' hypotheses or mental models-a world that is both evolutionary and complex. Inductive reasoning can be modeled in a variety of ways. The main body of the paper introduces and models a coordination problem-"the bar problem"-in which agents' expectations are forced to be subjective and to differ. It shows that while agents' beliefs never settle down, collectively they form an "ecology" that does converge to an equilibrium pattern. <br> - </details>

<details> <summary> <a href="https://mitpress.mit.edu/9780262681001/modeling-bounded-rationality/"> Modeling Bounded Rationality </a>by Ariel Rubinstein. MIT Press,
1998. <a href="link">  </a> </summary> The notion of bounded rationality was initiated in the 1950s by Herbert Simon; only recently has it influenced mainstream economics. In this book, Ariel Rubinstein defines models of bounded rationality as those in which elements of the process of choice are explicitly embedded. The book focuses on the challenges of modeling bounded rationality, rather than on substantial economic implications. In the first part of the book, the author considers the modeling of choice. After discussing some psychological findings, he proceeds to the modeling of procedural rationality, knowledge, memory, the choice of what to know, and group decisions.In the second part, he discusses the fundamental difficulties of modeling bounded rationality in games. He begins with the modeling of a game with procedural rational players and then surveys repeated games with complexity considerations. He ends with a discussion of computability constraints in games. The final chapter includes a critique by Herbert Simon of the author's methodology and the author's response. <br> - </details>

<details> <summary> <a href="https://www.pnas.org/doi/pdf/10.1073/pnas.211534898"> On the impossibility of predicting the behavior of rational agents </a>by Dean P. Foster, H. Peyton Young. PNAS, 2001. <a href="link">  </a> </summary> A foundational assumption in economics is that people are rational: they choose optimal plans of action given their predictions about future states of the world. In games of strategy this means that each player’s strategy should be optimal given his or her prediction of the opponents’ strategies. We demonstrate that there is an inherent tension between rationality and prediction when players are uncertain about their opponents’ payoff functions. Specifically, there are games in which it is impossible for perfectly rational players to learn to predict the future behavior of their opponents (even approximately) no matter what learning rule they use. The reason is that in trying to predict the next-period behavior of an opponent, a rational player must take an action this period that the opponent can observe. This observation may cause the opponent to alter his next-period behavior, thus invalidating the first player’s prediction. The resulting feedback loop has the property that, a positive fraction of the time, the predicted probability of some action next period differs substantially from the actual probability with which the action is going to occur. We conclude that there are strategic situations in which it is impossible in principle for perfectly rational agents to learn to predict the future behavior of other perfectly rational agents based solely on their observed actions. <br> - </details>

<details> <summary> <a href="https://doi.org/10.1145/195058.195445"> On complexity as bounded rationality (extended abstract) </a> by Papadimitriou, Christos H.; Yannakakis, Mihalis. Proceedings of the twenty-sixth annual ACM symposium on Theory of Computing, 1994. <a href="link">  </a> </summary> nan <br> - </details>

<br/>

### Collective Intelligence

<details> <summary> <a href="https://dl.acm.org/doi/pdf/10.1145/79147.79161"> Knowledge and common knowledge in a distributed environment </a>by Halpern JY, Moses Y. Journal of the ACM (JACM), 1990. <a href="link">  </a> </summary> Reasoning about knowledge seems to play a fundamental role in distributed systems. Indeed, such reasoning is a central part of the informal intuitive arguments used in the design of distributed protocols. Communication in a distributed system can be viewed as the act of transforming the system’s state of knowledge. This paper presents a general framework for formalizing and reasoning about knowledge in distributed systems. It is shown that states of knowledge of groups of processors are useful concepts for the design and analysis of distributed protocols. In particular, distributed knowledge corresponds to knowledge that is “distributed” among the members of the group, while common knowledge corresponds to a fact being “publicly known.” The relationship between common knowledge and a variety of desirable actions in a distributed system is illustrated. Furthermore, it is shown that, formally speaking, in practical systems common knowledge cannot be attained. A number of weaker variants of common knowledge that are attainable in many cases of interest are introduced and investigated. <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/1998/file/5129a5ddcd0dcd755232baa04c231698-Paper.pdf"> Using Collective Intelligence to Route Internet Traffic </a>by David H. Wolpert, Kagan Turner, Jeremy Frank. NeurIPS, 1998. <a href="link">  </a> </summary> A COllective INtelligence (COIN) is a set of interacting reinforcement learning (RL) algorithms designed in an automated fashion so that their collective behavior optimizes a global utility function. We summarize the theory of COINs, then present experiments using that theory to design COINs to control internet traffic routing. These experiments indicate that COINs outperform all previously investigated RL-based, shortest path routing algorithms.  <br> - </details>

<details> <summary> <a href="link"> General Principles of Learning-Based Multi-Agent Systems </a>by David H. Wolpert, Kevin R. Wheeler, Kagan Turner. International Conference on Autonomous Agents, 1999. <a href="link">  </a> </summary> We consider the problem of how to design large decentralized multi-agent systems (MAS’s) in an automated fashion, with little or no hand-tuning. Our approach has each agent run a reinforcement learning algorithm. This converts the problem into one of how to automatically set/update the reward functions for each of the agents so that the global goal is achieved. In particular we don not want the agents to “work at cross-purposes” as far as the global goal is concerned. We use the term artificial COllective INtelligence (COIN) to refer to systems that embody solutions to this problem. In this paper we present a summary of a mathematical framework for COINS. We then investigate the real-world applicability of the core concepts of that framework via two computer experiments: we show that our COINS perform near optimally in a difficult variant of Arthur’s bar problem [l] (and in psrtitular avoid the tragedy of the commons for that problem), and we also illustrate optimal performance for our COINS in the leader-follower problem.  <br> - </details>

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/wolpert01a.pdf"> Optimal Payoff Functions for Members of Collectives </a>by David H. Wolpert, Kagan Tumer. Advances in Complex Systems, 2001. <a href="link">  </a> </summary> We consider the problem of designing (perhaps massively distributed) collectives of computational processes to maximize a provided "world utility" function. We consider this problem when the behavior of each process in the collective can be cast as striving to maximize its own payoff utility function. For such cases the central design issue is how to initialize/update those payoff utility functions of the individual processes so as to induce behavior of the entire collective having good values of the world utility. Traditional "team game" approaches to this problem simply assign to each process the world utility as its payoff utility function. In previous work we used the "Collective Intelligence" (COIN) framework to derive a better choice of payoff utility functions, one that results in world utility performance up to orders of magnitude superior to that ensuing from the use of the team game utility. In this paper, we extend these results using a novel mathematical framework. Under that new framework we review the derivation of the general class of payoff utility functions that both (i) are easy for the individual processes to try to maximize, and (ii) have the property that if good values of them are achieved, then we are assured a high value of world utility. These are the "Aristocrat Utility" and a new variant of the "Wonderful Life Utility" that was introduced in the previous COIN work. We demonstrate experimentally that using these new utility functions can result in significantly improved performance over that of previously investigated COIN payoff utilities, over and above those previous utilities' superiority to the conventional team game utility. These results also illustrate the substantial superiority of these payoff functions to perhaps the most natural version of the economics technique of "endogenizing externalities." <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/3-540-45656-2_30.pdf"> Phe-Q: A Pheromone Based Q-Learning </a>by Ndedi Monekosso, Paolo Remagnino. LNAI, 2001. <a href="link">  </a> </summary> Biological systems have often provided inspiration for the design of artificial systems. On such example of a natural system that has inspired researchers is the ant colony. In this paper an algorithm for multi-agent reinforcement learning, a modified Q-learning, is proposed. The algorithm is inspired by the natural behaviour of ants, which deposit pheromones in the environment to communicate. The benefit besides simulating ant behaviour in a colony is to design complex multiagent systems. Complex behaviour can emerge from relatively simple interacting agents. The proposed Q-learning update equation includes a belief factor. The belief factor reflects the confidence the agent has in the pheromone detected in its environment. Agents communicate implicitly to co-operate in learning to solve a path-planning problem. The results indicate that combining synthetic pheromone with standard Q-learning speeds up the learning process. It will be shown that the agents can be biased towards a preferred solution by adjusting the pheromone deposit and evaporation rates. <br> - </details>

<details> <summary> <a href="http://faculty.washington.edu/paymana/swarm/bonabeau03-etcon.pdf"> Swarm Intelligence </a>by Eric Bonabeau. Emergent technology conference, 2003. <a href="link">  </a> </summary> Ants, Bees or Termites - all social insects - show impressive collective problem-solving capabilities. Properties associated with their group behaviour like self-organisation, robustness and flexibility are seen as characteristics that artificial systems for optimisation, control or task execution should exhibit. In the last decade, diverse efforts have been made to take social insects as an example and develop algorithms inspired by their strictly self-organised behaviour. These approaches can be subsumed under the concept of "Swarm Intelligence". <br> - </details>

<details> <summary> <a href="http://nozdr.ru/data/media/biblio/kolxoz/Cs/CsLn/L/Learning%20and%20Adaption%20in%20Multi-Agent%20Systems,%201%20conf.,%20LAMAS%202005(LNCS3898,%20Springer,%202006)(ISBN%203540330534)(224s).pdf#page=215"> Multi-type ACO for Light Path Protection </a>by Peter Vrancx, Ann Nowe, Kris Steenhaut. LAMAS, 2005. <a href="link">  </a> </summary> Backup trees (BTs) are a promising approach to network protection in optical networks. BTs allow us to protect a group of working paths against single network failures, while reserving only a minimum amount of network capacity for backup purposes. The process of constructing a set of working paths together with a backup tree is computationally very expensive, however. In this paper we propose a multi-agent approach based on ant colony optimization (ACO) for solving this problem. ACO algorithms use a set of relatively simple agents that model the behavior of real ants. In our algorithm multiple types of ants are used. Ants of the same type collaborate, but are in competition with the ants of other types. The idea is to let each type find a path in the network that is disjoint with that of other types. We also demonstrate a preliminary version of this algorithm in a series of simple experiments. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-030-77584-1.pdf"> A Primer on Multiple Intelligences </a>by Matthew N. O. Sadiku, Sarhan M. Musa. Springer, 2021. <a href="link">  </a> </summary> The concept of intelligence is interesting and is discussed every day. It has been central to the feld of psychology and remains hotly debated. It was frst introduced by Francis Galton in 1885. Intelligence or cognitive development is a biopsychological potential to process information that can be activated to solve problems. It is the capacity of the individual to act purposefully, to think rationally, and to deal effectively with his environment. The characteristic of intelligence is usually attributed to humans. Traditionally, intelligence is often regarded as a person’s intellectual capacity; something one is born with and that cannot be changed. It is the ability to learn, to proft from experience, the ability to adapt to new situations, and the ability to solve problems. It was generally believed that intelligence was a single entity that was inherited. It is fxed and can be measured. Historically, intelligence has been measured using the IQ test which is a general measure of one’s cognitive function. Other views of intelligence have emerged in recent years. Today, an increasing number of researchers, believe that there exists a multiple of intelligences, quite independent of each other. People have a unique blend of intelligences. The concept of multiple intelligences represents an effort to reframe the traditional conception of intelligence. Professor Howard Gardner at Harvard’s Graduate School of Education argued that there are better or alternative ways to measure intelligence than standard IQ tests. He frst proposed nine intelligences and insisted that all people are born with one or more intelligences. He believed human intelligence was multidimensional. This spectrum of intelligence indicates that people can be smart in a number of different ways. It implies that we are all intelligent in different ways, but there is always a primary, or more dominant, intelligence in each of us. Multiple intelligences theory states that everyone has all intelligences at varying degrees of profciency, while most will experience more dominant intelligences that impact the way they learn and interact with others. Each type of intelligence presents a distinct component of our total competence. There is no clear consensus about how many intelligences there are. In this book, we consider 19 multiple intelligences. The intelligences are possessed by everyone and vary in degree of development within each individual. <br> - </details>

<details> <summary> <a href="https://rohitvaish.in/data/Papers/[Satterthwaite]%20StrategyProofness%20and%20Arrow's%20Conditions.pdf"> Strategy-proofness and Arrow's conditions: Existence and correspondence theorems for voting procedures and social welfare functions </a> by Satterthwaite, Mark A.. Journal of Economic Theory, 1975. <a href="link">  </a> </summary> Consider a committee which must select one alternative from a set of three or more alternatives. Committee members each cast a ballot which the voting procedure counts. The voting procedure is strategy-proof if it always induces every committee member to cast a ballot revealing his preference. I prove three theorems. First, every strategy-proof voting procedure is dictatorial. Second, this paper’s strategy-proofness condition for voting procedures corresponds to Arrow’s rationality, independence of irrelevant alternatives, nonnegative response, and citizens’ sovereignty conditions for social welfare functions. Third, Arrow’s general possibility theorem is proven in a new manner. 1. INTR~OUOTI~N <br> - </details>

<details> <summary> <a href="https://www.jstor.org/stable/1914033"> AGGREGATION OF PREFERENCES WITH VARIABLE ELECTORATE </a> by Smith, John H.. Econometrica, 1973. <a href="link">  </a> </summary> IN THIS PAPER we consider procedures for going from several individual preferences among several alternatives, called candidates, to something which may be called a collective preference. The individual preferences take the form of (total) orderings of the alternatives, and the collective preference is to take the form of a (total) weak ordering (i.e., ties allowed). We consider certain properties which seem desirable in such systems and investigate which systems have these properties. The point of view taken here differs from that of other work in this area (e.g., [1, 2, 3, 4]) chiefly in asking that the procedure work for all possible sizes of the voting population, rather than for a fixed population, given in advance. This permits us to require, for example, that if each of two bodies of voters prefers candidate A to candidate B under a given procedure, then the combination of these bodies should prefer A to B under the same procedure. In Section 1 we give the formal definitions of an aggregation procedure and discuss certain desirable features, namely "neutrality" (treats candidates symmetrically), "separability" (the condition mentioned above), "monotonicity," and an "Archimedean" property which says, roughly, that a sufficiently large body with a given distribution of preferences can impose its will on any body of fixed size. In Section 2 we introduce certain procedures: "point systems" and "generalized point systems" (roughly, point systems allowing "infinitesimal points"), which are neutral and separable. They are monotonic if and only if the points are arranged in the "natural" order, and the point systems are, in addition, Archimedean. In Section 3 we prove a converse, namely that any neutral and separable procedure can be realized by a generalized point system and, if it is Archimedean, by a point system. This part requires some familiarity with the notions of least upper bound of a set of real numbers and bases of vector spaces. In Section 4 (which is largely independent of Section 3), we consider "point runoff" systems which use point systems in a succession of eliminations. Such systems are neither separable nor monotonic but do satisfy some very weak separability and monotonicity conditions. While these probably do not characterize point runoff systems, we know of no other systems satisfying them. <br> - </details>

<br/>

### Communication

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/BFb0017034.pdf"> Learning in Distributed Systems and Multiagent Environments </a>by P. Brazdil, M. Gams, S. Sian, L. Torgo, W. van de Velde . LNAI, 1991. <a href="link">  </a> </summary> The paper begins with the discussion on why we should be concerned with machine learning in the context of distributed AI. The rest of the paper is dedicated to various problems of multiagent learning. First, a common framework for comparing different existing systems is presented. It is pointed out that it is useful to distinguish when the individual agents communicate. Some systems communicate during the learning phase, others during the problem solving phase, for example. It is also important to consider how, that is in what language, the communication is established. The paper analyses several systems in this framework. Particular attention is paid to previous work done by the authors in this area. The paper covers use of redundant knowledge, knowledge integration, evaluation of hypothesis by a community of agents and resolution of language differences between agents. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Michael-Littman/publication/228786266_Altruism_in_the_evolution_of_communication/links/02e7e52c564728c081000000/Altruism-in-the-evolution-of-communication.pdf"> Altruism in the Evolution of Communication </a>by David H. Ackley, Michael L. Littman. Workshop on the Synthesis and Simulation of Living Systems, 1994. <a href="link">  </a> </summary> Computer models of evolutionary phenomena often assume that the fitness of an individual can be evaluated in isolation, but effective communication requires that individuals interact. Existing models directly reward speakers for improved behavior on the part of the listeners so that, essentially, effective communication is fitness. We present new models in which, even though "speaking truthfully" provides no tangible benefit to the speaker, effective communication nonetheless evolves. A large population is spatially distributed so that "communication range" approximately correlates with "breeding range" so that most of the time "you'll be talking to family" allowing kin selection to encourage the emergence of communication. However, the emergence of altruistic communication also creates niches that can be exploited by "information parasites." The new models display complex and subtle long-tenD dynamics as the global implications of such social dilemmas are played out <br> - </details>

<details> <summary> <a href="https://langev.com/pdf/quinn01evolvingCommunication.pdf"> Evolving Communication without Dedicated Communication Channels </a>by Matt Quinn. ECAL, 2001. <a href="link">  </a> </summary> Artificial Life models have consistently implemented communication as an exchange of signals over dedicated and functionally isolated channels. I argue that such a feature prevents models from providing a satisfactory account of the origins of communication and present a model in which there are no dedicated channels. Agents controlled by neural networks and equipped with proximity sensors and wheels are presented with a co-ordinated movement task. It is observed that functional, but non-communicative, behaviours which evolve in the early stages of the simulation both make possible, and form the basis of, the communicative behaviour which subsequently evolves. <br> - </details>

<details> <summary> <a href="nan"> Communication between Rational Agents </a> by Rabin, Matthew. Journal of Economic Theory, 1990. <a href="link">  </a> </summary> Conventional game-theoretic solution concepts  never  guarantee meaningful communication in cheap-talk games. I define a solution concept which does guarantee communication in some games. I assume full rationality without imposing equilibrium conditions, but add a natural behavioral assumption about how agents use language: agents have a propensity to speak the truth and to believe others speak the truth, but use the game's strategic incentives to check whether such behavior and beliefs are rational. I also define and prove the existence of an equilibrium version of the concept, and present examples where its predictions seem more natural than Farrell's neologism-proof equilibrium. <br> - </details>

<details> <summary> <a href="https://www.reidgsmith.com/The_Contract_Net_Protocol_Dec-1980.pdf"> The Contract Net Protocol: High-Level Communication and Control in a Distributed Problem Solver </a> by Smith. IEEE Transactions on Computers, 1980. <a href="link">  </a> </summary> The contract net protocol has been developed to specify problem-solving communication and control for nodes in a distributed problem solver. Task distribution is affected by a negotiation process, a discussion carried on between nodes with tasks to be executed and nodes that may be able to execute those tasks. <br> - </details>
<details> <summary> <a href="https://www.researchgate.net/profile/John-Tsitsiklis/publication/228341859_Communication_requirements_of_VCG-like_mechanisms_in_convex_environments/links/543369c00cf225bddcc9a77f/Communication-requirements-of-VCG-like-mechanisms-in-convex-environments.pdf"> Communication requirements of VCG-like mechanisms in convex environments </a>by Johari R, Tsitsiklis JN. In Proceedings of Allerton Conference, 2005. <a href="link">  </a> </summary> We develop VCG-like mechanisms for resource allocation environments where the players have concave utility functions, and the resource constraints can be represented through convex inequality constraints; multicommodity flow problems are a prime example. Unlike VCG mechanisms that require each player to communicate an entire utility function, our mechanisms only require each player to communicate a single scalar quantity. Despite the limited communication, we establish the existence of an efficient Nash equilibrium. Under some further assumptions, we also establish that all Nash equilibria are efficient. Our approach defines an entire family of resource allocation mechanisms; as a special case, we recover the class of mechanisms recently introduced by Yang and Hajek for a single resource. <br> - </details>

<br/>

### Convergent Learning

<details> <summary> <a href="http://www.dklevine.com/archive/refs4237.pdf"> Sequential equilibria </a>by Kreps DM, Wilson R. Econometrica: Journal of the Econometric Society, 1982. <a href="link">  </a> </summary> We propose a new criterion for equilibria of extensive games, in the spirit of Selten's perfectness criteria. This criterion requires that players' strategies be sequentially rational: every decision must be part of an optimal strategy for the remainder of the game. This entails specification of players' beliefs concerning how the game has evolved for each information set, including information sets off the equilibrium path. The properties of sequential equilibria are developed; in particular, we study the topological structure of the set of sequential equilibria. The connections with Selten's trembling-hand perfect equilibria are given. <br> - </details>

<details> <summary> <a href="http://www.dklevine.com/archive/refs4415.pdf"> Learning Mixed Equilibria </a> by Drew Fundenberg, David M. Kreps. Journal of Games and Economic Behvaior, 1993.  </summary> We study learning processes for finite strategic-form games, in which players use the history of past play to forecast play in the current period. In a generalization of fictitious play, we assume only that players asymptotically choose best responses to the historical frequencies of opponents′ past play. This implies that if the stage-game strategies converge, the limit is a Nash equilibrium. In the basic model, plays seems unlikely to converge to a mixed-strategy equilibrium, but such convergence is natural when the stage game is perturbed in the manner of Harsanyi′s purification theorem.  <br> - </details>

<details> <summary> <a href="http://www.eecs.harvard.edu/cs286r/courses/spring06/papers/kalailehrer93.pdf"> Rational learning leads to nash equilibrium </a>by Ehud Kalai, Ehud Lehrer. Econometrica, 1993. <a href="link">  </a> </summary> Each of n players, in an infinitely repeated game, starts with subjective beliefs about his opponents' strategies. If the individual beliefs are compatible with the true strategies chosen, then Bayesian updating will lead in the long run to accurate prediction of the future play of the game. It follows that individual players, who know their own payoff matrices and choose strategies to maximize their expected utility, must eventually play according to a Nash equilibrium of the repeated game. An immediate corollary is that, when playing a Harsanyi-Nash equilibrium of a repeated game of incomplete information about opponents' payoff matrices, players will eventually play a Nash equilibrium of the real game, as if they had complete information <br> - </details>

<details> <summary> <a href="https://cs.brown.edu/~mlittman/papers/ml96-generalized.pdf"> A generalized reinforcement-learning model: Convergence and applications </a>by Michael L. Littman, C. Szepesvari. ICML, 1996. <a href="link">  </a> </summary> Reinforcement learning is the process by which an autonomous agent uses its experience interacting with an environment to improve its behavior. The Markov decision process (MDP) model is a popular way of formalizing the reinforcement learning problem but it is by no means the only way. In this paper we show how many of the important theoretical results concerning reinforcement learning in MDPs extend to a generalized MDP model that includes MDPs two-player games and MDPs under a worst-case optimality criterion as special cases. The basis of this extension is a stochastic approximation theorem that reduces asynchronous convergence to synchronous con <br> - </details>

<details> <summary> <a href="http://users.ece.utexas.edu/~cmcaram/EE381V_2012F/elastic.pdf"> Charging and rate control for elastic traffic </a>by Kelly F. European transactions on Telecommunications, 1997. <a href="link">  </a> </summary> This paper addresses the issues of charging, rate control and routing for a communication network carrying elastic traffic, such as an ATM network offering an available bit rate service. A model is described from which max-min fairness of rates emerges as a limiting special case; more generally, the charges users are prepared to pay influence their allocated rates. In the preferred version of the model, a user chooses the charge per unit time that the user will pay; thereafter the user’s rate is determined by the network according to a proportional fairness criterion applied to the rate per unit charge. A system optimum is achieved when users’ choices of charges and the network’s choice of allocated rates are in equilibrium. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf"> The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems </a> by Caroline Claus, Craig Boutilier. AAAI, 1998.  </summary> Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.  <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/A:1007514623589.pdf"> Conjectural Equilibrium in Multiagent Learning </a>by MICHAEL P. WELLMAN, JUNLING HU. Machine Learning, 1998. <a href="link">  </a> </summary> Learning in a multiagent environment is complicated by the fact that as other agents learn, the environment effectively changes. Moreover, other agents’ actions are often not directly observable, and the actions taken by the learning agent can strongly bias which range of behaviors are encountered. We define the concept of a conjectural equilibrium, where all agents’ expectations are realized, and each agent responds optimally to its expectations. We present a generic multiagent exchange situation, in which competitive behavior constitutes a conjectural equilibrium. We then introduce an agent that executes a more sophisticated strategic learning strategy, building a model of the response of other agents. We find that the system reliably converges to a conjectural equilibrium, but that the final result achieved is highly sensitive to initial belief. In essence, the strategic learner’s actions tend to fulfill its expectations. Depending on the starting point, the agent may be better or worse off than had it not attempted to learn a model of the other agents at all. <br> - </details>

<details> <summary> <a href="https://econweb.ucsd.edu/~jandreon/Econ264/papers/Erev%20Roth%20AER%201998.pdf"> Predicting how people play games: reinforcement leaning in experimental games with unique, mixed strategy equilibria </a> by Ido Erev, Alvin E. Roth. The American Economic Review, 1998. </summary> We examine learning in all experiments we could locate involving 100 periods or more of games with a unique equilibrium in mixed strategies, and in a new experiment. We study both the ex post ("best fit") descriptive power of learning models, and their ex ante predictive power, by simulating each experiment using parameters estimated from the other experiments. Even a one-parameter reinforcement learning model robustly outperforms the equilibrium predictions. Predictive power is improved by adding "forgetting" and "experimentation," or by allowing greater rationality as in probabilistic fictitious play. Implications for developing a low-rationality, cognitive game theory are discussed. <br> - </details>

<details> <summary> <a href="https://cse.hkust.edu.hk/mjg_lib/Classes/COMP670O/Papers/y37e5ne8bbafjb5r.pdf"> Worst-case equilibria </a>by Koutsoupias E, Papadimitriou C. In Annual symposium on theoretical aspects of computer science, 1999. <a href="link">  </a> </summary> In a system in which noncooperative agents share a common resource, we propose the ratio between the worst possible Nash equilibrium and the social optimum as a measure of the effectiveness of the system. Deriving upper and lower bounds for this ratio in a model in which several agents share a very simple network leads to some interesting mathematics, results, and open problems. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Satinder-Singh-3/publication/234140138_Nash_Convergence_of_Gradient_Dynamics_in_Iterated_General-Sum_Games/links/55ad059508ae98e661a2ade1/Nash-Convergence-of-Gradient-Dynamics-in-Iterated-General-Sum-Games.pdf"> Nash Convergence of Gradient Dynamics in General-Sum Games </a>by Satinder Singh, Michael Kearns, Yishay Mansour. UAI, 2000. <a href="link">  </a> </summary> Multi-agent games are becoming an increasingly prevalent formalism for the study of electronic commerce and auctions. The speed at which transactions can take place and the growing complexity of electronic marketplaces makes the study of computationally simple agents an appealing direction. In this work, we analyze the behavior of agents that incrementally adapt their strategy through gradient ascent on expected payoff, in the simple setting of two-player, two-action, iterated general-sum games, and present a surprising result. We show that either the agents will converge to a Nash equilibrium, or if the strategies themselves do not converge, then their average payoffs will nevertheless converge to the payoffs of a Nash equilibrium <br> - </details>

<details> <summary> <a href="https://www.ma.imperial.ac.uk/~dturaev/Hart0.pdf"> A simple adaptive procedure leading to correlated equilibrium </a>by Hart S, Mas‐Colell A. Econometrica, 2000. <a href="link">  </a> </summary> We propose a new and simple adaptive procedure for playing a game: ‘‘regret-matching.’’ In this procedure, players may depart from their current play with probabilities that are proportional to measures of regret for not having used other strategies in the past. It is shown that our adaptive procedure guarantees that, with probability one, the empirical distributions of play converge to the set of correlated equilibria of the game. <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~mmv/papers/01ijcai-mike.pdf"> Rational and Convergent Learning in Stochastic Games </a>by Michael Bowling, Manuela Veloso. 2001. </summary> This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we briefly overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm, WoLF policy hillclimbing, that is based on a simple principle: “learn quickly while losing, slowly while winning.” The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.  <br> - </details>

<details> <summary> <a href="https://dspace.mit.edu/bitstream/handle/1721.1/3688/CS023.pdf?sequence=2&isAllowed=y"> Playing is believing: the role of beliefs in multi-agent learning </a> by Yu-Han Chang, Leslie Pack Kaelbling. NeurIPS, 2001. </summary> We propose a new classification for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classification, we review the optimality of existing algorithms and discuss some insights that can be gained. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the long-run against fair opponents. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/3-540-44795-4_33.pdf"> Social agents playing a periodical policy </a>by Ann Now´e, Johan Parent, and Katja Verbeeck. ECML, 2001. <a href="link">  </a> </summary> Coordination is an important issue in multiagent systems. Within the stochastic game framework this problem translates to policy learning in a joint action space. This technique however suffers some important drawbacks like the assumption of the existence of a unique Nash equilibrium and synchronicity, the need for central control, the cost of communication, etc. Moreover in general sum games it is not always clear which policies should be learned. Playing pure Nash equilibria is often unfair to at least one of the players, while playing a mixed strategy doesn’t give any guarantee for coordination and usually results in a sub-optimal payoff for all agents. In this work we show the usefulness of periodical policies, which arise as a side effect of the fairness conditions used by the agents. We are interested in games which assume competition between the players, but where the overall performance can only be as good as the performance of the poorest player. Players are social distributed reinforcement learners, who have to learn to equalize their payoff. Our approach is illustrated on synchronous one-step games as well as on asynchronous job scheduling games. <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/2002/file/f8e59f4b2fe7c5705bf878bbd494ccdf-Paper.pdf"> Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games </a>by Xiaofeng Wang, Tuomas Sandholm. NeurIPS, 2002. <a href="link">  </a> </summary> Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conflicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the first algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm’s parameters are easy to set to meet the convergence conditions. <br> - </details>

<details> <summary> <a href="https://www.sciencedirect.com/science/article/pii/S0004370202001212/pdf?crasolve=1&r=76e929ed589406db&ts=1669198737508&rtype=https&vrr=UKN&redir=UKN&redir_fr=UKN&redir_arc=UKN&vhash=UKN&host=d3d3LnNjaWVuY2VkaXJlY3QuY29t&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&iv=7f135f0e0222bf00fdd4304ee2a1ae9f&token=36303334373233626432363262646163646435326538633139323566646439336462623432323537373965623737383261653335393965373534323332333532616365373333346337623830666338373735616139396537333962383a333465363031303533343635303962346333666262613232&text=9e36f3db92abeb589d5da8a52095e27f9670f9d838b2ad4862938be7f725337583a19749fa0cc538c097a5baf504157a6cefe332eae16f6f272bc064381f4524219bd0363a96bdf58d6e26627fa730ff6cec4c9ad6d794be01e48ff4e223857fa5f9011bcb6a38bcfb560984c7bef5f934b93e74793f2a1547243e01e692b606979614b88b57ba5f4833e30854615a2d02124eae78f47ac7001997788775368d12924e437912cf828d5431f7a3e769024e7459bbd1997839f8b098ba49c013f6876d4d9d1a2b78a1705aeadb49566b37ec1f3f3a816efba12093a43af51508f89b60bbbc41bb6d3c2ca20e5dd140af278c1436b5ea7c507e8053624890df9b6f12f39a3dbf7cac42ab4dca5b8356300411cf58a5c91c81d91bd4811d65cd14aa4b3959746ebaac377a8115b4801ee33a&original=3f6d64353d3236333032333763363663366136323039316333326433373234316337393064267069643d312d73322e302d53303030343337303230323030313231322d6d61696e2e706466"> Multiagent learning using a variable learning rate </a>by Michael Bowling, Manuela Veloso. Artificial Intelligence, 2002. <a href="link">  </a> </summary> Learning to act in a multiagent environment is a difficult problem since the normal definition of an optimal policy no longer applies. The optimal policy at any moment depends on the policies of the other agents. This creates a situation of learning a moving target. Previous learning algorithms have one of two shortcomings depending on their approach. They either converge to a policy that may not be optimal against the specific opponents’ policies, or they may not converge at all. In this article we examine this learning problem in the framework of stochastic games. We look at a number of previous learning algorithms showing how they fail at one of the above criteria. We then contribute a new reinforcement learning technique using a variable learning rate to overcome these shortcomings. Specifically, we introduce the WoLF principle, “Win or Learn Fast”, for varying the learning rate. We examine this technique theoretically, proving convergence in self-play on a restricted class of iterated matrix games. We also present empirical results on a variety of more general stochastic games, in situations of self-play and otherwise, demonstrating the wide applicability of this method <br> - </details>

<details> <summary> <a href="https://papers.nips.cc/paper/2002/file/0d73a25092e5c1c9769a9f3255caa65a-Paper.pdf"> Efficient learning equilibrium </a>by Ronen I. Brafman, Moshe Tennenholtz. NeurIPS, 2002. <a href="link">  </a> </summary> We introduce efficient learning equilibrium (ELE), a normative approach to learning in noncooperative settings. In ELE, the learning algorithms themselves are required to be in equilibrium. In addition, the learning algorithms must arrive at a desired value after polynomial time, and a deviation from the prescribed ELE becomes irrational after polynomial time. We prove the existence of an ELE (where the desired value is the expected payoff in a Nash equilibrium) and of a Pareto-ELE (where the objective is the maximization of social surplus) in repeated games with perfect monitoring. We also show that an ELE does not always exist in the imperfect monitoring case. Finally, we discuss the extension of these results to general-sum stochastic games <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/2003/file/e71e5cd119bbc5797164fb0cd7fd94a4-Paper.pdf"> Extending Q-Learning to General Adaptive Multi-Agent Systems </a>by Gerald Tesauro. NeurIPS, 2003. <a href="link">  </a> </summary> Recent multi-agent extensions of Q-Learning require knowledge of other agents’ payoffs and Q-functions, and assume game-theoretic play at all times by all other agents. This paper proposes a fundamentally different approach, dubbed “Hyper-Q” Learning, in which values of mixed strategies rather than base actions are learned, and in which other agents’ strategies are estimated from observed actions via Bayesian inference. Hyper-Q may be effective against many different types of adaptive agents, even if they are persistently dynamic. Against certain broad categories of adaptation, it is argued that Hyper-Q may converge to exact optimal time-varying policies. In tests using Rock-Paper-Scissors, Hyper-Q learns to significantly exploit an Infinitesimal Gradient Ascent (IGA) player, as well as a Policy Hill Climber (PHC) player. Preliminary analysis of Hyper-Q against itself is also presented. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Spring/2001/SS-01-03/SS01-03-006.pdf"> Multi-agent influence diagrams for representing and solving games </a>by Koller D, Milch B. Games and economic behavior, 2003. <a href="link">  </a> </summary> Game theory provides a theoretical basis for defining rational behavior in multi-agent scenarios. However, the computational complexity of finding equilibria in large games has limited game theory’s practical applications. In this paper, we explore the application of structured probabilistic models to multi-agent scenarios. We define multi-agent influence diagrams (MAIDs), which represent games in a way that allows us to take advantage of independence relationships among variables. This representation allows us to define a notion of strategic relevance: D’ is strategically relevant to D if, to optimize the decision rule at D, the decision maker needs to know the decision rule at D’. We provide a sound and complete graphical criterion for determining strategic relevance. We then show how strategic relevance, which is made explicit by the MAID structure, can be exploited in algorithms for computing equilibria to obtain exponential savings in certain classes of games. <br> - </details>

<details> <summary> <a href="https://www.jair.org/index.php/jair/article/view/10393/24892"> Existence of Multiagent Equilibria with Limited Agents </a>by Michael Bowling, Manuela Veloso. JAIR, 2004. <a href="link">  </a> </summary> Multiagent learning is a necessary yet challenging problem as multiagent systems become moreprevalent and environments become more dynamic. Much of the groundbreaking work in this areadraws on notable results from game theory, in particular, the concept of Nash equilibria.  Learnersthat directly learn an equilibrium obviously rely on their existence.  Learners that instead seek toplay optimally with respect to the other players also depend upon equilibria since equilibria arefixed points for learning.  From another perspective, agents with limitations are real and common.These may be undesired physical limitations as well as self-imposed rational limitations, such asabstraction and approximation techniques, used to make learning tractable. This article explores theinteractions of these two important concepts:  equilibria and limitations in learning.  We introducethe question of whether equilibria continue to exist when agents have limitations.  We look at thegeneral effects limitations can have on agent behavior, and define a natural extension of equilibriathat accounts for these limitations. Using this formalization, we make three major contributions: (i)a counterexample for the general existence of equilibria with limitations, (ii) sufficient conditionson limitations that preserve their existence, (iii) three general classes of games and limitations thatsatisfy  these  conditions.   We  then  present  empirical  results  from  a  specific  multiagent  learningalgorithm applied to a specific instance of limited agents.  These results demonstrate that learningwith limitations is feasible, when the conditions outlined by our theoretical analysis hold. <br> - </details>

<details> <summary> <a href="https://ww2.odu.edu/~jsokolow/projects/files/Best-Response%20Multiagent%20Learning%20in%20Non-Stationary%20Environments.pdf"> Best-Response Multiagent Learning in Non-Stationary Environments </a>by Michael Weinberg, Jeffrey S. Rosenschein. AAMAS, 2004. <a href="link">  </a> </summary>
This paper investigates a relatively new direction in Multiagent Reinforcement Learning. Most multiagent learning techniques focus on Nash equilibria as elements of both the learning algorithm and its evaluation criteria. In contrast, we propose a multiagent learning algorithm that is optimal in the sense of finding a best-response policy, rather than in reaching an equilibrium. We present the first learning algorithm that is provably optimal against restricted classes of non-stationary opponents. The algorithm infers an accurate model of the opponent’s non-stationary strategy, and simultaneously creates a best-response policy against that strategy. Our learning algorithm works within the very general framework of N-player, general-sum stochastic games, and learns both the game structure and its associated optimal policy <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2006/AAAI06-108.pdf"> A polynomial-time algorithm for Action-Graph Games </a>by Jiang AX, Leyton-Brown K. In PROCEEDINGS OF THE NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, 2006. <a href="link">  </a> </summary> Action-Graph Games (AGGs) (Bhat & Leyton-Brown 2004) are a fully expressive game representation which can compactly express strict and context-specific independence and anonymity structure in players’ utility functions. We present an efficient algorithm for computing expected payoffs under mixed strategy profiles. This algorithm runs in time polynomial in the size of the AGG representation (which is itself polynomial in the number of players when the in-degree of the action graph is bounded). We also present an extension to the AGG representation which allows us to compactly represent a wider variety of structured utility functions. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/s10994-006-0143-1.pdf"> AWESOME: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents </a>by Vincent Conitzer, Tuomas Sandholm. Machine Learning, 2007. <a href="link">  </a> </summary> Two minimal requirements for a satisfactory multiagent learning algorithm are that it 1. learns to play optimally against stationary opponents and 2. converges to a Nash equilibrium in self-play. The previous algorithm that has come closest, WoLF-IGA, has been proven to have these two properties in 2-player 2-action (repeated) games—assuming that the opponent’s mixed strategy is observable. Another algorithm, ReDVaLeR (which was introduced after the algorithm described in this paper), achieves the two properties in games with arbitrary numbers of actions and players, but still requires that the opponents’ mixed strategies are observable. In this paper we present AWESOME, the first algorithm that is guaranteed to have the two properties in games with arbitrary numbers of actions and players. It is still the only algorithm that does so while only relying on observing the other players’ actual actions (not their mixed strategies). It also learns to play optimally against opponents that eventually become stationary. The basic idea behind AWESOME (Adapt When Everybody is Stationary, Otherwise Move to Equilibrium) is to try to adapt to the others’ strategies when they appear stationary, but otherwise to retreat to a precomputed equilibrium strategy. We provide experimental results that suggest that AWESOME converges fast in practice. The techniques used to prove the properties of AWESOME are fundamentally different from those used for previous algorithms, and may help in analyzing future multiagent learning algorithms as well. <br> - </details>

<details> <summary> <a href="https://www.math.cmu.edu/~af1p/Teaching/INFONET/Papers/EconomicModels/Congestion1.pdf"> A class of games possessing pure-strategy Nash equilibria </a> by Rosenthal, Robert; Rosenthal, Robert W.. International Journal of Game Theory, 1973. <a href="link">  </a> </summary> A class of noncooperative games (of interest in certain applications) is described. Each game in the class is shown to possess at least one Nash equilibrium in pure strategies. <br> - </details>

<details> <summary> <a href="https://timroughgarden.org/papers/ncg.pdf"> Bounding the inefficiency of equilibria in nonatomic congestion games </a> by Roughgarden, Tim; Tardos, Éva. Games and Economic Behavior, 2004. <a href="link">  </a> </summary> Abstract   Equilibria in noncooperative games are typically inefficient, as illustrated by the Prisoner's Dilemma. In this paper, we quantify this inefficiency by comparing the payoffs of equilibria to the payoffs of a “best possible” outcome. We study a nonatomic version of the congestion games defined by Rosenthal [Int. J. Game Theory 2 (1973) 65], and identify games in which equilibria are  approximately optimal  in the sense that no other outcome achieves a significantly larger total payoff to the players—games in which optimization by individuals approximately optimizes the social good, in spite of the lack of coordination between players. Our results extend previous work on traffic routing games. <br> - </details>

<details> <summary> <a href="http://www.maths.lse.ac.uk/personal/stengel/TEXTE/focs04.pdf"> Exponentially many steps for finding a Nash equilibrium in a bimatrix game </a> by Savani, Rahul; von Stengel, B.. 45th Annual IEEE Symposium on Foundations of Computer Science, 2004. <a href="link">  </a> </summary> The Lemke-Howson algorithm is the classical algorithm for the problem NASH of finding one Nash equilibrium of a bimatrix game. It provides a constructive and elementary proof of existence of an equilibrium, by a typical "directed parity argument", which puts NASH into the complexity class PPAD. This paper presents a class of bimatrix games for which the Lemke-Howson algorithm takes, even in the best case, exponential time in the dimension d of the game, requiring /spl Omega/((/spl theta//sup 3/4/)/sup d/) many steps, where /spl theta/ is the golden ratio. The "parity argument" for NASH is thus explicitly shown to be inefficient. The games are constructed using pairs of dual cyclic polytopes with 2d suitably labeled facets in d-space. <br> - </details>

<details> <summary> <a href="https://nscpolteksby.ac.id/ebook/files/Ebook/Computer%20Engineering/Algorithmic%20Game%20Theory%20(2007)/11.%20Chapter%2010%20-%20Mechanism%20Design%20without%20Money.pdf"> Algorithmic Game Theory: Mechanism Design without Money </a> by Schummer, James; Vohra, Rakesh., 2007. <a href="link">  </a> </summary> Despite impossibility results on general domains, there are some classes of situations in which there exist interesting dominant-strategy mechanisms. While some of these situations (and the resulting mechanisms) involve the transfer of money, we examine some that do not. Specifically, we analyze problems where agents have single-peaked preferences over a one-dimensional “public” policy space; and problems where agents must match with each other. <br> - </details>

<details> <summary> <a href="https://www.rand.org/content/dam/rand/pubs/reports/2006/R1538.pdf"> A note on the Lemke-Howson algorithm </a> by Shapley, Lloyd S.., 1974. <a href="link">  </a> </summary> The Lemke-Howson algorithm for bimatrix games provides both an elementary proof of the existence of equilibrium points and an efficient computational method for finding at least one equilibrium point. The first half of this paper presents a geometrical view of the algorithm that makes its operation especially easy to visualize. Several illustrations are given, including Wilson’s example of “inaccessible” equilibrium points. The second half presents an orientation theory for the equilibrium points of (nondegenerate) bimatrix games and the Lemke-Howson paths that interconnect them; in particular, it is shown that there is always one more “negative” than “positive” equilibrium point. <br> - </details>

<details> <summary> <a href="https://www.scinapse.io/papers/2000604241#fullText"> Correlated equilibrium payoffs and public signalling in absorbing games </a> by Solan, Eilon; Vohra, Rakesh. International Journal of Game Theory, 2002. <a href="link">  </a> </summary> An absorbing game is a repeated game where some action combinations are absorbing, in the sense that whenever they are played, there is a positive probability that the game terminates, and the players receive some terminal payoff at every future stage. We prove that every multi-player absorbing game admits a correlated equilibrium payoff. In other words, for every >0 there exists a probability distribution p over the space of pure strategy profiles that satisfies the following. With probability at least 1-, if a pure strategy profile is chosen according to p and each player is informed of his pure strategy, no player can profit more than in any sufficiently long game by deviating from the recommended strategy. <br> - </details>
<details> <summary> <a href="https://arxiv.org/pdf/1301.2281.pdf"> Graphical models for game theory </a>by Kearns M, Littman ML, Singh S. arXiv preprint arXiv:1301.2281, 2013. <a href="link">  </a> </summary> 
We introduce a compact graph-theoretic representation for multi-party game theory. Our main result is a provably correct and efficient algorithm for computing approximate Nash equilibria in one-stage games represented by trees or sparse graphs. <br> - </details>

<br/>

### Coordination

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1994/AAAI94-065.pdf"> Learning to coordinate without sharing information </a>by Sandip Sen, Mahendra Sekaran, John Hale. National Conference on Artificial Intelligence, 1994. <a href="link">  </a> </summary> Researchers in the field of Distributed Artificial Intelligence (DAI) have been developing efficient mechanisms to coordinate the activities of multiple autonomous agents. The need for coordination arises because agents have to share resources and expertise required to achieve their goals. Previous work in the area includes using sophisticated information exchange protocols, investigating heuristics for negotiation, and developing formal models of possibilities of conflict and cooperation among agent interests. In order to handle the changing requirements of continuous and dynamic environments, we propose learning as a means to provide additional possibilities for effective coordination. We use reinforcement learning techniques on a block pushing problem to show that agents can learn complimentary policies to follow a desired path without any knowledge about each other. We theoretically analyze and experimentally verify the effects of learning rate on system convergence, and demonstrate benefits of using learned coordination knowledge on similar problems. Reinforcement learning based coordination can be achieved in both cooperative and non-cooperative domains, and in domains with noisy communication channels and other stochastic characteristics that present a formidable challenge to using other coordination schemes. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/1302.3561.pdf"> Learning Conventions in Multiagent Stochastic Domains using Likelihood Estimates  </a>by Craig Boutilier. UAI, 1996. <a href="link">  </a> </summary> Fully cooperative multiagent systems-those in which agents share a joint utility model-is of special interest in AI. A key problem is that of ensuring that the actions of individual agents are coordinated, especially in settings where the agents are autonomous decision makers. We investigate approaches to learning coordinated strategies in stochastic domains where an agent's actions are not directly observable by others. Much recent work in game theory has adopted a Bayesian learning perspective to the more general problem of equilibrium selection, but tends to assume that actions can be observed. We discuss the special problems that arise when actions are not observable, including effects on rates of convergence, and the effect of action failure probabilities and asymmetries. We also use likelihood estimates as a means of generalizing fictitious play learning models in our setting. Finally, we propose the use of maximum likelihood as a means of removing strategies from consideration, with the aim of convergence to a conventional equilibrium, at which point learning and deliberation can cease. <br> - </details>

<details> <summary> <a href="https://www.cs.toronto.edu/~cebly/Papers/_download_/tark96.pdf"> Planning, Learning and Coordination in Multiagent Decision Processes </a>by Craig Boutilier. TARK, 1996. <a href="link">  </a> </summary> There has been a growing interest in AI in the design of multiagent systems, especially in multiagent cooperative planning. In this paper, we investigate the extent to which methods from single-agent planning and learning can be applied in multiagent settings. We survey a number of different techniques from decision-theoretic planning and reinforcement learning and describe a number of interesting issues that arise with regard to coordinating the policies of individual agents. To this end, we describe multiagent Markov decision processes as a general model in which to frame this discussion. These are special n-person cooperative games in which agents share the same utility function. We discuss coordination mechanisms based on imposed conventions (or social laws) as well as learning methods for coordination. Our focus is on the decomposition of sequential decision processes so that coordination can be learned (or imposed) locally, at the level of individual states. We also discuss the use of structured problem representations and their role in the generalization of learned conventions and in approximation. <br> - </details>

<details> <summary> <a href="http://www.ens.utulsa.edu/~sandip/research/web/papers/CoordinationLearner1995Sen.pdf"> Multiagent coordination with learning classifier systems </a>by Sandip Sen, Mahendra Sekaran. IJCAI, 1996. <a href="link">  </a> </summary> Researchers in the field of Distributed Artificial Intelligence (DAI) [Bond and Gasser, 1988] have developed a variety of agent coordination schemes under different assumptions about agent capabilities and relationships. Most of these schemes rely on shared knowledge or authority relationships between agents. These kinds of information may not be available or may be manipulated by malevolent agents. We have used reinforcement learning [Barto et al., 1989] as a coordination mechanism that imposes little cognitive burden on agents and does not suffer from the above-mentioned shortcomings [Sekaran and Sen, 1994; Sen et al., 1994]. In this paper, we evaluate a particular reinforcement learning methodology, a genetic algorithm based machine learning mechanism known as classifier systems [Holland, 1986] for developing action policies to optimize environmental feedback. Action policies that provide a mapping between perceptions and actions can be used by multiple agents to learn coordination strategies without having to rely on shared information. These agents are unaware of the capabilities of other agents and may or may not be cognizant of goals to achieve. We show that through repeated problem-solving experience, these agents can develop policies to maximize environmental feedback that can be interpreted as goal achievement from the viewpoint of an external observer. Experimental results from a couple of multiagent domains show that classifier systems can be more effective than the more widely used Q-learning scheme for multiagent coordination <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/publication/226319923_A_Distributed_Approach_for_Coordination_of_Traffic_Signal_Agents"> A game-theoretic approach to coordination of traffic signal agents </a>by Bazzan A. L. C. PhD thesis, 1997. <a href="link">  </a> </summary> Innovative control strategies are needed to cope with the increasing urban traffic chaos. In most cases, the currently used strategies are based on a central traffic-responsive control system which can be demanding to implement and maintain. Therefore, a functional and spatial decentralization is desired. For this purpose, distributed artificial intelligence and multi-agent systems have come out with a series of techniques which allow coordination and cooperation. However, in many cases these are reached by means of communication and centrally controlled coordination processes, giving little room for decentralized management. Consequently, there is a lack of decision-support tools at managerial level (traffic control centers) capable of dealing with decentralized policies of control and actually profiting from them. In the present work a coordination concept is used, which overcomes some disadvantages of the existing methods. This concept makes use of techniques of evolutionary game theory: intersections in an arterial are modeled as individually-motivated agents or players taking part in a dynamic process in which not only their own local goals but also a global one has to be taken into account. The role of the traffic manager is facilitated since s/he has to deal only with tactical ones, leaving the operational issues to the agents. Thus the system ultimately provides support for the traffic manager to decide on traffic control policies. Some application in traffic scenarios are discussed in order to evaluate the feasibility of transferring the responsibility of traffic signal coordination to agents. The results show different performances of the decentralized coordination process in different scenarios (e.g. the flow of vehicles is nearly equal in both opposing directions, one direction has a clearly higher flow, etc.). Therefore, the task of the manager is facilitate once s/he recognizes the scenario and acts accordingly. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/BFb0055027.pdf"> A Framework for Coordination and Learning Among Teams of Agents </a>by Hung H. Bui, Svetha Venkatesh, Dorota Kieronska. LNAI, 1998. <a href="link">  </a> </summary> We present a framework for team coordination under incomplete information based on the theory of incomplete information games. When the true distribution of the uncertainty involved is not known in advance, we consider a repeated interaction scenario and show that the agents can learn to estimate this distribution and share their estimations with one another. Over time, as the set of agents' estimations become more accurate, the utility they can achieve approaches the optimal utility when the true distribution is known, while the communication requirement for exchanging the estimations among the agents can be kept to a minimal level.  <br> - </details>

<details> <summary> <a href="http://www.ens.utulsa.edu/~sandip/jetai.pdf"> Individual learning of coordination knowledge </a>by Sandip Sen, Mahendra Sekaran. Journal of Experimental and Theoretical Artificial Intelligence, 1998. <a href="link">  </a> </summary> Social agents, both human and computational, inhabiting a world containing multiple active agents, need to coordinate their activities. This is because agents share resources, and without proper coordination or “rules of the road”, everybody will be interfering with the plans of others. As such, we need coordination schemes that allow agents to effectively achieve local goals without adversely affecting the problem-solving capabilities of other agents. Researchers in the field of Distributed Artificial Intelligence (DAI) have developed a variety of coordination schemes under different assumptions about agent capabilities and relationships. Whereas some of these research have been motivated by human cognitive biases, others have approached it as an engineering problem of designing the most effective coordination architecture or protocol. We evaluate individual and concurrent learning by multiple, autonomous agents as a means for acquiring coordination knowledge. We show that a uniform reinforcement learning algorithm suffices as a coordination mechanism in both cooperative and adversarial situations. Using a number of multiagent learning scenarios with both tight and loose coupling between agents and with immediate as well as delayed feedback, we demonstrate that agents can consistently develop effective policies to coordinate their actions without explicit information sharing. We demonstrate the viability of using both the Q-learning algorithm and genetic algorithm based classifier systems with different payoff schemes, namely the bucket brigade algorithm (BBA) and the profit sharing plan (PSP), for developing agent coordination on two different multi-agent domains. In addition, we show that a semi-random scheme for action selection is preferable to the more traditional fitness proportionate selection scheme used in classifier systems. <br> - </details>

<details> <summary> <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=29893127a377d37f16662dc482e012ff900bd55a"> Coordinated Reinforcement Learning </a>by Carlos Guestrin, Michail Lagoudakis, Ronald Parr. AAAI, 2002. <a href="link">  </a> </summary> We present several new algorithms for multiagent reinforcement learning. A common feature of these algorithms is a parameterized, structured representation of a policy or value function. This structure is leveraged in an approach we call coordinated reinforcement learning, by which agents coordinate both their action selection activities and their parameter updates. Within the limits of our parametric representations, the agents will determine a jointly optimal action without explicitly considering every possible action in their exponentially large joint action space. Our methods differ from many previous reinforcement learning approaches to multiagent coordination in that structured communication and coordination between agents appears at the core of both the learning algorithm and the execution architecture. Our experimental results, comparing our approach to other RL methods, illustrate both the quality of the policies obtained and the additional benefits of coordination <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2002/AAAI02-050.pdf"> Reinforcement Learning of Coordination in Cooperative Multiagent Systems </a>by Kapetanakis S. and Kudenko D. AAAI, 2002. <a href="link">  </a> </summary> We report on an investigation of reinforcement learning techniques for the learning of coordination in cooperative multiagent systems. Specifically, we focus on a novel action selection strategy for Q-learning (Watkins 1989). The new technique is applicable to scenarios where mutual observation of actions is not possible. To date, reinforcement learning approaches for such independent agents did not guarantee convergence to the optimal joint action in scenarios with high miscoordination costs. We improve on previous results (Claus & Boutilier 1998) by demonstrating empirically that our extension causes the agents to converge almost always to the optimal joint action even in these difficult cases. <br> - </details>

<details> <summary> <a href="https://link.springer.com/chapter/10.1007/3-540-36187-1_36"> Learning to Reach the Pareto Optimal Nash Equilibrium as a Team </a>by Katja Verbeeck, Ann Nowe, Tom Lenaerts, Johan Parent.  LNAI, 2002. <a href="link">  </a> </summary> Coordination is an important issue in multi-agent systems when agents want to maximize their revenue. Often coordination is achieved through communication, however communication has its price. We are interested in finding an approach where the communication between the agents is kept low, and a global optimal behavior can still be found. In this paper we report on an efficient approach that allows independent reinforcement learning agents to reach a Pareto optimal Nash equilibrium with limited communication. The communication happens at regular time steps and is basicallya signal for the agents to start an exploration phase. During each exploration phase, some agents exclude their current best action so as to give the team the opportunityto look for a possiblyb etter Nash equilibrium. This technique of reducing the action space byexclusions was onlyrecen tlyin troduced for finding periodical policies in games of conflicting interests. Here, we explore this technique in repeated common interest games with deterministic or stochastic outcomes. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Georgios-Chalkiadakis/publication/2553318_Coordination_in_Multiagent_Reinforcement_Learning_A_Bayesian_Approach/links/09e4151363949e7f80000000/Coordination-in-Multiagent-Reinforcement-Learning-A-Bayesian-Approach.pdf"> Coordination in Multiagent Reinforcement Learning: A Bayesian Approach </a>by Georgios Chalkiadakis, Craig Boutilier. AAMAS, 2003. <a href="link">  </a> </summary> Much emphasis in multiagent reinforcement learning (MARL) research is placed on ensuring that MARL algorithms (eventually) converge to desirable equilibria. As in standard reinforcement learning, convergence generally requires sufficient exploration of strategy space. However, exploration often comes at a price in the form of penalties or foregone opportunities. In multiagent settings, the problem is exacerbated by the need for agents to “coordinate” their policies on equilibria. We propose a Bayesian model for optimal exploration in MARL problems that allows these exploration costs to be weighed against their expected benefits using the notion of value of information. Unlike standard RL models, this model requires reasoning about how one’s actions will influence the behavior of other agents. We develop tractable approximations to optimal Bayesian exploration, and report on experiments illustrating the benefits of this approach in identical interest games. <br> - </details>

<details> <summary> <a href="https://core.ac.uk/download/pdf/54045748.pdf"> Coordination of independent learners in cooperative Markov games </a>by La¨etitia Matignon, Guillaume J. Laurent, Nadine Le Fort-Piat. Technical Report, 2009. <a href="link">  </a> </summary> In the framework of fully cooperative multi-agent systems, independent agents learning by reinforcement must overcome several difficulties as the coordination or the impact of exploration. The study of these issues allows first to synthesize the characteristics of existing reinforcement learning decentralized methods for independent learners in cooperative Markov games. Then, given the difficulties encountered by these approaches, we focus on two main skills: optimistic agents, which manage the coordination in deterministic environments, and the detection of the stochasticity of a game. Indeed, the key difficulty in stochastic environment is to distinguish between various causes of noise. The SOoN algorithm is so introduced, standing for “Swing between Optimistic or Neutral”, in which independent learners can adapt automatically to the environment stochasticity. Empirical results on various cooperative Markov games notably show that SOoN overcomes the main factors of non-coordination and is robust face to the exploration of other agents <br> - </details>

<br/>

<details> <summary> <a href="https://www.sciencedirect.com/science/article/pii/0304406882900064"> Optimal coordination mechanisms in generalized principal–agent problems </a> by Myerson, Roger B. Journal of Mathematical Economics, 1982. <a href="link">  </a> </summary> The general principal–agent problem is formulated, in which agents have both private information and private decisions, unobservable to the principal. It is shown that the principal can restrict himself to incentive-compatible direct coordination mechanisms, in which agents report their information to the principal, who then recommends to them decisions forming a correlated equilibrium. In the finite case, optimal coordination mechanisms can be found by linear programming. Some basic issues relating to systems with many principals are also discussed. Non-cooperative equilibria between interacting principals do not necessarily exist, so quasi-equilibria are defined and shown to exist. <br> - </details>

<br/>

### Cooperation

<details> <summary> <a href="https://web.media.mit.edu/~cynthiab/Readings/tan-MAS-reinfLearn.pdf"> Multi-Agent Reinforcement Learning Independent vs Cooperative Agents </a>by Ming Tan. ICML, 1993. <a href="link">  </a> </summary> Intelligent human agents exist in a cooperative social environment that facilitates learning. They learn not only by trial-and-error but also through cooperation by sharing instantaneous information episodic experience and learned knowledge. The key investigations of this paper are, "Given the same number of reinforcement learning agents will cooperative agents outperform independent agents who do not communicate during learning?" and "What is the price for such cooperation?" Using independent agents as a benchmark cooperative agents are studied in following ways: (1) sharing sensation, (2) sharing episodes and (3) sharing learned policies. This paper shows that as (a) additional sensation from another agent is beneficial if it can be used efficiently, (b) sharing learned policies or episodes among agents speeds up learning at the cost of communication and (c) for joint tasks agents engaging in partnership can significantly outperform independent agents although they may learn slowly in the beginning. These tradeoffs are not just limited to multi-agent reinforcement learning. <br> - </details>

<details> <summary> <a href="https://www.ai.rug.nl/~mwiering/GROUP/ARTICLES/TR-29-97-soccer.pdf"> Learning team strategies with multiple policy-sharing agents: A soccer case study </a>by R. Salustowicz, M. Wiering, and J. Schmidhuber. Technical report, 1997. <a href="link">  </a> </summary> We use simulated soccer to study multiagent learning. Each team's players (agents) share action set and policy, but may behave differently due to position-dependent inputs. All agents making up a team are rewarded or punished collectively in case of goals. We conduct simulations with varying team sizes, and compare several learning algorithms: TD-Q learning with linear neural networks (TD-Q), Probabilistic Incremental Program Evolution (PIPE), and a PIPE version that learns by coevolution (CO-PIPE). TD-Q is based on learning evaluation functions (EFs) mapping input/action pairs to expected reward. PIPE and CO-PIPE search policy space directly. They use adaptive probability distributions to synthesize programs that calculate action probabilities from current inputs. Our results show that linear TD-Q encounters several difficulties in learning appropriate shared EFs. PIPE and CO-PIPE, however, do not depend on EFs and find good policies faster and more reliably. This suggests that in some multiagent learning scenarios direct search in policy space can offer advantages over EF-based approaches. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/3-540-39963-1.pdf?pdf=button"> Evolving Behaviors for Cooperating Agents </a>by Jeffrey K. Bassett, Kenneth A. De Jong. Symposium on Methodologies for Intelligent Systems, 2000. <a href="link">  </a> </summary> A good deal of progress has been made in the past few years in the design and implementation of control programs for autonomous agents. A natural extension of this work is to consider solving difficult tasks with teams of cooperating agents. Our interest in this area is motivated in part by our involvement in a Navy-sponsored micro air vehicle (MAV) project in which the goal is to solve difficult surveillance tasks using a large team of small inexpensive autonomous air vehicles rather than a few expensive piloted vehicles. Our approach to developing control programs for these MAVs is to use evolutionary computation techniques to evolve behavioral rule sets. In this paper we describe our architecture for achieving this, and we present some of our initial results. <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839146&casa_token=Qb2aIlZRh9IAAAAA:1BrtA4ESZr_iJJJ4vhG6HZstOeQ-qfuqfs3iIGu2HutZb5bXpR9KyJTslQNF4YeYJ9l-Jw-mGac5BDk&tag=1"> Advantages of Cooperation Between Reinforcement Learning Agents in Difficult Stochastic Problems </a>by Hamid R. Berenji, David Vengerov. International Conference on Fuzzy Systems, 2000. <a href="link">  </a> </summary> This paper presents the first results in understanding the reasons for cooperative advantage between reinforcement learning agents. We consider a cooporation method which consists of using and updating a common policy. We tested this method on a complex fuzzy reinforcement learning problem and found that cooperation brings larger than expected benefits. More precisely, we found that K cooperative agents oach learning for N time steps outperform K independent agents each learning in a separate world for K*N time steps. In this paper, we explain the observed phenomenon and determine the necessary conditions for its presence in a wide class of reinforccment learning problems.  <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839146&casa_token=Y-3elWT3fZUAAAAA:YAI-tJ6dnLkc6QZ7rGlVy0sOeKOEw3PRxqCdi2igfTJWnFcMbn6Zl5Y9WN-Iq9RlcNyP__7dPKsQJg&tag=1"> Advantages of Cooperation Between Reinforcement Learning Agents in Difficult Stochastic Problems  </a>by Hamid R. Berenji, David Vengerov. Fuzzy Systems, 2000. <a href="link">  </a> </summary> This paper presents the first results in understanding the reasons for cooperative advantage betwccn reinforcement learning agents. We consider a cooporation method which consists of using and updating a common policy. We tested this method on a complex fumy reinforcement learning problem and found that cooperation brings larger than expected benefits. More precisely, we found that K cooperative agents oach learning for N time steps outperform K independent agents oach learning in a separate world for K*N time steps. In this paper we explain the observed phenomenon and determine the necessary conditions for its presence in a wide class of reinforccment learning problems. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/cs/0105032.pdf"> Learning to Cooperate via Policy Search </a>by Leonid Peshkin, Kee-Eung Kim, Nicolas Meuleau, Leslie Pack Kaelbling. UAI, 2000. <a href="link">  </a> </summary> Cooperative games are those in which both agents share the same payoff structure. Valuebased reinforcement-learning algorithms, such as variants of Q-learning, have been applied to learning cooperative games, but they only apply when the game state is completely observable to both agents. Policy search methods are a reasonable alternative to value-based methods for partially observable environments. In this paper, we provide a gradient-based distributed policysearch method for cooperative games and compare the notion of local optimum to that of Nash equilibrium. We demonstrate the effectiveness of this method experimentally in a small, partially observable simulated soccer domain <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/2003/file/c8067ad1937f728f51288b3eb986afaa-Paper.pdf"> All learning is local: Multi-agent learning in global reward games </a>by Yu-Han Chang, Tracey Ho, Leslie Pack Kaelbling. NeurIPS, 2003. <a href="link">  </a> </summary> In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efficient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman filtering to allow an agent to construct a good training signal and learn an effective policy <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~sandholm/cs15-892F15/MarginalContributionEC05.pdf"> Marginal contribution nets: A compact representation scheme for coalitional games </a>by Ieong S, Shoham Y. In Proceedings of the 6th ACM Conference on Electronic Commerce, 2005. <a href="link">  </a> </summary> We present a new approach to representing coalitional games based on rules that describe the marginal contributions of the agents. This representation scheme captures characteristics of the interactions among the agents in a natural and concise manner. We also develop efficient algorithms for two of the most important solution concepts, the Shapley value and the core, under this representation. The Shapley value can be computed in time linear in the size of the input. The emptiness of the core can be determined in time exponential only in the treewidth of a graphical interpretation of our representation. <br> - </details>

<details> <summary> <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/ieong06multi.pdf"> Multi-attribute coalitional games </a>by Ieong S, Shoham Y. In Proceedings of the 7th ACM Conference on Electronic Commerce, 2006. <a href="link">  </a> </summary> We study coalitional games where the value of cooperation among the agents are solely determined by the attributes the agents possess, with no assumption as to how these attributes jointly determine this value. This framework allows us to model diverse economic interactions by picking the right attributes. We study the computational complexity of two coalitional solution concepts for these games — the Shapley value and the core. We show how the positive results obtained in this paper imply comparable results for other games studied in the literature. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2008/AAAI08-015.pdf"> Bayesian Coalitional Games </a>by Ieong S, Shoham Y. AAAI, 2008. <a href="link">  </a> </summary> We introduce Bayesian Coalitional Games (BCGs), a generalization of classical coalitional games to settings with uncertainties. We define the semantics of BCG using the partition model, and generalize the notion of payoffs to contracts among agents. To analyze these games, we extend the solution concept of the core under three natural interpretations—ex ante, ex interim, and ex post—which coincide with the classical definition of the core when there is no uncertainty. In the special case where agents are risk-neutral, we show that checking for core emptiness under all three interpretations can be simplified to linear feasibility problems similar to that of their classical counterpart. <br> - </details>

<br/>

### Decentralised Partially Observable MDPs

<details> <summary> <a href="https://www.researchgate.net/profile/Stacy-Marsella-2/publication/2570317_Taming_Decentralized_POMDPs_Towards_Efficient_Policy_Computation_for_Multiagent_Settings/links/573085d508aee022975c43b3/Taming-Decentralized-POMDPs-Towards-Efficient-Policy-Computation-for-Multiagent-Settings.pdf"> Taming Decentralized POMDPs: Towards Efficient Policy Computation for Multiagent Settings </a>by R. Nair, M. Tambe, M. Yokoo, D. Pynadath, S. Marsella. IJCAI, 2003. <a href="link">  </a> </summary> The problem of deriving joint policies for a group of agents that maximize some joint reward function can be modeled as a decentralized partially observable Markov decision process (POMDP). Yet, despite the growing importance and applications of decentralized POMDP models in the multiagents arena, few algorithms have been developed for efficiently deriving joint policies for these models. This paper presents a new class of locally optimal algorithms called “Joint Equilibrium based search for policies (JESP)”. We first describe an exhaustive version of JESP and subsequently a novel dynamic programming approach to JESP. Our complexity analysis reveals the potential for exponential speedups due to the dynamic programming approach. These theoretical results are verified via empirical comparisons of the two JESP versions with each other and with a globally optimal brute-force search algorithm. Finally, we prove piece-wise linear and convexity (PWLC) properties, thus taking steps towards developing algorithms for continuous belief states <br> - </details>

<br/>

### Decision Theory

<details> <summary> <a href="https://www.ijcai.org/Proceedings/91-1/Papers/011.pdf"> A decision-theoretic approach to coordinating multiagent interactions </a>by Piotr J. Gmytrasiewicz, Edmund H. Durfee, David K. Weh. IJCAI, 1991. <a href="https://www.Summary.so/instadeep/Multiagent-Learning-Basics-Challenges-and-Prospect-21cb7b4294b84a4188cafd184a3deed8">  </a> </summary> We describe a decision-theoretic method that an autonomous agent can use to model multiagent situations and behave rationally based on its model. Our approach, which we call the Recursive Modeling Method, explicitly accounts for the recursive nature of multiagent reasoning. Our method lets an agent recursively model another agent's decisions based on probabilistic views of how that agent perceives the multiagent situation, which in turn are derived from hypothesizing how that other agent perceives the initial agent's possible decisions, and so on. Further, we show how the possibility of multiple interactions can affect the decisions of agents, allowing cooperative behavior to emerge as a rational choice of selfish agents that otherwise might behave uncooperatively <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/1301.3836.pdf"> The Complexity of Decentralized Control of Markov Decision Processes </a>by Daniel S. Bernstein, Shlomo Zilberstein, Neil Immerman. UAI, 2000. <a href="link">  </a> </summary> Planning for distributed agents with partial state information is considered from a decision theoretic perspective. We describe generalizations of both the MDP and POMDP models that allow for decentralized control. For even a small number of agents, the finite-horizon problems corresponding to both of our models are complete for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov processes. In contrast to the MDP and POMDP problems, the problems we consider provably do not admit polynomialtime algorithms and most likely require doubly exponential time to solve in the worst case. We have thus provided mathematical evidence corresponding to the intuition that decentralized planning problems cannot easily be reduced to centralized problems and solved exactly using established techniques.  <br> - </details>

<details> <summary> <a href="https://www.jair.org/index.php/jair/article/view/10339/24717"> Decision-Theoretic Bidding Based on Learned Density Models in Simultaneous, Interacting Auctions </a>by P. Stone, R. S. P., M. L. Littman, J. A. Csirik, and D. McAlleste. JAIR, 2003. <a href="link">  </a> </summary> Auctions are becoming an increasingly popular method for transacting business, especially over the Internet. This article presents a general approach to building autonomous bidding agents to bid in multiple simultaneous auctions for interacting goods. A core component of our approach learns a model of the empirical price dynamics based on past data and uses the model to analytically calculate, to the greatest extent possible, optimal bids. We introduce a new and general boosting-based algorithm for conditional density estimation problems of this kind, i.e., supervised learning problems in which the goal is to estimate the entire conditional distribution of the real-valued label. This approach is fully implemented as ATTac-2001, a top-scoring agent in the second Trading Agent Competition (TAC-01). We present experiments demonstrating the effectiveness of our boosting-based price predictor relative to several reasonable alternatives. <br> - </details>

<details> <summary> <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ece97535af9133441ed84283636c3c427e8c0f47"> Decision Procedures for BDI Logics </a> by Rao, Anand S.; Michael P. Georgeff; Georgeff, Michael; Michael P. Georgeff. Journal of Logic and Computation, 1998. <a href="link">  </a> </summary> The study of computational agents capable of rational behaviour has received increasing attention in recent years. A number of theoretical formalizations for such multi-agent systems have been proposed. However, most of these formalizations do not have a strong semantic basis nor a sound and complete axiomatization. Hence, it has not been clear as to how these formalizations could assist in building agents in practice. This paper explores a particular type of multi-agent system, in which each agent is viewed as having the three mental attitudes of belief (B), desire (D), and intention (I). It provides a family of multi-modal branching-time BDI logics with a possible-worlds semantics, categorizes them, provides sound and complete axiomatizations, and gives constructive tableaubased decision procedures for testing the satisfiability and validity of formulas. The computational complexity of these decision procedures is no greater than the complexity of their underlying temporal logic component. <br> - </details>

<br/>

### Dispersion Games

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2002/AAAI02-061.pdf"> Dispersion games: general definitions and some specific learning results </a>by T. Grenager, R. Powers, Y. Shoham. AAAI, 2002. <a href="link">  </a> </summary> Dispersion games are the generalization of the anticoordination game to arbitrary numbers of agents and actions. In these games agents prefer outcomes in which the agents are maximally dispersed over the set of possible actions. This class of games models a large number of natural problems, including load balancing in computer science, niche selection in economics, and division of roles within a team in robotics. Our work consists of two main contributions. First, we formally define and characterize some interesting classes of dispersion games. Second, we present several learning strategies that agents can use in these games, including traditional learning rules from game theory and artificial intelligence, as well as some special purpose strategies. We then evaluate analytically and empirically the performance of each of these strategies. <br> - </details>

<br/>

### Evaluation

<details> <summary> <a href="https://www.aaai.org/Papers/Workshops/2002/WS-02-06/WS02-06-013.pdf"> Analyzing Complex Strategic Interactions in Multi-Agent Systems </a>by William E. Walsh, Rajarshi Das, Gerald Tesauro, Jeffrey O. Kephart. AAAI, 2002. <a href="link">  </a> </summary>  We develop a model for analyzing complex games with repeated interactions, for which a full game-theoretic analysis is intractable. Our approach treats exogenously specified, heuristic strategies, rather than the atomic actions, as primitive, and computes a heuristic-payoff table specifying the expected payoffs of the joint heuristic strategy space. We analyze two games based on (i) automated dynamic pricing and (ii) continuous double auction. For each game we compute Nash equilibria of previously published heuristic strategies. To determine the most plausible equilibria, we study the replicator dynamics of a large population playing the strategies. In order to account for errors in estimation of payoffs or improvements in strategies, we also analyze the dynamics and equilibria based on perturbed payoff <br> - </details>

<details> <summary> <a href="https://apps.dtic.mil/sti/pdfs/ADA436200.pdf#page=104"> Run the GAMUT: A Comprehensive Approach to Evaluating Game-Theoretic Algorithms </a>by Eugene Nudelman, Jennifer Wortman, Yoav Shoham. AAMAS, 2004. <a href="link">  </a> </summary> We present GAMUT, a suite of game generators designed for testing game-theoretic algorithms. We explain why such a generator is necessary, offer a way of visualizing relationships between the sets of games supported by GAMUT, and give an overview of GAMUT’s architecture. We highlight the importance of using comprehensive test data by benchmarking existing algorithms. We show surprisingly large variation in algorithm performance across different sets of games for two widely-studied problems: computing Nash equilibria and multiagent learning in repeated games. <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/2004/file/220a7f49d42406598587a66f02584ac3-Paper.pdf"> New Criteria and a New Algorithm for Learning in Multi-Agent Systems </a>by Rob Powers, Yoav Shoham. NeurIPS, 2004. <a href="link">  </a> </summary> We propose a new set of criteria for learning algorithms in multi-agent systems, one that is more stringent and (we argue) better justified than previous proposed criteria. Our criteria, which apply most straightforwardly in repeated games with average rewards, consist of three requirements: (a) against a specified class of opponents (this class is a parameter of the criterion) the algorithm yield a payoff that approaches the payoff of the best response, (b) against other opponents the algorithm’s payoff at least approach (and possibly exceed) the security level payoff (or maximin value), and (c) subject to these requirements, the algorithm achieve a close to optimal payoff in self-play. We furthermore require that these average payoffs be achieved quickly. We then present a novel algorithm, and show that it meets these new criteria for a particular parameter class, the class of stationary opponents. Finally, we show that the algorithm is effective not only in theory, but also empirically. Using a recently introduced comprehensive game theoretic test suite, we show that the algorithm almost universally outperforms previous learning algorithms <br> - </details>

<details> <summary> <a href="https://www.emse.fr/~boissier/enseignement/sma05/exposes/PowersShoham_LearningAgainstBoundedMemory.pdf"> Learning against opponents with bounded memory </a>by Rob Powers, Yoav Shoham. IJCAI, 2005. <a href="link">  </a> </summary> Recently, a number of authors have proposed criteria for evaluating learning algorithms in multiagent systems. While well-justified, each of these has generally given little attention to one of the main challenges of a multi-agent setting: the capability of the other agents to adapt and learn as well. We propose extending existing criteria to apply to a class of adaptive opponents with bounded memory which we describe. We then show an algorithm that provably achieves an epsilon-best response against this richer class of opponents while simultaneously guaranteeing a minimum payoff against any opponent and performing well in self-play. This new algorithm also demonstrates strong performance in empirical tests against a variety of opponents in a wide range of environments <br> - </details>

<br/>

### Evolutionary Game Theory and Strategies

<details> <summary> <a href="http://etherplan.com/the-logic-of-animal-conflict.pdf"> The logic of animal conflict </a>by Maynard Smith, J., Price, G.R. Nature, 1973. <a href="link">  </a> </summary> Conflicts between animals of the same species usually are of “limited war” type, not causing serious injury. This is often explained as due to group or species selection for behaviour benefiting the species rather than individuals. Game theory and computer simulation analyses show, however, that a “limited war” strategy benefits individual animals as well as the species. <br> - </details>

<details> <summary> <a href="https://www.reed.edu/biology/courses/BIO342/2012_syllabus/2012_readings/smith_1976_games.pdf"> Evolution and the Theory of Games </a>by J. Maynard Smith. American Scientist, 1976. <a href="link">  </a> </summary> n situations characterized by conflict of interest, the best strategy to adopt depends on what others are doing. <br> - </details>

<details> <summary> <a href="http://math.uchicago.edu/~shmuel/Modeling/Axelrod%20and%20Hamilton.pdf"> The Evolution of Cooperation </a>by Robert Axelrod, William D. Hamilton. Science, 1981. <a href="link">  </a> </summary> Cooperation in organisms, whether bacteria or primates, has been a difficulty for evolutionary theory since Darwin. On the assumption that interactions between pairs of individuals occur on a probabilistic basis, a model is developed based on the concept of an evolutionarily stable strategy in the context of the Prisoner's Dilemma game. Deductions from the model, and the results of a computer tournament show how cooperation based on reciprocity can get started in an asocial world, can thrive while interacting with a wide range of other strategies, and can resist invasion once fully established. Potential applications include specific aspects of territoriality, mating, and disease. <br> - </details>

<details> <summary> <a href="https://www.econstor.eu/bitstream/10419/95028/1/wp347.pdf"> An Introduction to Evolutionary Game Theory </a>by Weibull, Jörgen W. IFN, 1992. <a href="link">  </a> </summary> Please note that these lecture notes are incomplete and may contain typos and errors. I hope to have a more full-fledged version in a few months, and appreciate in the meantime comments and suggestions.  <br> - </details>

<details> <summary> <a href="http://bobby.cs-i.brandeis.edu/papers/icga5.pdf"> Competitive Environments Evolve Better Solutions for Complex Tasks </a>by Peter J. Angeline, Jordan B. Pollack. ICGA, 1993. <a href="link">  </a> </summary> In the typical genetic algorithm experiment, the fitness function is constructed to be independent of the contents of the population to provide a consistent objective measure. Such objectivity entails significant knowledge about the environment which suggests either the problem has previously been solved or other non-evolutionary techniques may be more efficient. Furthermore, for many complex tasks an independent fitness function is either impractical or impossible to provide. In this paper, we demonstrate that competitive fitness functions, i.e. fitness functions that are dependent on the constituents of the population, can provide a more robust training environment than independent fitness functions. We describe three differing methods for competitive fitness, and discuss their respective advantages <br> - </details>

<details> <summary> <a href="https://www.econstor.eu/bitstream/10419/94705/1/wp407.pdf"> Nash Equilibrium and Evolution by Imitation </a>by Bjornerstedt J., and Weibull, J. The Rational Foundations of Economic Behavior, 1995. <a href="link">  </a> </summary> Nash's "mass action" interpretation of his equilibrium concept does not presume that the players know the game or are capable of sophisticated calculations. Instead, players are repeatedly and randomly drawn from large populations to play the game, one population for each player position, and base their strategy choice on observed payoffs. The present paper examines in some detail such an interpretation in a dass of population dynamics based on adaptation by way of imitation of successful behaviors. Drawing from results in evolutionary game theory, implications of dynamic stability for aggregate Nash equilibrium play are discussed.  <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Spring/1996/SS-96-01/SS96-01-005.pdf"> Evolutionary computing in multi-agent environments </a>by Lawrence Bull, Terence C Fogarty. AAAI, 1996. <a href="link">  </a> </summary> The fields of Artificial Intelligence and Artificial Life have both focused on complex systems in which agents must cooperate to achieve certain goals. In our work we examine the performance of the genetic algorithm when applied to systems of this type. That is, we examine the use of population-based evolutionary computing techniques within cooperative multi-agent environments. In extending the genetic algorithm to such environments we introduce three macro-level operators to reduce the amount of knowledge required a priori; the joining of agents (symbiogenesis), the transfer of genetic material between agents and the speciation of initially homogeneous agents. These operators are used in conjunction with a generic rule-based framework, a simplified version of Pittsburgh-style classifier systems, which we alter to allow for direct systemic communication to evolve between the thus represented agents. In this paper we use a simulated trail following task to demonstrate these techniques, finding that they can give improved performance <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Forrest-Bennett-Iii/publication/2345524_Discovery_by_Genetic_Programming_of_a_Cellular_Automata_Rule_that_is_Better_than_any_Known_Rule_for_the_Majority_Classification_Problem/links/0912f50a3c6bd0ccd2000000/Discovery-by-Genetic-Programming-of-a-Cellular-Automata-Rule-that-is-Better-than-any-Known-Rule-for-the-Majority-Classification-Problem.pdf"> Discovery by Genetic Programming of a Cellular Automata Rule that is Better than any Known Rule for the Majority Classification Problem </a>by David Andre, Forrest H Bennett, John R. Koza. Genetic programming, 1996. <a href="link">  </a> </summary> It is difficult to program cellular automata. This is especially true when the desired computation requires global communication and global integration of information across great distances in the cellular space. Various human-written algorithms have appeared in the past two decades for the vexatious majority classification task for one-dimensional two-state cellular automata. This paper describes how genetic programming with automatically defined functions evolved a rule for this task with an accuracy of 82.326%. This level of accuracy exceeds that of the original 1978 Gacs-Kurdyumov-Levin (GKL) rule, all other known human-written rules, and all other known rules produced by automated methods. The rule evolved by genetic programming is qualitatively different from all previous rules in that it employs a larger and more intricate repertoire of domains and particles to represent and communicate information across the cellular space. <br> - </details>

<details> <summary> <a href="http://www.cs.unibo.it/babaoglu/courses/cas06-07/resources/tutorials/Evolving_Cellular_Automata.pdf"> Evolving Cellular Automata with Genetic Algorithms: A Review of Recent Work </a>by Melanie Mitchell, James P. Crutchfield, Rajarshi Das. EvCA, 1996. <a href="link">  </a> </summary> We review recent work done by our group on applying genetic algorithms (GAs) to the design of cellular automata (CAs) that can perform computations requiring global coordination. A GA was used to evolve CAs for two computational tasks: density classification and synchronization. In both cases, the GA discovered rules that gave rise to sophisticated emergent computational strategies. These strategies can be analyzed using a "computational mechanics" framework in which "particles" carry information and interactions between particles effects information processing. This framework can also be used to explain the process by which the strategies were designed by the GA. The work described here is a first step in employing GAs to engineer useful emergent computation in decentralized multi-processor systems. It is also a first step in understanding how an evolutionary process can produce complex systems with sophisticated collective computational abilities. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Spring/1996/SS-96-01/SS96-01-009.pdf"> Methods for Competitive and Cooperative Co-evolution </a>by John Grefenstette, Robert Daley. AAAI, 1996. <a href="link">  </a> </summary> We have been investigating evolutionary methods to design behavioral strategies for intelligent robots in multi-agent environments. Such ~nvironments resemble an ecological system in which species evolve and adapt in a complex interaction with other evolving and adapting species. This paper will report on our investigations of alternative co-evolutionary approaches in the context of a simulated multi-agent environment <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Thomas-Haynes/publication/2751435_Co--adaptation_in_a_Team/links/573b730208ae298602e45732/Co--adaptation-in-a-Team.pdf"> Co-adaptation in a Team </a>by Thomas D. Haynes, Sandip Sen. IJCIO, 1997. <a href="link">  </a> </summary> We introduce a cooperative co-evolutionary system to facilitate the development of teams of heterogeneous agents. We believe that k different behavioral strategies for controlling the actions of a group of k agents can combine to form a cooperation strategy which efficiently achieves global goals. We both examine the on-line adaption of behavioral strategies utilizing genetic programming and demonstrate the successful co-evolution of cooperative individuals. We present a new crossover mechanism for genetic programming systems in order to facilitate the evolution of more than one member in the team during each crossover operation. Our goal is to reduce the time needed to evolve an effective team. <br> - </details>

<details> <summary> <a href="https://www.econstor.eu/bitstream/10419/94851/1/wp487.pdf"> What have we learned from Evolutionary Game Theory so far? </a>by Weibull, Jörgen W. IFN, 1997. <a href="link">  </a> </summary> Evolutionary theorizing has a long tradition in economics. Only recently has this approach been brought into the framework of noncooperative game theory. Evolutionary game theory studies the robustness of strategic behavior with respect to evolutionary forces in the context of games played many times in large populations of boundedly rational agents. This new strand in economic theory has lead to new predictions and opened up doors to other social sciences. The discussion will be focused on the following questions: What distinguishes the evolutionary approach from the rationalistic? What are the most important ndings in evolutionary game theory so far? What are the next challenges for evolutionary game theory in economics? <br> - </details>

<details> <summary> <a href="https://core.ac.uk/download/pdf/6518937.pdf"> Learning through Reinforcement and Replicator Dynamics </a>by Borgers, T., Sarin, R. Journal of Economic Theory, 1997. <a href="link">  </a> </summary> This paper considers a version of R. R. Bush and F. Mosteller's stochastic learning theory in the context of games. We show that in a continuous time limit the learning model converges to the replicator dynamics of evolutionary game theory. Thus we provide a non-biological interpretation of evolutionary game theory. <br> - </details>

<details> <summary> <a href="https://web.unbc.ca/~russellt/swarm/hitoshiiba1.pdf"> Evolutionary learning of communicating agents </a>by Hitoshi Iba. Journal of Information Sciences, 1998. <a href="link">  </a> </summary> This paper presents the emergence of the cooperative behavior for communicating agents by means of Genetic Programming (GP). Our experimental domains are the pursuit game and the robot navigation task. We conduct experiments with the evolution of the communicating agents and show the effectiveness of the emergent communication in terms of the robustness of generated GP programs. The performance of GP-based multi-agent learning is discussed with comparative experiments by using different breeding strategies, i.e., homogenous breeding and heterogeneous breeding <br> - </details>

<details> <summary> <a href="http://www.fulviofrisone.com/attachments/article/412/Hofbauer%20Evolutionary%20Games%20and%20Population%20Dynamics.pdf"> Evolutionary Games and Population Dynamics </a>by Hofbauer, J., Sigmund, K. Cambridge University Press, 1998. <a href="link">  </a> </summary> Every form of behaviour is shaped by trial and error. Such stepwise adaptation can occur through individual learning or through natural selection, the basis of evolution. Since the work of Maynard Smith and others, it has been realised how game theory can model this process. Evolutionary game theory replaces the static solutions of classical game theory by a dynamical approach centred not on the concept of rational players but on the population dynamics of behavioural programmes. In this book the authors investigate the nonlinear dynamics of the self-regulation of social and economic behaviour, and of the closely related interactions between species in ecological communities. Replicator equations describe how successful strategies spread and thereby create new conditions which can alter the basis of their success, i.e. to enable us to understand the strategic and genetic foundations of the endless chronicle of invasions and extinctions which punctuate evolution. In short, evolutionary game theory describes when to escalate a conflict, how to elicit cooperation, why to expect a balance of the sexes, and how to understand natural selection in mathematical terms. <br> - </details>

<details> <summary> <a href="http://gpbib.cs.ucl.ac.uk/cache/cache/.hidden_13-jun_2087063365/http___www.cs.ucl.ac.uk_staff_W.Langdon_aigp3_ch19.pdf"> Evolving Multiple Agents by Genetic Programming </a>by Hitoshi Iba. MIT press, 1999. <a href="link">  </a> </summary> On the emergence of the cooperative behaviour for multiple agents by means of Genetic Programming (GP). Our experimental domains are multi-agent test beds, i.e., the robot navigation task and the Tile World. The world consists of a simulated robot agent and a simulated environment which is both dynamic and unpredictable. In our previous paper, we proposed three types of strategies, i.e, homogeneous breeding, heterogeneous breeding, and co-evolutionary breeding, for the purpose of evolving the cooperative behavior. We use the heterogeneous breeding in this paper. The previous Q-learning approach commonly used for the multi-agent task has the difficulty with the combinatorial explosion for many agents. This is because the state space for Q-table is so huge for the practical computer resources. We show how successfully GP-based multi-agent learning is applied to multi-agent tasks and compare the performance with Q-learning by experiments. Thereafter, we conduct experiments with the evolution of the communicating agents. The communication is an essential factor for the emergence of cooperation. This is because a collaborative agent must be able to handle situations in which conflicts arise and must be capable of negotiating with other agents to reach an agreement. The effectiveness of the emergent communication is empirically shown in terms of the robustness of generated GP programs. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Herbert-Gintis/publication/235734033_Game_Theory_Evolving/links/0c96052f7a69760977000000/Game-Theory-Evolving.pdf"> Game Theory Evolving </a>by Herbert Gintis. Princeton University Press, 2000. <a href="link">  </a> </summary> Since its original publication Game Theory Evolving has been considered the best textbook on evolutionary game theory. This completely revised and updated second edition of Game Theory Evolving contains new material and shows students how to apply game theory to model human behavior in ways that reflect the special nature of sociality and individuality. The textbook continues its in-depth look at cooperation in teams, agent-based simulations, experimental economics, the evolution and diffusion of preferences, and the connection between biology and economics. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Kam-Chuen-Jim/publication/12106374_Talking_Helps_Evolving_Communicating_Agents_for_the_Predator-Prey_Pursuit_Problem/links/564cadcf08aedda4c13435ff/Talking-Helps-Evolving-Communicating-Agents-for-the-Predator-Prey-Pursuit-Problem.pdf"> Talking Helps: Evolving Communicating Agents for the Predator-Prey Pursuit Problem </a>by Kam-Chuen Jim, C. Lee Giles. Artifical life, 2000. <a href="link">  </a> </summary> We analyze a general model of multi-agent communication in which all agents communicate simultaneously to a message board. A genetic algorithm is used to evolve multi-agent languages for the predator agents in a version of the predator-prey pursuit problem. We show that the resulting behavior of the communicating multi-agent system is equivalent to that of a Mealy finite state machine whose states are determined by the agents’ usage of the evolved language. Simulations show that the evolution of a communication language improves the performance of the predators. Increasing the language size (and thus increasing the number of possible states in the Mealy machine) improves the performance even further. Furthermore, the evolved communicating predators perform significantly better than all previous work on similar preys. We introduce a method for incrementally increasing the language size which results in an effective coarse-to-fine search that significantly reduces the evolution time required to find a solution. We present some observations on the effects of language size, experimental setup, and prey difficulty on the evolved Mealy machines. In particular, we observe that the start state is often revisited, and incrementally increasing the language size results in smaller Mealy machines. Finally, a simple rule is derived that provides a pessimistic estimate on the minimum language size that should be used for any multi-agent problem. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/3-540-45356-3.pdf"> A Game-Theoretic Approach to the Simple Coevolutionary Algorithm </a>by Sevan G. Ficici, Jordan B. Pollack. LNCS, 2000. <a href="link">  </a> </summary> The fundamental distinction between ordinary evolutionary algorithms (EA) and co-evolutionary algorithms lies in the interaction between coevolving entities. We believe that this property is essentially game-theoretic in nature. Using game theory, we describe extensions that allow familiar mixing-matrix and Markov-chain models of EAs to address coevolutionary algorithm dynamics. We then employ concepts from evolutionary game theory to examine design aspects of conventional coevolutionary algorithms that are poorly understood. <br> - </details>

<details> <summary> <a href="https://watermark.silverchair.com/282794.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsYwggLCBgkqhkiG9w0BBwagggKzMIICrwIBADCCAqgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMBRcVXHaCrhh9DPsJAgEQgIICecvIjI-AyHPqDEtoqxG45etk8Kr3qZnjlgYcF9HmV5Zgk3mbn6FJ-OzHZWLMlN-KdRwXGkIjDVCEOB8A-qFzdZz484BYJeQqM-d1usHshJkVd8nuz2mGVsBsS1PT9YPlw68yuUWF1UzLK41IyXZmZk4Pu58zMpQSken2DXuBZzC5R0btcbkHVhC_PuRR2Empi2m-xjW8dxWV5R3sbNiDnQQFLjm7BZZXTEE1qV1GvUCW0DdIDhcWJpa-LP6NZ1G0Evo4nlcVC4h1ZNzgizSe5oKTU3hJjeltMD35akait35q5EiPh6xMFqhykSJAAu-krMio05VNfHqAZ-n9vrW5g2W6z38eSBMCPmOx4VaLh9-vCyxHTIosjdbrsKISJ1F2t1AZitrUPniIOLr07aPZBVQHT1lxegrDKEQ-CzvB5Zg7cyVgyBRHzYKswHkCNij_3teAAB7Wi8DSmJWVKOovllkEuXayDtwEy8cktVZsgCGunE9Mz1wZKB0Pj6UGiccWtbGTS63nTNBgFh4ZntPAChV6x3olsXbzqcn0snIVvDtUzS_ziOyIGSD1WrHcZCQUFcWiZXPg059UQHKxunkIJCoZTqEhq5Aicg0MBOG7RPfBMIMv4GZtEPtNRuo3Ax1kFbdmlN2_FwnbSAtMYnh5an1Z3SjVeWkKCko2FBP6yigjm2vOIJBs0Q1s3sCNAFIoa7fSIH2jgmykzWsFQdRe-0ke7GHPs0UhzzDIYogT569IVE7FUeyY3lAHMFTcH3k8XlSCtnCu84J3nXoghqfHArjcDNZnTEg9mxu5W17_tnJzHhuU-viP7hzQu0gFaej7yaaCLRKkClTkuQ"> Evolution of biological information </a>by Thomas D. Schneider. Nucleic Acids Research, 2000. <a href="link">  </a> </summary> How do genetic systems gain information by evolutionary processes? Answering this question precisely requires a robust, quantitative measure of information. Fortunately, 50 years ago Claude Shannon defined information as a decrease in the uncertainty of a receiver. For molecular systems, uncertainty is closely related to entropy and hence has clear connections to the Second Law of Thermodynamics. These aspects of information theory have allowed the development of a straightforward and practical method of measuring information in genetic control systems. Here this method is used to observe information gain in the binding sites for an artificial ‘protein’ in a computer simulation of evolution. The simulation begins with zero information and, as in naturally occurring genetic systems, the information measured in the fully evolved binding sites is close to that needed to locate the sites in the genome. The transition is rapid, demonstrating that information gain can occur by punctuated equilibrium. <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/30801544/Tuybnaic02-with-cover-page-v2.pdf?Expires=1668008453&Signature=D2CzKtWwSHhSIlxHOl7NcfNf3Be7IomezobJzZPLDBT2~3nA28zKf7xPQkQq13XFBfGgEIhx3IvzL9OHu4abVVSf9TxdFZwCaNg7JODf81a8~bBg2y9CITtTYBtmpw8gxQw9mXc4dpHBEc9dKwjLi18zC47x2e9gr4ZX3uYeRu6JflBxR6FmqwvlNzR4VxPvTv0DwgKdnALkVedwDLaGUlE7iQEd5VQgNhy8ZF-76bZ8qhGWv4FNdrFY5bjVAbJ2nz4vYcM2AAc6qNE~if9VjBARd1hkg0-3U7WLUDk2UnRzBz2rn9Z7ra75pN2MQB0VtpXAQHuh8gG5Od~MBOIfuA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> Towards a relation between learning agents and evolutionary dynamics </a>by Karl Tuyls, Tom Lenaerts, Katja Verbeeck, Sam Maes. BNAIC, 2002. <a href="link">  </a> </summary> Modeling learning agents in the context of Multi-agent Systems requires insight in the type and form of interactions with the environment and other agents in the system. Usually, these agents are modeled similar to the different players in a standard game theoretical model. In this paper we examine whether evolutionary game theory, and more specifically the replicator dynamics, is an adequate theoretical model for the study of the dynamics of reinforcement learning agents in a multi-agent system. As a first step in this direction we extend the results of [1, 9] to a more general reinforcement learning framework, i.e. Learning Automata. <br> - </details>

<details> <summary> <a href="https://www.ijcai.org/Proceedings/03/Papers/095.pdf"> When Evolving Populations is Better than Coevolving Individuals: The Blind Mice Problem </a>by Thomas Miconi. IJCAI, 2003. <a href="link">  </a> </summary> This paper is about the evolutionary design of multi-agent systems. An important part of recent research in this domain has been focusing on collaborative revolutionary methods. We expose possible drawbacks of these methods, and show that for a non-trivial problem called the "blind mice" problem, a classical GA approach in which whole populations are evaluated, selected and crossed together (with a few tweaks) finds an elegant and non-intuitive solution more efficiently than cooperative coevolution. The difference in efficiency grows with the number of agents within the simulation. We propose an explanation for this poorer performance of cooperative coevolution, based on the intrinsic fragility of the evaluation process. This explanation is supported by theoretical and experimental arguments.  <br> - </details>

<details> <summary> <a href="http://www.tesseract.org/paul/papers/gecco03-ccea.pdf"> Exploring the Explorative Advantage of the Cooperative Coevolutionary (1+1) EA </a>by Thomas Jansen, R. Paul Wiegand. GECCO, 2003. <a href="link">  </a> </summary> Using a well-known cooperative coevolutionary function optimization framework, a very simple cooperative coevolutionary (1+1) EA is defined. This algorithm is investigated in the context of expected optimization time. The focus is on the impact the cooperative coevolutionary approach has and on the possible advantage it may have over more traditional evolutionary approaches. Therefore, a systematic comparison between the expected optimization times of this coevolutionary algorithm and the ordinary (1+1) EA is presented. The main result is that separability of the objective function alone is is not sufficient to make the cooperative coevolutionary approach beneficial. By presenting a clear structured example function and analyzing the algorithms’ performance, it is shown that the cooperative coevolutionary approach comes with new explorative possibilities. This can lead to an immense speed-up of the optimization. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Karl-Tuyls/publication/221471451_On_a_Dynamical_Analysis_of_Reinforcement_Learning_in_Games_Emergence_of_Occam%27s_Razor/links/0c9605203e5ba30157000000/On-a-Dynamical-Analysis-of-Reinforcement-Learning-in-Games-Emergence-of-Occams-Razor.pdf"> On a Dynamical Analysis of Reinforcement Learning in Games: Emergence of Occam’s Razor </a>by Karl Tuyls, Katja Verbeeck, Sam Maes. Lecture Notes in Computer Science, 2003. <a href="link">  </a> </summary> Modeling learning agents in the context of Multi-agent Systems requires an adequate understanding of their dynamic behaviour. Usually, these agents are modeled similar to the different players in a standard game theoretical model. Unfortunately traditional Game Theory is static and limited in its usefelness. Evolutionary Game Theory improves on this by providing a dynamics which describes how strategies evolve over time. In this paper, we discuss three learning models whose dynamics are related to the Replicator Dynamics(RD). We show how a classical Reinforcement Learning(RL) technique, i.e. Qlearning relates to the RD. This allows to better understand the learning process and it allows to determine how complex a RL model should be. More precisely, Occam’s Razor applies in the framework of games, i.e. the simplest model (Cross) suffices for learning equilibria. An experimental verification in all three models is presented. <br> - </details>

<details> <summary> <a href="https://ochicken.top/Library/Mathematics/Dynamical_Systems/Nonlinear_and_Chaos_%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%92%8C%E6%B7%B7%E6%B2%8C/smale-def-eq.pdf"> Differential Equations, Dynamical Systems, and an Introduction to Chaos </a>by MW Hirsch, S Smale, RL Devaney. Elsevier, 2003. <a href="link">  </a> </summary> Hirsch, Devaney, and Smale's classic "Differential Equations, Dynamical Systems, and an Introduction to Chaos" has been used by professors as the primary text for undergraduate and graduate level courses covering differential equations. It provides a theoretical approach to dynamical systems and chaos written for a diverse student population among the fields of mathematics, science, and engineering. Prominent experts provide everything students need to know about dynamical systems as students seek to develop sufficient mathematical skills to analyze the types of differential equations that arise in their area of study. The authors provide rigorous exercises and examples clearly and easily by slowly introducing linear systems of differential equations. Calculus is required as specialized advanced topics not usually found in elementary differential equations courses are included, such as exploring the world of discrete dynamical systems and describing chaotic systems. This is a classic text by three of the world's most prominent mathematicians. It continues the tradition of expository excellence. It contains updated material and expanded applications for use in applied studies. <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/30888627/p693-with-cover-page-v2.pdf?Expires=1668008714&Signature=c6qzlgxHi0g~03AfAJcY0D1i7Pi0TbjmgXXrMbaE-mYV52qJJLFcEMYIjkqj59wHIJyC69sEI6dMenm8neQP9IORV-tpCNigyRWlRS5b8WL4gIFLqBodbIlr10KlLaY6~zNRHY-shOzVixDraeDdIT3qCqh-Kjb~S3uSnoIRRgKSV8p5XzeW1SAnJEcXRnM1ZZY6VTWiVejZoH02f-g9Tx1LiDmvp8XI2FJK0FuMrR-iFpcFcafc44q8bSu9HxhPBW3OPll4~vT4H3U~dMyv2h-lo-vp6gmuoQLPpym~Gc1lqBQsQb8ngo0ZDEdESlUz-ayTFD8CS6cgWm17HHO~IQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> A selection-mutation model for Q-learning in multi-agent systems </a>by Karl Tuyls, Katja Verbeeck, Tom Lenaerts. AAMAS, 2003. <a href="link">  </a> </summary> Although well understood in the single-agent framework, the use of traditional reinforcement learning (RL) algorithms in multi-agent systems (MAS) is not always justified. The feedback an agent experiences in a MAS, is usually influenced by the other agents present in the system. Multi agent environments are therefore non-stationary and convergence and optimality guarantees of RL algorithms are lost. To better understand the dynamics of traditional RL algorithms we analyze the learning process in terms of evolutionary dynamics. More specifically we show how the Replicator Dynamics (RD) can be used as a model for Q-learning in games. The dynamical equations of Q-learning are derived and illustrated by some well chosen experiments. Both reveal an interesting connection between the exploitationexploration scheme from RL and the selection-mutation mechanisms from evolutionary game theory. <br> - </details>

<details> <summary> <a href="https://cs.gmu.edu/~lpanait/papers/panait03improving.pdf"> Improving Coevolutionary Search for Optimal Multiagent Behaviors </a>by Liviu Panait, R. Paul Wiegand, Sean Luke. IJCAI, 2003. <a href="link">  </a> </summary> Evolutionary computation is a useful technique for learning behaviors in multiagent systems. Among the several types of evolutionary computation, one natural and popular method is to coevolve multiagent behaviors in multiple, cooperating populations. Recent research has suggested that coevolutionary systems may favor stability rather than performance in some domains. In order to improve upon existing methods, this paper examines the idea of modifying traditional coevolution, biasing it to search for maximal rewards. We introduce a theoretical justification of the improved method and present experiments in three problem domains. We conclude that biasing can help coevolution find better results in some multiagent problem domains. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-540-39857-8_38.pdf"> Extended Replicator Dynamics as a Key to Reinforcement Learning in Multi-agent Systems </a>by Karl Tuyls, Dries Heytens, Ann Nowe, Bernard Manderick.  ECML, 2003. <a href="link">  </a> </summary> Modeling learning agents in the context of Multi-agent Systems requires an adequate understanding of their dynamic behaviour. Evolutionary Game Theory provides a dynamics which describes how strategies evolve over time. B¨orgers et al. [1] and Tuyls et al. [11] have shown how classical Reinforcement Learning (RL) techniques such as Cross-learning and Q-learning relate to the Replicator Dynamics (RD). This provides a better understanding of the learning process. In this paper, we introduce an extension of the Replicator Dynamics from Evolutionary Game Theory. Based on this new dynamics, a Reinforcement Learning algorithm is developed that attains a stable Nash equilibrium for all types of games. Such an algorithm is lacking for the moment. This kind of dynamics opens an interesting perspective for introducing new Reinforcement Learning algorithms in multi-state games and MultiAgent Systems. <br> - </details>

<details> <summary> <a href="http://www.tesseract.org/paul/papers/Panait2004ppsn-final.pdf"> A Visual Demonstration of Convergence Properties of Cooperative Coevolution </a>by Liviu Panait, R. Paul Wiegand, Sean Luke. PPSN, 2004. <a href="link">  </a> </summary> We introduce a model for cooperative coevolutionary algorithms (CCEAs) using partial mixing, which allows us to compute the expected long-run convergence of such algorithms when individuals’ fitness is based on the maximum payoff of some N evaluations with partners chosen at random from the other population. Using this model, we devise novel visualization mechanisms to attempt to qualitatively explain a difficult-to-conceptualize pathology in CCEAs: the tendency for them to converge to suboptimal Nash equilibria. We further demonstrate visually how increasing the size of N, or biasing the fitness to include an ideal-collaboration factor, both improve the likelihood of optimal convergence, and under which initial population configurations they are not much help. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-540-30115-8_18.pdf"> Analyzing Multi-agent Reinforcement Learning Using Evolutionary Dynamics </a>by ’t Hoen, P.J., Tuyls, K. ECML, 2004. <a href="link">  </a> </summary> In this paper, we show how the dynamics of Q-learning can be visualized and analyzed from a perspective of Evolutionary Dynamics (ED). More specifically, we show how ED can be used as a model for Qlearning in stochastic games. Analysis of the evolutionary stable strategies and attractors of the derived ED from the Reinforcement Learning (RL) application then predict the desired parameters for RL in MultiAgent Systems (MASs) to achieve Nash equilibriums with high utility. Secondly, we show how the derived fine tuning of parameter settings from the ED can support application of the COllective INtelligence (COIN) framework. COIN is a proved engineering approach for learning of cooperative tasks in MASs. We show that the derived link between ED and RL predicts performance of the COIN framework and visualizes the incentives provided in COIN toward cooperative behavior. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-1-4419-8909-3.pdf"> Selection in Coevolutionary Algorithms and the Inverse Problem </a>by Sevan Ficici, Ofer Melnik, Jordan Pollack. Springer, 2004. <a href="link">  </a> </summary> The inverse problem in the collective intelligence framework concerns how the private utility functions of agents can be engineered so that their selfish behaviors collectively give rise to a desired world state. In this chapter we examine several selection and fitnesssharing methods used in coevolution and consider their operation with respect to the inverse problem. The methods we test are truncation and linear-rank selection and competitive and similarity-based fitness sharing. Using evolutionary game theory to establish the desired world state, our analyses show that variable-sum games with polymorphic Nash are problematic for these methods. Rather than converge to polymorphic Nash, the methods we test produce cyclic behavior, chaos, or attractors that lack game-theoretic justification and therefore fail to solve the inverse problem. The private utilities of the evolving agents may thus be viewed as poorly factored—improved private utility does not correspond to improved world utility. <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1555000&casa_token=CmWmYz9F4AUAAAAA:7bw2f3NZZdUZlt1kFoEIWEVVD__xdLeBOvnp6Ebemfmkgj4wbhX5ntu2JREjL0cMdb0EmQ7gYpdW9qI&tag=1"> On Social Learning and Robust Evolutionary Algorithm Design in Economic Games </a>by Floortje Alkemade, Han La Poutre, Hans Amman. Congress on Evolutionary Computation, 2005. <a href="link">  </a> </summary> Agent-based computational economics (ACE) combines elements from economics and computer science. In this paper, we focus on the relation between the evolutionary technique that is used and the economic problem that is modeled. In the field of ACE, economic simulations often derive parameter settings for the genetic algorithm directly from the values of the economic model parameters. In this paper we compare two important approaches that are dominating in ACE and show that the above practice may hinder the performance of the GA and thereby hinder agent learning. More specifically, we show that economic model parameters and evolutionary algorithm parameters should be treated separately by comparing the two widely used approaches to social learning with respect to their convergence properties and robustness. This leads to new considerations for the methodological aspects of evolutionary algorithm design within the field of ACE. We also present improved social (ACE) simulation results for the Cournot oligopoly game, yielding (higher profit) Cournot-Nash equilibria instead of the competitive equilibria. <br> - </details>

<details> <summary> <a href="http://nozdr.ru/data/media/biblio/kolxoz/Cs/CsLn/L/Learning%20and%20Adaption%20in%20Multi-Agent%20Systems,%201%20conf.,%20LAMAS%202005(LNCS3898,%20Springer,%202006)(ISBN%203540330534)(224s).pdf#page=163"> The Success and Failure of Tag-Mediated Evolution of Cooperation </a>by Austin McDonald, Sandip Sen. LAMAS, 2005. <a href="link">  </a> </summary> Use of tags to limit partner selection for playing has been shown to produce stable cooperation in agent populations playing the Prisoner’s Dilemma game. There is, however, a lack of understanding of how and why tags facilitate such cooperation. We start with an empirical investigation that identifies the key dynamics that result in sustainable cooperation in PD. Sufficiently long tags are needed to achieve this effect. A theoretical analysis shows that multiple simulation parameters including tag length, mutation rate and population size will have significant effect on sustaining cooperation. Experiments partially validate these observations. Additionally, we claim that tags only promote mimicking and not coordinated behavior in general, i.e., tags can promote cooperation only if cooperation requires identical actions from all group members. We illustrate the failure of the tag model to sustain cooperation by experimenting with domains where agents need to take complementary actions to maximize payoff <br> - </details>

<br/>

### Exploration

<details> <summary> <a href="http://nozdr.ru/data/media/biblio/kolxoz/Cs/CsLn/L/Learning%20and%20Adaption%20in%20Multi-Agent%20Systems,%201%20conf.,%20LAMAS%202005(LNCS3898,%20Springer,%202006)(ISBN%203540330534)(224s).pdf#page=173"> An Adaptive Approach for the Exploration-Exploitation Dilemma and Its Application to Economic Systems </a>by Lilia Rejeb, Zahia Guessoum, Rym MHallah. LAMAS, 2005. <a href="link">  </a> </summary> Learning agents have to deal with the exploration-exploitation dilemma. The choice between exploration and exploitation is very difficult in dynamic systems; in particular in large scale ones such as economic systems. Recent research shows that there is neither an optimal nor a unique solution for this problem. In this paper, we propose an adaptive approach based on metarules to adapt the choice between exploration and exploitation. This new adaptive approach relies on the variations of the performance of the agents. To validate the approach, we apply it to economic systems and compare it to two adaptive methods originally proposed by Wilson: one local and one global. Moreover, we compare different exploration strategies and focus on their influence on the performance of the agents. <br> - </details>

<br/>

### Extensive Form Games

<details> <summary> <a href="http://theory.stanford.edu/~megiddo/pdf/recall.pdf"> The complexity of two-person zero-sum games in extensive form </a>by Koller D, Megiddo N. Games and economic behavior, 1992. <a href="link">  </a> </summary> This paper investigates the complexity of finding max-min strategies for finite two-person zero-sum games in the extensive form. The problem of determining whether a player with imperfect recall can guarantee himself a certain payoff is shown to be NP-hard. When both players have imperfect recall, this is even harder. Moreover, the max-min behavior strategy of such a player may use irrational numbers. Thus, for games with imperfect recall, computing the max-min strategy or the value of the game is a hard problem. For a game with perfect recall, we present an algorithm for computing a max-min behavior strategy, which runs in time polynomial in the size of the game. <br> - </details>

<details> <summary> <a href="link"> Efficient computation of equilibria for extensive two-person games </a>by Koller D, Megiddo N, Von Stengel B. Games and economic behavior, 1996. <a href="link">  </a> </summary> The Nash equilibria of a two-person, non-zero-sum game are the solutions of a certain linear complementarity problem (LCP). In order to use this for solving a game in extensive form, the game must first be converted to a strategic description such as the normal form. The classical normal form, however, is often exponentially large in the size of the game tree. If the game has perfect recall, a linear-sized strategic description is the sequence form. For the resulting small LCP, we show that an equilibrium is found efficiently by Lemke’s algorithm, a generalization of the Lemke–Howson method. <br> - </details>

<details> <summary> <a href="https://www.tau.ac.il/~samet/papers/learning-to-play.pdf">  Learning to play games in extensive form by valuation </a>by Phillipe Jehiel, Dov Samet. NAJ Economics, 2001. <a href="link">  </a> </summary> Game theoretic models of learning which are based on the strategic form of the game cannot explain learning in games with large extensive form. We study learning in such games by using valuation of moves. A valuation for a player is a numeric assessment of her moves that purports to reflect their desirability. We consider a myopic player, who chooses moves with the highest valuation. Each time the game is played, the player revises her valuation by assigning the payoff obtained in the play to each of the moves she has made. We show for a repeated win–lose game that if the player has a winning strategy in the stage game, there is almost surely a time after which she always wins. When a player has more than two payoffs, a more elaborate learning procedure is required. We consider one that associates with each move the average payoff in the rounds in which this move was made. When all players adopt this learning procedure, with some perturbations, then, with probability 1 there is a time after which strategies that are close to subgame perfect equilibrium are played. A single player who adopts this procedure can guarantee only her individually rational payoff <br> - </details>

<details> <summary> <a href="https://core.ac.uk/download/pdf/6805957.pdf"> Reexamination of the perfectness concept for equilibrium points in extensive games </a> by Selten, Reinhard. International Journal of Game Theory, 1975. <a href="link">  </a> </summary> The concept of a perfect equilibrium point has been introduced in order to exclude the possibility that disequilibrium behavior is prescribed on unreached subgames. (Selten 1965 and 1973). Unfortunately this definition of perfectness does not remove all difficulties which may arise with respect to unreached parts of the game. It is necessary to reexamine the problem of defining a satisfactory non-cooperative equilibrium concept for games in extensive form. Therefore a new concept of a perfect equilibrium point will be introduced in this paper. In retrospect the earlier use of the word "perfect" was premature. Therefore a perfect equilibrium point in the old Sense will be called "subgame perfect". The new definition of perfectness has the property that a perfect equilibrium point is always subgame perfect but a subgame perfect equilibrium point may not be perfect. It will be shown that every finite extensive game with perfect recall has at least one perfect equilibrium point. Since subgame perfectness cannot be detected in the normal form, it is clear that for the purpose of the investigation of the problem of perfectness, the normal form is an inadequate representation of the extensive form. It will be convenient to introduce an "agent normal form" as a more adequate representation of games with perfect recall. <br> - </details>

<br/>

### Fictitious Play

<details> <summary> <a href="https://dash.harvard.edu/bitstream/handle/1/3198694/fudenberg_consistency.pdf?sequence=2&origin=publicationDetail"> Consistency and Cautious Fictitious Play </a>by Drew Fudenberg, David K. Levine. Economic Dynamics and Control, 1995. <a href="link">  </a> </summary> We study a variation of fictitious play, in which the probability of each action is an exponential function of that action’s utility against the historical frequency of opponents’ play. Regardless of the opponents’ strategies, the utility received by an agent using this rule is nearly the best that could be achieved against the historical frequency. Such rules are approximately optimal in i.i.d. environments, and guarantee nearly the minmax regardless of opponents’ behavior. Fictitious play shares these properties provided it switches “infrequently” between actions. We also study the long run outcomes when all players use consistent and cautious rules <br> - </details>

<br/>

### Foundational Theory

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/neumann44a.pdf"> Theory of Games and Economic Behaviour </a>by von Neumann, J., Morgenstern, O. Princeton University Press, 1944. <a href="link">  </a> </summary> This is the classic work upon which modern-day game theory is based. What began more than sixty years ago as a modest proposal that a mathematician and an economist write a short paper together blossomed, in 1944, when Princeton University Press published Theory of Games and Economic Behavior. In it, John von Neumann and Oskar Morgenstern conceived a groundbreaking mathematical theory of economic and social organization, based on a theory of games of strategy. Not only would this revolutionize economics, but the entirely new field of scientific inquiry it yielded--game theory--has since been widely used to analyze a host of real-world phenomena from arms races to optimal policy choices of presidential candidates, from vaccination policy to major league baseball salary negotiations. And it is today established throughout both the social sciences and a wide range of other sciences. <br> - </details>

<details> <summary> <a href="https://gtl.csa.iisc.ac.in/gametheory/Classics/NCG.pdf"> Non-cooperative games </a>by J. Nash. Annals of Mathematics, 1951. <a href="link">  </a> </summary> Theory of n-person games called cooperative in the absence of coalitions. <br> - </details>

<details> <summary> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1063129/"> Equilibrium Points in N-Person Games </a> by Nash, John F.. Proceedings of the National Academy of Sciences of the United States of America, 1950. <a href="link">  </a> </summary> nan <br> - </details>
<details> <summary> <a href="https://kylewoodward.com/blog-data/pdfs/references/kohlberg+mertens-econometrica-journal-of-the-econometric-society-1986A.pdf"> On the strategic stability of equilibria </a>by Kohlberg E, Mertens JF. Econometrica: Journal of the Econometric Society, 1986. <a href="link">  </a> </summary> A basic problem in the theory of noncooperative games is the following: which Nash equilibria are strategically stable, i.e. self-enforcing, and does every game have a strategically stable equilibrium? We list three conditions which seem necessary for strategic stability- backwards induction, iterated dominance, and invariance-and define a set-valued equilibrium concept that satisfies all three of them. We prove that every game has at least one such equilibrium set. Also, we show that the departure from the usual notion of single-valued equilibrium is relatively minor, because the sets reduce to points in all generic games. <br> - </details>

<details> <summary> <a href="https://kylewoodward.com/blog-data/pdfs/references/kohlberg+mertens-econometrica-journal-of-the-econometric-society-1986A.pdf"> On the strategic stability of equilibria </a>by Kohlberg E, Mertens JF. Econometrica: Journal of the Econometric Society, 1986. <a href="link">  </a> </summary> A basic problem in the theory of noncooperative games is the following: which Nash equilibria are strategically stable, i.e. self-enforcing, and does every game have a strategically stable equilibrium? We list three conditions which seem necessary for strategic stability- backwards induction, iterated dominance, and invariance-and define a set-valued equilibrium concept that satisfies all three of them. We prove that every game has at least one such equilibrium set. Also, we show that the departure from the usual notion of single-valued equilibrium is relatively minor, because the sets reduce to points in all generic games. <br> - </details>

<details> <summary> <a href="https://www.academia.edu/44301133/Notes_On_The_Theory_Of_Choice_Underground_Classics_in_Economics_by_David_Kreps"> Notes on the theory of choice </a>by Kreps, D. M. Boulder, CO: Westview Press, 1988.  <a href="link">  </a> </summary> In this book, Professor Kreps presents a first course on the basic models of choice theory that underlie much of economic theory. This course, taught for several years at the Graduate School of Business, Stanford University, gives the student an introduction to the axiomatic method of economic analysis, without placing too heavy a demand on mathematical sophistication. The course begins with the basics of choice and revealed preference theory and then discusses numerical representations of ordinal preference. Models with uncertainty come next: first is von Neumann-Morgenstern utility, and then choice under uncertainty with subjective uncertainty, using the formulation of Anscombe and Aumann, and then sketching the development of Savage's classic theory. Finally, the course delves into a number of special topics, including de Finetti's theorem, modeling choice on a part of a larger problem, dynamic choice, and the empirical evidence against the classic models. <br> - </details>

<details> <summary> <a href="link"> Games with incomplete information played by “Bayesian” players, Part I. The basic model </a>by Harsanyi JC. Management science, 1967. <a href="link">  </a> </summary> The paper develops a new theory for the analysis of games with incomplete information where the players are uncertain about some important parameters of the game situation, such as the payoff functions, the strategies available to various players, the information other players have about the game, etc. However, each player has a subjective probability distribution over the alternative possibilities. 

In most of the paper it is assumed that these probability distributions entertained by the different players are mutually "consistent", in the sense that they can be regarded as conditional probability distributions derived from a certain "basic probability distribution" over the parameters unknown to the various players. But later the theory is extended also to cases where the different players' subjective probability distributions fail to satisfy this consistency assumption. 

In cases where the consistency assumption holds, the original game can be replaced by a game where nature first conducts a lottery in accordance with the basic probablity distribution, and the outcome of this lottery will decide which particular subgame will be played, i.e., what the actual values of the relevant parameters will be in the game. Yet, each player will receive only partial information about the outcome of the lottery, and about the values of these parameters. However, every player will know the "basic probability distribution" governing the lottery. Thus, technically, the resulting game will be a game with complete information. It is called the Bayes-equivalent of the original game. Part I of the paper describes the basic model and discusses various intuitive interpretations for the latter. Part II shows that the Nash equilibrium points of the Bayes-equivalent game yield "Bayesian equilibrium points" for the original game. Finally, Part III considers the main properties of the "basic probablity distribution". 
 <br> - </details>

<details> <summary> <a href="link"> Games with incomplete information played by “Bayesian” players, part II. Bayesian equilibrium points </a>by Harsanyi JC. Management science, 1968. <a href="link">  </a> </summary> Part I of this paper has described a new theory for the analysis of games with incomplete information. It has been shown that, if the various players' subjective probability distributions satisfy a certain mutual-consistency requirement, then any given game with incomplete information will be equivalent to a certain game with complete information, called the "Bayes-equivalent" of the original game, or briefly a "Bayesian game." 

Part II of the paper will now show that any Nash equilibrium point of this Bayesian game yields a "Bayesian equilibrium point" for the original game and conversely. This result will then be illustrated by numerical examples, representing two-person zero-sum games with incomplete information. We shall also show how our theory enables us to analyse the problem of exploiting the opponent's erroneous beliefs. However, apart from its indubitable usefulness in locating Bayesian equilibrium points, we shall show it on a numerical example (the Bayes-equivalent of a two-person cooperative game) that the normal form of a Bayesian game is in many cases a highly unsatisfactory representation of the game situation and has to be replaced by other representations (e.g., by the semi-normal form). We shall argue that this rather unexpected result is due to the fact that Bayesian games must be interpreted as games with "delayed commitment" whereas the normal-form representation always envisages a game with "immediate commitment." 
 <br> - </details>

<details> <summary> <a href="http://www3.ub.tu-berlin.de/ihv/001686626.pdf"> Game Theory: Analysis of Conflict </a>by Myerson, R. Harvard University Press, 1991. <a href="link">  </a> </summary> Eminently suited to classroom use as well as individual study, Roger Myerson's introductory text provides a clear and thorough examination of the models, solution concepts, results, and methodological principles of noncooperative and cooperative game theory. Myerson introduces, clarifies, and synthesizes the extraordinary advances made in the subject over the past fifteen years, presents an overview of decision theory, and comprehensively reviews the development of the fundamental models: games in extensive form and strategic form, and Bayesian games with incomplete information. Game Theory will be useful for students at the graduate level in economics, political science, operations research, and applied mathematics. Everyone who uses game theory in research will find this book essential. <br> - </details>

<details> <summary> <a href="https://arielrubinstein.tau.ac.il/books/GT.pdf"> A Course in Game Theory </a>by Martin J. Osborne and Ariel Rubinstein.  MIT Press, 1994. <a href="link">  </a> </summary> A Course in Game Theory presents the main ideas of game theory at a level suitable for graduate students and advanced undergraduates, emphasizing the theory's foundations and interpretations of its basic concepts. The authors provide precise definitions and full proofs of results, sacrificing generalities and limiting the scope of the material in order to do so. The text is organized in four parts: strategic games, extensive games with perfect information, extensive games with imperfect information, and coalitional games. It includes over 100 exercises. <br> - </details>

<details> <summary> <a href="https://www.emse.fr/~beaune/maml/sen-weiss-MAL99.pdf"> Learning in Multiagent Systems </a>by Sandip Sen, Gerhard Weiss. MIT Press, 1999. <a href="link">  </a> </summary> Chapter 6 of the book. <br> - </details>

<details> <summary> <a href="https://epdf.tips/the-theory-of-learning-in-games.html"> The Theory of Learning in Games </a>by Drew Fudenberg, David K. Levine. MIT Press, 1999. <a href="link">  </a> </summary> In economics, most noncooperative game theory has focused on equilibrium in games, especially Nash equilibrium and its refinements. The traditional explanation for when and why equilibrium arises is that it results from analysis and introspection by the players in a situation where the rules of the game, the rationality of the players, and the players' payoff functions are all common knowledge. Both conceptually and empirically, this theory has many problems. In The Theory of Learning in Games Drew Fudenberg and David Levine develop an alternative explanation that equilibrium arises as the long-run outcome of a process in which less than fully rational players grope for optimality over time. The models they explore provide a foundation for equilibrium theory and suggest useful ways for economists to evaluate and modify traditional equilibrium concepts. <br> - </details>

<details> <summary> <a href="http://www.misserpirat.dk/main/docs/00000013.pdf"> Algorithms, Games, and the Internet </a>by
Christos H. Papadimitrio. STOC, 2001. <a href="link">  </a> </summary> If the Internet is the next great subject for Theoretical Computer Science to model and illuminate mathematically, then Game Theory, and Mathematical Economics more generally, are likely to prove useful tools. In this talk I survey some opportunities and challenges in this important frontier. <br> - </details>

<details> <summary> <a href="http://robotics.stanford.edu/~eugnud/papers/computing-nash-journal.pdf"> Simple search methods for finding a Nash equilibrium </a> by Porter, Ryan; Nudelman, Eugene; Shoham, Yoav. Games and Economic Behavior, 2008. <a href="link">  </a> </summary> We present two simple search methods for computing a sample Nash equilibrium in a normal-form game: one for 2-player games and one for n-player games. Both algorithms bias the search towards supports that are small and balanced, and employ a backtracking procedure to efficiently explore these supports. Making use of a new comprehensive testbed, we test these algorithms on many classes of games, and show that they perform well against the state of the art--the Lemke-Howson algorithm for 2-player games, and Simplicial Subdivision and Govindan-Wilson for n-player games. <br> - </details>

<details> <summary> <a href="https://beckassets.blob.core.windows.net/product/readingsample/410258/9780521437882_excerpt_001.pdf"> Two-Sided Matching: A Study in Game-Theoretic Modeling and Analysis </a> by Roth, Alvin E.; Sotomayor, Marilda., 1990. <a href="link">  </a> </summary> Foreword Robert Auman Acknowledgment 1. Introduction Part I. One-To-One Matching: the Marriage Model: 2. Stable matchings 3. The structure of the set of stable matchings 4. Strategic questions Part II. Many-To-One Matching: Models in which Firms May Employ Many Workers: 5. The college admissions model and the labor market for medical interns 6. Discrete models with money, and more complex preferences Part III. Models of One-To-One Matching with Money as a Continuous Variable: 7. A simple model of one seller and many buyers 8. The assignment game 9. The generalization of the assignment model Part IV. Epilogue: 10. Open questions and research directions Bibliography Indexes. <br> - </details>

<details> <summary> <a href="https://www.academia.edu/169632/Economics_and_Language_Cambridge_University_Press_2000"> Economics and language index </a> by Ariel Rubinstein., 2000. <a href="link">  </a> </summary> Part I. Economics of Language: 1. Choosing semantic properties of language 2. Evolution gives meaning to language 3. Strategic considerations in pragmatics Part II. Language of Economics: 4. Decision making and language 5. On the rhetoric of game theory Part III. Comments Johan van Benthem, Tilman Borgers and Barton Lipman. <br> - </details>

<details> <summary> <a href="https://zoo.cs.yale.edu/classes/cs470/materials/aima2010.pdf"> Artificial Intelligence: A Modern Approach </a> by Stuart J. Russell; Peter Norvig., 1995. <a href="link">  </a> </summary> The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence. <br> - </details>

<details> <summary> <a href="https://www.gwern.net/docs/statistics/decision/1972-savage-foundationsofstatistics.pdf"> The Foundations of Statistics </a> by Good, Irving John; Savage, Leonard J.., 1955. <a href="link">  </a> </summary> The Foundations of Statistics <br> - </details>

<details> <summary> <a href="http://elcenia.com/iamapirate/schelling.pdf"> The Strategy of Conflict. </a> by Rapoport, Anatol; Schelling, Thomas C.. Journal of the American Statistical Association, 1961. <a href="link">  </a> </summary> I. Elements of a Theory of Strategy 1. The Retarded Science of International Strategy 2. An Essay on Bargaining 3. Bargaining, Communication, and Limited War II. A Reorientation of Game Theory 4. Toward a Theory of Interdependent Decision 5. Enforcement, Communication, and Strategic Moves 6. Game Theory and Experimental Research III. Strategy with a Random Ingredient 7. Randomization of Promises and Threats 8. The Threat That Leaves Something to Chance IV. Surprise Attack: A Study in Mutual Distrust 9. The Reciprocal Fear of Surprise Attack 10. Surprise Attack and Disarmament Appendices A. Nuclear Weapons and Limited War B. For the Abandonment of Symmetry in Game Theory C. Re-interpretation of a Solution Concept for "Noncooperative" Games Index <br> - </details>

<details> <summary> <a href="https://books.google.co.za/books/about/Expression_and_Meaning.html?id=1WqLLMG1XiIC&redir_esc=y"> Expression and Meaning: Studies in the Theory of Speech Acts </a> by Searle, John R.., 1979. <a href="link">  </a> </summary> Acknowledgements Introduction Origins of the essays 1. A taxonomy of illocutionary acts 2. Indirect speech acts 3. The logical status of fictional discourse 4. Metaphor 5. Literal meaning 6. Referential and attributive 7. Speech acts and recent linguistics Bibliography Index. <br> - </details>
<details> <summary> <a href="https://researchspace.auckland.ac.nz/bitstream/handle/2292/169/218.pdf?sequence=1"> Foundations of strategic equilibrium </a>by Hillas J, Kohlberg E. Handbook of Game Theory with Economic Applications, 2002. <a href="link">  </a> </summary> This is the third volume of the Handbook of Game Theory with Economic Applications. Since the publication of multi-Volume 1 a decade ago, game theory has continued to develop at a furious pace, and today it is the dominant tool in economic theory. The three volumes together cover the fundamental theoretical aspects, a wide range of applications to economics, several chapters on applications to political science and individual chapters on applications to disciplines as diverse as evolutionary biology, computer science, law, psychology and ethics. The authors are the most eminent practitioners in the field, including three Nobel Prize winners.The topics covered in the present volume include strategic ("Nash") equilibrium; incomplete information; two-person non-zero-sum games; noncooperative games with a continuum of players; stochastic games; industrial organization; bargaining, inspection; economic history; the Shapley value and its applications to perfectly competitive economies, to taxation, to public goods and to fixed prices; political science; law mechanism design; and game experimentation. <br> - </details>

<details> <summary> <a href="https://epdf.tips/reasoning-about-uncertainty-2nd-edition.html"> Reasoning about uncertainty </a>by Halpern, Joseph Y. MIT press, 2017. <a href="link">  </a> </summary> Uncertainty is a fundamental and unavoidable feature of daily life; in order to deal with uncertaintly intelligently, we need to be able to represent it and reason about it. In this book, Joseph Halpern examines formal ways of representing uncertainty and considers various logics for reasoning about it. While the ideas presented are formalized in terms of definitions and theorems, the emphasis is on the philosophy of representing and reasoning about uncertainty; the material is accessible and relevant to researchers and students in many fields, including computer science, artificial intelligence, economics (particularly game theory), mathematics, philosophy, and statistics.

Halpern begins by surveying possible formal systems for representing uncertainty, including probability measures, possibility measures, and plausibility measures. He considers the updating of beliefs based on changing information and the relation to Bayes' theorem; this leads to a discussion of qualitative, quantitative, and plausibilistic Bayesian networks. He considers not only the uncertainty of a single agent but also uncertainty in a multi-agent framework. Halpern then considers the formal logical systems for reasoning about uncertainty. He discusses knowledge and belief; default reasoning and the semantics of default; reasoning about counterfactuals, and combining probability and counterfactuals; belief revision; first-order modal logic; and statistics and beliefs. He includes a series of exercises at the end of each chapter. <br> - </details>

<br/>

### General-Sum Games

<details> <summary> <a href="https://webdocs.cs.ualberta.ca/~bowling/papers/00icml.pdf"> Convergence Problems of General-Sum Multiagent Reinforcement Learning </a>by Michael Bowling. ICML, 2000. </summary> Stochastic games are a generalization of MDPs to multiple agents, and can be used as a framework for investigating multiagent learning. Hu and Wellman (1998) recently proposed a multiagent Q-learning method for general-sum stochastic games. In addition to describing the algorithm, they provide a proof that the method will converge to a Nash equilibrium for the game under specified conditions. The convergence depends on a lemma stating that the iteration used by this method is a contraction mapping. Unfortunately the proof is incomplete. In this paper we present a counterexample and flaw to the lemma’s proof. We also introduce strengthened assumptions under which the lemma holds, and examine how this affects the classes of games to which the theoretical result can be applied  <br> - </details>

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/littman01a.pdf"> Friend-or-foe Q-learning in general-sum games </a>by Michael L. Littman. ICML, 2001. <a href="link">  </a> </summary> This paper describes an approach to reinforcement learning in multiagent general-sum games in which a learner is told to treat each other agent as either a friend" or foe". This Q-learning-style algorithm provides strong convergence guarantees compared to an existing Nash-equilibrium-based learning rule. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-02/SS02-02-012.pdf"> Correlated-Q Learning </a>by Amy Greenwald, Keith Hall, Roberto Serrano. NeurIPS workshop on multi-agent learning, 2002. <a href="link">  </a> </summary> Bowling named two desiderata for multiagent learning algorithms: rationality and convergence. This paper introduces correlated-Q learning, a natural generalization of Nash-Q and FF-Q that satisfies these criteria. Nash-Q satisfies rationality, but in general it does not converge. FF-Q satisfies convergence, but in general it is not rational. Correlated-Q satisfies rationality by construction. This papers demonstrates the empirical convergence of correlated-Q on a standard testbed of general-sum Markov games. <br> - </details>

<details> <summary> <a href="https://www.jmlr.org/papers/volume4/hu03a/hu03a.pdf"> Nash Q-Learning for General-Sum Stochastic Games </a>by Junling Hu, Michael Wellman. JMLR, 2003. <a href="link">  </a> </summary> We extend Q-learning to a noncooperative multiagent context, using the framework of general-sum stochastic games. A learning agent maintains Q-functions over joint actions, and performs updates based on assuming Nash equilibrium behavior over the current Q-values. This learning protocol provably converges given certain restrictions on the stage games (defined by Q-values) that arise during learning. Experiments with a pair of two-player grid games suggest that such restrictions on the game structure are not necessarily required. Stage games encountered during learning in both grid environments violate the conditions. However, learning consistently converges in the first grid game, which has a unique equilibrium Q-function, but sometimes fails to converge in the second, which has three different equilibrium Q-functions. In a comparison of offline learning performance in both games, we find agents are more likely to reach a joint optimal path with Nash Q-learning than with a single-agent Q-learning method. When at least one agent adopts Nash Q-learning, the performance of both agents is better than using single-agent Q-learning. We have also implemented an online version of Nash Q-learning that balances exploration with exploitation, yielding improved performance. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Sandip-Sen-2/publication/2831062_Towards_a_Pareto-optimal_solution_in_general-sum_games/links/54dbfb250cf28d3de65e2dc8/Towards-a-Pareto-optimal-solution-in-general-sum-games.pdf"> Towards a Pareto Optimal Solution in general-sum games </a>by Sandip Sen, Stephane Airiau, Rajatish Mukherjee. AAMAS, 2003. <a href="link">  </a> </summary> Multiagent learning literature has investigated iterated two-player games to develop mechanisms that allow agents to learn to converge on Nash Equilibrium strategy profiles. Such equilibrium configuration implies that there is no motivation for one player to change its strategy if the other does not. Often, in general sum games, a higher payoff can be obtained by both players if one chooses not to respond optimally to the other player. By developing mutual trust, agents can avoid iterated best responses that will lead to a lesser payoff Nash Equilibrium. In this paper we work with agents who select actions based on expected utility calculations that incorporates the observed frequencies of the actions of the opponent(s). We augment this stochastically-greedy agents with an interesting action revelation strategy that involves strategic revealing of one's action to avoid worst-case, pessimistic moves. We argue that in certain situations, such apparently risky revealing can indeed produce better payoff than a non-revealing approach. In particular, it is possible to obtain Pareto-optimal solutions that dominate Nash Equilibrium. We present results over a large number of randomly generated payoff matrices of varying sizes and compare the payoffs of strategically revealing learners to payoffs at Nash equilibrium. <br> - </details>

<details> <summary> <a href="https://www.sciencedirect.com/science/article/pii/S0022000005800637"> On the complexity of the parity argument and other inefficient proofs of existence </a> by Papadimitriou, Christos H.. Journal of Computer and System Sciences, 1994. <a href="link">  </a> </summary> We define several new complexity classes of search problems, “between” the classes FP and FNP. These new classes are contained, along with factoring, and the class PLS, in the class TFNP of search problems in FNP that always have a witness. A problem in each of these new classes is defined in terms of an implicitly given, exponentially large graph. The existence of the solution sought is established via a simple graph-theoretic argument with an inefficiently constructive proof; for example, PLS can be thought of as corresponding to the lemma “every dag has a sink.” The new classes, are based on lemmata such as “every graph has an even number of odd-degree nodes.” They contain several important problems for which no polynomial time algorithm is presently known, including the computational versions of Sperner's lemma, Brouwer's fixpoint theorem, Chévalley's theorem, and the Borsuk-Ulam theorem, the linear complementarity problem for P-matrices, finding a mixed equilibrium in a non-zero sum game, finding a second Hamilton circuit in a Hamiltonian cubic graph, a second Hamiltonian decomposition in a quartic graph, and others. Some of these problems are shown to be complete. <br> - </details>

<br/>

### Hierachical Learning

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/a:1022140919877.pdf"> Recent Advances in Hierarchical Reinforcement Learning </a>by Andrew G. Barto, Sridhar Mahadevan. Discrete Event Dynamic Systems, 2003. <a href="link">  </a> </summary> Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting. <br> - </details>

<details> <summary> <a href="https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1183&context=cs_faculty_pubs"> Learning to Communicate and Act using Hierarchical Reinforcement Learning </a>by Mohammad Ghavamzadeh, Sridhar Mahadevan. AAMAS, 2004. <a href="link">  </a> </summary> In this paper, we address the issue of rational communication behavior among autonomous agents. The goal is for agents to learn a policy to optimize the communication needed for proper coordination, given the communication cost. We extend our previously reported cooperative hierarchical reinforcement learning (HRL) algorithm to include communication decisions and propose a new multiagent HRL algorithm, called COM-Cooperative HRL. In this algorithm, we define cooperative subtasks to be those subtasks in which coordination among agents significantly improves the performance of the overall task. Those levels of the hierarchy which include cooperative subtasks are called cooperation levels. Coordination skills among agents are learned faster by sharing information at the cooperation levels, rather than the level of primitive actions. We add a communication level to the hierarchical decomposition of the problem below each cooperation level. Before making a decision at a cooperative subtask, agents decide if it is worthwhile to perform a communication action. A communication action has a certain cost and provides each agent at a certain cooperation level with the actions selected by the other agents at the same level. We demonstrate the efficacy of the COM-Cooperative HRL algorithm as well as the relation between the communication cost and the learned communication policy using a multiagent taxi domain. <br> - </details>

<br/>

### Incomplete Information Games

<details> <summary> <a href="http://www.ma.huji.ac.il/~zamir/papers/22_IJGT85.pdf"> Formulation of bayesian analysis for games with incomplete information </a>by J-F. Mertens, S. Zamir. International Journal of Game Theory, 1985. <a href="link">  </a> </summary> A formal model is given of Harsanyi's infinite hierarchies of beliefs. It is shown that the model closes with some Bayesian game with incomplete information, and that any such game can be approximated by one with a finite number of states of world. <br> - </details>

<details> <summary> <a href="https://ai.stanford.edu/~koller/Papers/Koller+Pfeffer:IJCAI95.pdf"> Generating and Solving Imperfect Information Games </a>by Koller, D., and Pfeffer, A. IJCAI: Proceedings of the International Joint Conference on Artificial Intelligence, 1995.  <a href="link">  </a> </summary> 

<details> <summary> <a href="link"> Regret minimizing equilibria and mechanisms for games with strict type uncertainty </a>by Hyafil N, Boutilier C. arXiv preprint arXiv:1207.4147, 2012. <a href="link">  </a> </summary> Mechanism design has found considerable application to the construction of agent-interaction protocols. In the standard setting, the type (e.g., utility function) of an agent is not known by other agents, nor is it known by the mechanism designer. When this uncertainty is quantified probabilistically, a mechanism induces a game of incomplete information among the agents. However, in many settings, uncertainty over utility functions cannot easily be quantified. We consider the problem of incomplete information games in which type uncertainty is strict or unquantified. We propose the use of minimax regret as a decision criterion in such games, a robust approach for dealing with type uncertainty. We define minimax-regret equilibria and prove that these exist in mixed strategies for finite games. We also consider the problem of mechanism design in this framework by adopting minimax regret as an optimization criterion for the designer itself, and study automated optimization of such mechanisms. <br> - </details>

<br/>

### No-Regret Learning

<details> <summary> <a href="http://static.cs.brown.edu/people/amy/papers/icml.pdf"> On no-regret learning, fictitious play, and nash equilibrium </a>by Jafari, C., Greenwald, A., Gondek, D., Ercal, G. ICML, 2001. <a href="link">  </a> </summary> This paper addresses the question what is the outcome of multi-agent learning via no-regret algorithms in repeated games? Specifically, can the outcome of no-regret learning be characterized by traditional game-theoretic solution concepts, such as Nash equilibrium? The conclusion of this study is that no-regret learning is reminiscent of fictitious play: play converges to Nash equilibrium in dominancesolvable, constant-sum, and generalsum 2  2 games, but cycles exponentially in the Shapley game. Notably, however, the information required of fictitious play far exceeds that of noregret learning. <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/2004/file/88fee0421317424e4469f33a48f50cb0-Paper.pdf"> Convergence and No-Regret in Multiagent Learning </a>by Michael Bowling. NeurIPS, 2004. <a href="link">  </a> </summary> Learning in a multiagent system is a challenging problem due to two key factors. First, if other agents are simultaneously learning then the environment is no longer stationary, thus undermining convergence guarantees. Second, learning is often susceptible to deception, where the other agents may be able to exploit a learner’s particular dynamics. In the worst case, this could result in poorer performance than if the agent was not learning at all. These challenges are identifiable in the two most common evaluation criteria for multiagent learning algorithms: convergence and regret. Algorithms focusing on convergence or regret in isolation are numerous. In this paper, we seek to address both criteria in a single algorithm by introducing GIGA-WoLF, a learning algorithm for normalform games. We prove the algorithm guarantees at most zero average regret, while demonstrating the algorithm converges in many situations of self-play. We prove convergence in a limited setting and give empirical results in a wider variety of situations. These results also suggest a third new learning criterion combining convergence and regret, which we call negative non-convergence regret (NNR). <br> - </details>

<br/>

### Opponent Modelling

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1996/AAAI96-017.pdf"> Learning other agents preferences in multiagent negotiation </a>by authors. AAAI, 1996. <a href="link">  </a> </summary> In multiagent systems, an agent does not usually have complete information about the preferences and decision making processes of other agents. This might prevent the agents from making coordinated choices, purely due to their ignorance of what others want. This paper describes the integration of a learning module into a communication-intensive negotiating agent architecture. The learning module gives the agents the ability to learn about other agents’ preferences via past interactions. Over time, the agents can incrementally update their models of other agents’ preferences and use them to make better coordinated decisions. Combining both communication and learning, as two complement knowledge acquisition methods, helps to reduce the amount of communication needed on average, and is justified in situations where communication is computationally costly or simply not desirable (e.g. to preserve the individual privacy).  <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Spring/2000/SS-00-01/SS00-01-026.pdf"> Learning Models of Other Agents Using Influence Diagrams </a>by Dicky Suryadi, Piotr J. Gmytrasiewicz. AAAI, 2000. <a href="link">  </a> </summary> We adopt decision theory as a descriptive paradigm to model rational agents. We use influence diagrams as a modeling representation of agents, which is used to interact with them and to predict their behavior. In this paper, we provide a framework that an agent can use to learn the models of other agents in a multi-agent system (MAS) based on their observed behavior. Since the correct model is usually not known with certainty our agents maintain a number of possible models and assign a probability to each of them being correct. When none of the available models is likely to be correct, we modify one of them to better account for the observed behaviors. The modification refines the parameters of the influence diagram used to model the other agent’s capabilities, preferences, or beliefs. The modified model is then allowed to compete with the other models and the probability assigned to it being correct can be arrived at based on how well it predicts the behaviors of the other agent already observed. <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/53841950/Learning_Mutual_Trust20170712-17959-yl7mr9-libre.pdf?1499904603=&response-content-disposition=inline%3B+filename%3DLearning_Mutual_Trust.pdf&Expires=1670310190&Signature=ZEnHSv7fmyCN0C0xXlwqMOhEAz2Sxo~KgxCxfCtExJxTl1wTjHQvYvQy~KMgANz5LFt91gTK5FWkY3s1zfFHBptW3VclE2PdBE98kSQUIKTPQZIyfLRDKzEgh8MtpXb8qApbm-JpNTkVRDREfqfbDQB~A6icH8~dAKvTLNj71xvYiUPWyljZ~zoTbC6CnMM~5o7HQQk~sOUUjREcDb1A9Eoypkqk9HXqJQNJMgqzpU1d-yBTyaF25o3fQUj4Fhz7v7jJWqrQYUvMYsMsOdap89dQ0JfLhN2MozHO1H4oHlrA5QQ4Yq3YLDjQD7RpjBIP1Wmgo2tLHiHe3gBB8rkiaA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> Learning mutual trust </a>by Bikramjit Banerjee, Rajatish Mukherjee, Sandip Sen. Workshop on Deception, Fraud and Trust in Agent Societies, 2000. <a href="link">  </a> </summary> Multiagent learning literature has looked at iterated twoplayer games to develop mehanisms that allow agents to learn to converge on Nash Equilibrium strategy profiles. Such equilibrium configuration implies that there is no motivation for one player to hange its strategy if the other does not. Often, in general sum games, a higher payoff an be obtained by both players if one chooses not to respond optimally to the other player. By developing mutual trust, agents can avoid iterated best responses that will lead to a lesser payoff Nash Equilibrium. In this paper we consider 1-level agents (modelers) who selet ations based on expeted utility considering probability distributions over the ations of the opponent(s). We show that in certain situations, such stochastially-greedy agents an perform better (by developing mutually trusting behavior) than those that expliitly attempt to converge to Nash Equilibrium <br> - </details>

<details> <summary> <a href="http://strategicreasoning.org/wp-content/uploads/2010/03/csr01.pdf"> Learning about other agents in a dynamic multiagent system </a>by Junling Hu, Michael Wellman. Journal of Cognitive Systems Research, 2001. <a href="link">  </a> </summary> We analyze the problem of learning about other agents in a class of dynamic multiagent systems, where performance of the primary agent depends on behavior of the others. We consider an online version of the problem, where agents must learn models of the others in the course of continual interactions. Various levels of recursive models are implemented in a simulated double auction market. Our experiments show learning agents on average outperform non-learning agents who do not use information about others. Among learning agents, those with minimum recursion assumption generally perform better than the agents with more complicated, though often wrong assumptions. <br> - </details>

<br/>

### Relational Learning

<details> <summary> <a href="https://lirias.kuleuven.be/retrieve/383228"> Multi-agent Relational Reinforcement Learning </a>by Tom Croonenborghs, Karl Tuyls, Jan Ramon, and Maurice Bruynooghe. LAMAS, 2005. <a href="link">  </a> </summary> In this paper we report on using a relational state space in multi-agent reinforcement learning. There is growing evidence in the Reinforcement Learning research community that a relational representation of the state space has many benefits over a propositional one. Complex tasks as planning or information retrieval on the web can be represented more naturally in relational form. Yet, this relational structure has not been exploited for multi-agent reinforcement learning tasks and has only been studied in a single agent context so far. In this paper we explore the powerful possibilities of using Relational Reinforcement Learning (RRL) in complex multi-agent coordination tasks. More precisely, we consider an abstract multi-state coordination problem, which can be considered as a variation and extension of repeated stateless Dispersion Games. Our approach shows that RRL allows to represent a complex state space in a multi-agent environment more compactly and allows for fast convergence of learning agents. Moreover, with this technique, agents are able to make complex interactive models (in the sense of learning from an expert), to predict what other agents will do and generalize over this model. This enables to solve complex multi-agent planning tasks, in which agents need to be adaptive and learn, with more powerful tools. <br> - </details>

<br/>

### Repeated Games

<details> <summary> <a href="http://www-stat.wharton.upenn.edu/~steele/Resources/Projects/SequenceProject/Hannan.pdf"> Approximation to bayes risk in repeated plays </a>by James Hannan. Contributions to the Theory of Games, 1959. <a href="link">  </a> </summary> This paper is concerned with the development of a dynamic theoryof decision under uncertainty. The results obtained are directly applicableto the development of a dynamic theory of games in which at least one play­er is, at each stage, fully informed on the joint empirical distribution ofthe past choices of strategies of the rest. Since the decision problem canbe Imbedded in a sufficiently unspecified game theoretic model, the paperis written in the language and notation of the general two person game, in which, however, player  I’s motivation is completely unspecified. <br> - </details>

<details> <summary> <a href="https://scholars.huji.ac.il/sites/default/files/abrahamn/files/bounded.pdf"> Bounded complexity justifies cooperation in finitely repeated prisoner’s dilemma </a>by Abraham Neyman. Economic Letters, 1985. <a href="link">  </a> </summary> Cooperation in the finitely repeated prisoner's dilemma is justified, without departure from strict utility maximization or complete information, but under the assumption that there are bounds (possibly very large) to the complexity of the strategies that the players may use. <br> - </details>

<details> <summary> <a href="http://www.econ.ucla.edu/workingpapers/wp735.pdf"> Noncomputable strategies and discounted repeated games </a>by John H. Nachbar, William R. Zame. Economic Theory, 1996. <a href="link">  </a> </summary> A number of authors have used formal models of computation to capture the idea of “bounded rationality” in repeated games. Most of this literature has used computability by a finite automaton as the standard. A conceptual difficulty with this standard is that the decision problem is not “closed.” That is, for every strategy implementable by an automaton, there is some best response implementable by an automaton, but there may not exist any algorithm forfinding such a best response that can be implemented by an automaton. However, such algorithms can always be implemented by a Turing machine, the most powerful formal model of computation. In this paper, we investigate whether the decision problem can be closed by adopting Turing machines as the standard of computability. The answer we offer is negative. Indeed, for a large class of discounted repeated games (including the repeated Prisoner's Dilemma) there exist strategies implementable by a Turing machine for whichno best response is implementable by a Turing machine. <br> - </details>

<details> <summary> <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fa466323f685599a7ae5b40d0ca921cb9a2c0a0d"> On Multiagent Q-Learning in a Semi-competitive Domain </a>by Tuomas W. Sandholm, Robert H. Crites. Adaptation and Learning in Multiagent Systems, 1996. <a href="link">  </a> </summary> Q-learning is a recent reinforcement learning (RL) algorithm that does not need a model of its environment and can be used on-line. Therefore it is well-suited for use in repeated games against an unknown opponent. Most RL research has been confined to single agent settings or to multiagent settings where the agents have totally positively correlated payoffs (team problems) or totally negatively correlated payoffs (zero-sum games). This paper is an empirical study of reinforcement learning in the iterated prisoner's dilemma (IPD), where the agents' payoffs are neither totally positively nor totally negatively correlated. RL is considerably more difficult in such a domain. This paper investigates the ability of a variety of Q-learning agents to play the IPD game against an unknown opponent. In some experiments, the opponent is the fixed strategy Tit-for-Tat, while in others it is another Q-learner. All the Q-learners learned to play optimally against Tit-for-Tat. Playing against another learner was more difficult because the adaptation of the other learner creates a nonstationary environment in ways that are detailed in the paper. The learners that were studied varied along three dimensions: the length of history they received as context, the type of memory they employed (lookup tables based on restricted history windows or recurrent neural networks (RNNs) that can theoretically store features from arbitrarily deep in the past), and the exploration schedule they followed. Although all the learners faced difficulties when playing against other learners, agents with longer history windows, lookup table memories, and longer exploration schedules fared best in the IPD games. <br> - </details>

<details> <summary> <a href="http://www.dklevine.com/archive/refs4576.pdf"> Prediction, Optimization, and Learning in Repeated Games </a>by John H. Nachbar. Econometrica, 1997. <a href="link">  </a> </summary> Consider a two-player discounted repeated game in which each player optimizes with respect to a prior belief about his opponent's repeated game strategy. One would like to argue that if beliefs are cautious, then each player's best response will be in the support, loosely speaking, of his opponent's belief and that, therefore, players will learn as the game unfolds to predict the continuation path of play. If this conjecture were true, a convergence result due to Kalai and Lehrer would imply that the continuation path of the repeated game would asymptotically resemble that of a Nash equilibrium. One would thus have constructed a theory in which Nash equilibrium behavior is a necessary long-run consequence of optimization by cautious players. This paper points out an obstacle to such a theory. Loosely put, in many repeated games, if players optimize with respect to beliefs that satisfy a diversity condition termed neutrality, then each player will choose a strategy that his opponent was certain would not be played. <br> - </details>

<details> <summary> <a href="http://rob.schapire.net/papers/FreundScYY.pdf"> Adaptive Game Playing Using Multiplicative Weights </a>by Yoav Freund, Robert E. Schapire. Games and Economic Behavior, 1999. <a href="link">  </a> </summary> We present a simple algorithm for playing a repeated game. We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy. Our bounds are nonasymptotic and hold for any opponent. The algorithm, which uses the multiplicative-weight methods of Littlestone and Warmuth, is analyzed using the Kullback–Liebler divergence. This analysis yields a new, simple proof of the min–max theorem, as well as a provable method of approximately solving a game. A variant of our game-playing algorithm is proved to be optimal in a very strong sense <br> - </details>

<details> <summary> <a href="https://www.cs.utexas.edu/users/pstone/Papers/bib2html-links/threats-ATAL2001.pdf"> Implicit negotiation in repeated games </a>by Peter Stone and Michael L. Littman. ATAL, 2001. <a href="link">  </a> </summary> In business-related interactions such as the on-going high-stakes FCC spectrum auctions, explicit communication among participants is regarded as collusion, and is therefore illegal. In this paper, we consider the possibility of autonomous agents engaging in implicit negotiation via their tacit interactions. In repeated general-sum games, our testbed for studying this type of interaction, an agent using a ``best response'' strategy maximizes its own payoff assuming its behavior has no effect on its opponent. This notion of best response requires some degree of learning to determine the fixed opponent behavior. Against an unchanging opponent, the best-response agent performs optimally, and can be thought of as a ``follower, '' since it adapts to its opponent. However, pairing two best-response agents in a repeated game can result in suboptimal behavior. We demonstrate this suboptimality in several different games using variants of Q-learning as an example of a best-response strategy. We then examine two ``leader'' strategies that induce better performance from opponent followers via stubbornness and threats. These tactics are forms of implicit negotiation in that they aim to achieve a mutually beneficial outcome without using explicit communication outside of the game. <br> - </details>

<details> <summary> <a href="https://www.cs.utexas.edu/~pstone/Courses/394Rfall16/resources/week10-threats.pdf"> Leading Best-Response Strategies in Repeated Games </a>by Peter Stone, Michael L. Littman. IJCAI, 2001. <a href="link">  </a> </summary> First steps toward agents that can reason this way: Negotiation without explicit communication! <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~mmv/papers/01ijcai-mike.pdf"> Sophisticated EWA Learning and Strategic Teaching in Repeated Games </a>by Colin F. Camerer, Teck-Hua Ho, Juin-Kuan Chong. Journal of Economic Theory, 2002. <a href="https://www.Summary.so/instadeep/Multiagent-Learning-Basics-Challenges-and-Prospect-21cb7b4294b84a4188cafd184a3deed8">   </a> </summary> Most learning models assume players are adaptive (i.e., they respond only to their own previous experience and ignore others' payo® information) and behavior is not sensitive to the way in which players are matched. Empirical evidence suggests otherwise. In this paper, we extend our adaptive experienceweighted attraction (EWA) learning model to capture sophisticated learning and strategic teaching in repeated games. The generalized model assumes there is a mixture of adaptive learners and sophisticated players. An adaptive learner adjusts his behavior the EWA way. A sophisticated player rationally best-responds to her forecasts of all other behaviors. A sophisticated player can be either myopic or farsighted. A farsighted player develops multiple-period rather than single-period forecasts of others' behaviors and chooses to `teach' the other players by choosing a strategy scenario that gives her the highest discounted net present value. We estimate the model using data from p-beauty contests and repeated trust games with incomplete information. The generalized model is better than the adaptive EWA model in describing and predicting behavior. Including teaching also allows an empirical learning-based approach to reputation formation which predicts better than a quantal-response extension of the standard typebased approach. <br> - </details>

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/littman03a.pdf"> A Polynomial-time Nash Equilibrium Algorithm for Repeated Games </a>by Michael L. Littman, Peter Stone. ACM conference on Electronic commerce, 2003. <a href="link">  </a> </summary> With the increasing reliance on game theory as a foundation for auctions and electronic commerce, efficient algorithms for computing equilibria in multiplayer general-sum games are of great theoretical and practical interest. The computational complexity of finding a Nash equilibrium for a one-shot bimatrix game is a well known open problem. This paper treats a closely related problem, that of finding a Nash equilibrium for an average-payoff repeated bimatrix game, and presents a polynomial-time algorithm. Our approach draws on the “folk theorem” from game theory and shows how finite-state equilibrium strategies can be found efficiently and expressed succinctly <br> - </details>

<details> <summary> <a href="https://digitalcommons.montclair.edu/cgi/viewcontent.cgi?article=1586&context=compusci-facpubs"> The Role of Reactivity in Multiagent Learning </a>by Bikramjit Banerjee, Jing Peng. AAMAS, 2004. <a href="link">  </a> </summary> In this paper we take a closer look at a recently proposed classification scheme for multiagent learning algorithms. Based on this scheme an exploitation mechanism (we call it the Exploiter) was developed that could beat various Policy Hill Climbers (PHC) and other fair opponents in some repeated matrix games. We show on the contrary that some fair opponents may actually beat the Exploiter in repeated games. This clearly indicates a deficiency in the original classification scheme which we address. Specifically, we introduce a new measure called Reactivity that measures how fast a learner can adapt to an unexpected hypothetical change in the opponent’s policy. We show that in some games, this new measure can approximately predict the performance of a player, and based on this measure we explain the behaviors of various algorithms in the Matching Pennies game, which was inexplicable by the original scheme. Finally we show that under certain restrictions, a player that consciously tries to avoid exploitation may be unable to do so. <br> - </details>

<details> <summary> <a href="https://www.sciencedirect.com/science/article/pii/089982569290008G"> On platers with a bounded number of states </a> by Papadimitriou, Christos H. Games and Economic Behavior, 1992. <a href="link">  </a> </summary> Designing the best response automation for the repeated prisoner's dilemma with a bounded number of states is shown to be an NP-complete problem, whereas it is easy when no bound is imposed. The results of a computation regarding two-state automata are also reported. <br> - </details>

<br/>

### Robotic Teams

<details> <summary> <a href="https://www.aaai.org/Papers/Workshops/1997/WS-97-03/WS97-03-002.pdf"> Learning Roles: Behavioral Diversity in Robot Teams </a>by Tucker Balch. AAAI, 1997. <a href="">  </a> </summary> This paper describes research investigating behavioral specialization in learning robot teams. Each agent is provided a common set of skills (motor schema-based behavioral assemblages) from which it builds a taskachieving strategy using reinforcement learning. The agents learn individually to activate particular behavioral assemblages given their current situation and a reward signal. The experiments, conducted in robot soccer simulations, evaluate the agents in terms of performance, policy convergence, and behavioral diversity. The results show that in many cases, robots will autorustically diversify by choosing heterogeneous behaviors. The degree of diversification and the performance of the team depend on the reward structure. When the entire team is jointly rewarded or penalized (global reinforcement), teams tend towards heterogeneous behavior. When agents are provided feedback individually (local reinforcement), they converge to identical policies. <br> - </details>

<details> <summary> <a href="https://data.exppad.com/public/papers/Machine%20Learning/MMARL/Mataric%20(1997)%3A%20Reinforcement%20Learning%20in%20the%20Multi-Robot%20Domain.pdf"> Reinforcement Learning in the Multi-Robot Domain </a>by MAJA J. MATARIC. Autonomous Robots, 1997. <a href="link">  </a> </summary> This paper describes a formulation of reinforcement learning that enables learning in noisy, dynamic environments such as in the complex concurrent multi-robot learning domain. The methodology involves minimizing the learning space through the use of behaviors and conditions, and dealing with the credit assignment problem through shaped reinforcement in the form of heterogeneous reinforcement functions and progress estimators. We experimentally validate the approach on a group of four mobile robots learning a foraging task <br> - </details>

<details> <summary> <a href="https://web.unbc.ca/~russellt/swarm/robocupgp98.pdf"> Genetic Programming Produced Competitive Soccer Softbot Teams for RoboCup97 </a>by Sean Luke. Genetic Programming, 1998. <a href="link">  </a> </summary> At RoboCup, teams of autonomous robots or software softbots compete in simulated soccer matches to demonstrate cooperative robotics techniques in a very difficult, real-time, noisy environment. At the IJCAI/RoboCup97 softbot competition, all entries but ours used human-crafted cooperative decision-making behaviors. We instead entered a softbot team whose high-level decision making behaviors had been entirely evolved using genetic programming. Our team won its first two games against human-crafted opponent teams, and received the RoboCup Scientific Challenge Award. This report discusses the issues we faced and the approach we took to use GP to evolve our robot soccer team for this difficult environment. <br> - </details>

<details> <summary> <a href="https://smartech.gatech.edu/bitstream/handle/1853/21573/alaa99.pdf"> Reward and Diversity in Multirobot Foraging </a>by Tucker Balch. IJCAI, 1999. <a href="link">  </a> </summary> This research seeks to quantify the impact of the choice of reward function on behavioral diversity in learning robot teams. The methodology developed for this work has been applied to multirobot foraging soccer and cooperative movement. This paper focuses specifically on results in multirobot foraging. In these experiments three types of reward are used with Qlearning to train a multirobot team to forageing a local performancebased reward a global performancebased reward and a heuristic strategy referred to as shaped reinforcement. Local strategies provide each agent a specific reward according to its own behavior while global rewards provide all the agents on the team the same reward simultaneously. Shaped reinforcement provides a heuristic reward for an agent's action given its situation. The experiments indicate that local performance based rewards and shaped reinforcement generate statistically similar results, they both provide the best performance and the least diversity. Finally learned policies are demonstrated on a team of Nomadic Technologies' Nomad-150 robots. <br> - </details>

<details> <summary> <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e10cabac2e5e90dafd9ccea24578bf0cd3f987d0"> Heterogeneity in the Coevolved Behaviors of Mobile Robots: The Emergence of Specialists </a>by Mitchell A. Potter, Lisa A. Meeden, Alan C. Schult. IJCAI, 2001. <a href="link">  </a> </summary> Many mobile robot tasks can be most efficiently solved when a group of robots is utilized. The type of organization, and the level of coordination and communication within a team of robots affects the type of tasks that can be solved. This paper examines the tradeoff of homogeneity versus heterogeneity in the controlsystems by allowing a team of robots to coevolve their high-level controllers given differentlevels of difficulty of the task. Our hypothesis is that simply increasing the difficulty of a task is not enough to induce a team of robots to create specialists. The key factor is not difficulty per se, but the number of skill sets necessary to successfully solve the task. As the number of skills needed increases, the more beneficial and necessary heterogeneity becomes. We demonstrate this in the task domain of herding, where one or more robots must herd another robot into a confined space. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Fall/2004/FS-04-02/FS04-02-003.pdf"> Co-Evolving Team Capture Strategies for Dissimilar Robots </a>by H. Joseph Blumenthal, Gary B. Parker. AAAI, 2004. <a href="link">  </a> </summary> Evolving team members to act cohesively is a complex and challenging problem. To allow the greatest range of solutions in team problem solving, heterogeneous agents are desirable. To produce highly specialized agents, team members should be evolved in separate populations. Co-evolution in separate populations requires a system for selecting suitable partners for evaluation at trial time. Selecting too many partners for evaluation drives computation time to unreasonable levels, while selecting too few partners blinds the GA from recognizing highly fit individuals. In previous work, we employed a method based on punctuated anytime learning which periodically tests a number of partner combinations to select a single individual from each population to be used at trail time. We began testing our method in simulation using a two-agent box pushing task. We then expanded our research by simulating a predator-prey scenario in which all the agents had the exact same capabilities. In this paper, we report the expansion of our work by applying this method of team learning to five dissimilar robots. <br> - </details>

<details> <summary> <a href="http://nozdr.ru/data/media/biblio/kolxoz/Cs/CsLn/L/Learning%20and%20Adaption%20in%20Multi-Agent%20Systems,%201%20conf.,%20LAMAS%202005(LNCS3898,%20Springer,%202006)(ISBN%203540330534)(224s).pdf#page=185"> Efficient Reward Functions for Adaptive Multi-rover Systems </a>by Kagan Tumer, Adrian Agogino. LAMAS, 2005. <a href="link">  </a> </summary> This chapter focuses on deriving reward functions that allow multiple agents to co-evolve efficient control policies that maximize a system level reward in noisy and dynamic environments. The solution we present is based on agent rewards satisfying two crucial properties. First, the agent reward function and global reward function has to be aligned, that is, an agent maximizing its agent-specific reward should also maximize the global reward. Second, the agent has to receive sufficient “signal” from its reward, that is, an agent’s action should have a large influence over its agent-specific reward. Agents using rewards with these two properties will evolve the correct policies quickly. This hypothesis is tested in episodic and non-episodic, continuous-space multi-rover environment where rovers evolve to maximize a global reward function over all rovers. The environments are dynamic (i.e. changes over time), noisy and have restriction on communication between agents. We show that a control policy evolved using agent-specific rewards satisfying the above properties outperforms policies evolved using global rewards by up to 400%. More notably, in the presence of a larger number of rovers or rovers with noisy and communication limited sensors, the proposed method outperforms global reward by a higher percentage than in noisefree conditions with a small number of rovers. <br> - </details>

<br/>

### Stochastic Games

<details> <summary> <a href="https://apps.dtic.mil/sti/pdfs/ADA385122.pdf"> An Analysis of Stochastic Game Theory for Multiagent Reinforcement Learning </a>by Michael Bowling, Manuela Veloso. Technical Report, 2000. <a href="link">  </a> </summary> Learning behaviors in a multiagent environmentis crucial for developing and adapting multiagent systems. Reinforcement learning techniques have addressed this problem for a single agent acting in a stationary environment, which is modeled as a Markov decision process (MDP). But, multiagent environments are inherently non-stationary since the other agents are free to change their behavior as they also learn and adapt. Stochastic games, first studied in the game theory community, are a natural extension of MDPs to include multiple agents. In this paper we contribute a comprehensive presentation of the relevant techniques for solving stochastic games from both the game theory community and reinforcement learning communities. We examine the assumptions and limitations of these algorithms, and identify similarities between these algorithms, single agent reinforcement learners, and basic game theory techniques <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Nobuo-Suematsu/publication/221454885_A_multiagent_reinforcement_learning_algorithm_using_extended_optimal_response/links/00b495331616ee577e000000/A-multiagent-reinforcement-learning-algorithm-using-extended-optimal-response.pdf"> A Multiagent Reinforcement Learning Algorithm
using Extended Optimal Response </a>by Nobuo Suematsu, Akira Hayashi. AAMAS, 2002. <a href="link">  </a> </summary> Stochastic games provides a theoretical framework to multiagent reinforcement learning. Based on the framework, a multiagent reinforcement learning algorithm for zero-sum stochastic games was proposed by Littman and it was extended to general-sum games by Hu and Wellman. Given a stochastic game, if all agents learn with their algorithm, we can expect that the policies of the agents converge to a Nash equilibrium. However, agents with their algorithm always try to converge to a Nash equilibrium independent of the policies used by the other agents. In addition, in case there are multiple Nash equilibria, agents must agree on the equilibrium where they want to reach. Thus, their algorithm lacks adaptability in a sense. In this paper, we propose a multiagent reinforcement learning algorithm. The algorithm uses the extended optimal response which we introduce in this paper. It will converge to a Nash equilibrium when other agents are adaptable, otherwise it will make an optimal response. We also provide some empirical results in three simple stochastic games, which show that the algorithm can realize what we intend. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/1206.3277.pdf"> A Polynomial-time Nash Equilibrium Algorithm for Repeated Stochastic Games </a>by Enrique Munoz de Cote, Michael L. Littman. Preprint, 2012. <a href="link">  </a> </summary> We present a polynomial-time algorithm that always finds an (approximate) Nash equilibrium for repeated two-player stochastic games. The algorithm exploits the folk theorem to derive a strategy profile that forms an equilibrium by buttressing mutually beneficial behavior with threats, where possible. One component of our algorithm efficiently searches for an approximation of the egalitarian point, the fairest pareto-efficient solution. The paper concludes by applying the algorithm to a set of grid games to illustrate typical solutions the algorithm finds. These solutions compare very favorably to those found by competing algorithms, resulting in strategies with higher social welfare, as well as guaranteed computational efficiency. <br> - </details>

<details> <summary> <a href="https://link.springer.com/book/10.1007/978-94-010-0189-2"> Stochastic Games and Applications </a> by Neyman, Abraham; Sorin, Sylvain; Sorin, S.., 2003. <a href="link">  </a> </summary> This volume is based on lectures given at the NATO Advanced Study Institute on "Stochastic Games and Applications," which took place at Stony Brook, NY, USA, July 1999. It gives the editors great pleasure to present it on the occasion of L.S. Shapley's eightieth birthday, and on the fiftieth "birthday" of his seminal paper "Stochastic Games," with which this volume opens. We wish to thank NATO for the grant that made the Institute and this volume possible, and the Center for Game Theory in Economics of the State University of New York at Stony Brook for hosting this event. We also wish to thank the Hebrew University of Jerusalem, Israel, for providing continuing financial support, without which this project would never have been completed. In particular, we are grateful to our editorial assistant Mike Borns, whose work has been indispensable. We also would like to acknowledge the support of the Ecole Poly tech nique, Paris, and the Israel Science Foundation. March 2003 Abraham Neyman and Sylvain Sorin ix STOCHASTIC GAMES L.S. SHAPLEY University of California at Los Angeles Los Angeles, USA 1. Introduction In a stochastic game the play proceeds by steps from position to position, according to transition probabilities controlled jointly by the two players. <br> - </details>

<details> <summary> <a href="https://www2.cs.siu.edu/~hexmoor/classes/CS539-F10/Shapley.pdf"> Stochastic Games </a> by Shapley, Lloyd S.. Proceedings of the National Academy of Sciences of the United States of America, 1953. <a href="link">  </a> </summary> In a stochastic game the play proceeds by steps from position to position, according to transition probabilities controlled jointly by the two players. </details>

<br/>

### Theoretical and Conceptual Frameworks

<details> <summary> <a href="https://courses.cs.duke.edu/spring07/cps296.3/littman94markov.pdf"> Markov games as a framework for multiagent reinforcement learning  </a>by Michael L. Littman. ICML, 1994. <a href="link">  </a> </summary> In the Markov decision process(MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic. <br> - </details>

<details> <summary> <a href="https://www.lirmm.fr/~jq/Cours/3cycle/module/HuWellman98icml.pdf"> Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm </a>by Junling Hu, Michael P. Wellman. ICML, 1998. <a href="link">  </a> </summary> In this paper, we adopt general-sum stochastic games as a framework for multiagent reinforcement learning. Our work extends previous work by Littman on zero-sum stochastic games to a broader framework. We design a multiagent Q-learning method under this framework, and prove that it converges to a Nash equilibrium under specified conditions. This algorithm is useful for finding the optimal strategy when there exists a unique Nash equilibrium in the game. When there exist multiple Nash equilibria in the game, this algorithm should be combined with other learning techniques to find optimal strategies. <br> - </details>

<details> <summary> <a href="https://www.semanticscholar.org/paper/Multiagent-Reinforcement-Learning-in-Stochastic-Hu-Wellman/7ce14dbb9add4d9656746703babd00d8f765b22a"> Multiagent reinforcement learning in stochastic games </a>by Hu, J., Wellman, M.P. Cambridge University Press, 1999. <a href="link">  </a> </summary> We adopt stochastic games as a general framework for dynamic noncooperative systems. This framework provides a way of describing the dynamic interactions of agents in terms of individuals' Markov decision processes. By studying this framework, we go beyond the common practice in the study of learning in games, which primarily focus on repeated games or extensive-form games. For stochastic games with incomplete information, we design a multiagent reinforcement learning method which allows agents to learn Nash equilibrium strategies. We show in both theory and experiments that this algorithm converges. From the viewpoint of machine learning research, our work helps to establish the theoretical foundation for applying reinforcement learning, originally deened for single-agent systems, to multiagent systems. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Francois-Charpillet/publication/220793963_Incremental_reinforcement_learning_for_designing_multi-agent_systems/links/00b4952138f2c99034000000/Incremental-reinforcement-learning-for-designing-multi-agent-systems.pdf"> Incremental Reinforcement Learning for Designing Multi-Agent Systems </a>by Olivier Buffet, Alain Dutech, François Charpillet. AGENTS, 2001. <a href="link">  </a> </summary> Complex systems seem more easily desribed and controlled if seen as Multi-Agent Systems (MAS): constellation of satellites, multi-robots appliations... The design of a MAS is usually built "by hand", with the need of repeated simulations to tune the system. Here, we propose to build the system by having eah agent locally learn its own behavior. The use of Reinforcement Learning (RL) in the context of MAS suffers from several limitations which can make the learning task nearly impossible: Combinatorial explosion. The computational burden of RL algorithms grows exponentially with the number of states and ations in the system. Hidden global state. Usual situated agents an only rely on an imperfect, loal and partial perception of their environment. Credit assignment problem. When a positive reward is given by the environment, it is not always evident to credit positively the "good" actions that led to this reward. Our answer to these problems is to use a decentralized adapted incremental learning algorithm based on a classial RL tehnique <br> - </details>


<details> <summary> <a href="https://core.ac.uk/download/pdf/42805562.pdf"> Agent oriented programming: An overview of the framework and summary of recent research </a> by Shoham, Yoav. Artificial Intelligence, 1992. <a href="link">  </a> </summary> This is a short overview of the agent oriented programming (AOP) framework. AOP can be viewed as an specialization of object-oriented programming. The state of an agent consists of components called beliefs, choices, capabilities, commitments, and possibly others; for this reason the state of an agent is called its mental state. The mental state of agents is captured formally in an extension of standard epistemic logics: beside temporalizing the knowledge and belief operators, AOP introduces operators for commitment, choice and capability. Agents are controlled by agent programs, which include primitives for communicating with other agents. In the spirit of speech-act theory, each communication primitives is of a certain type: informing, requesting, offering, and so on. This paper describes these features in a little more detail, and summarizes recent results and ongoing AOP-related work. <br> - </details>

<br/>

### Theses

<details> <summary> <a href="http://reports-archive.adm.cs.cmu.edu/anon/1998/CMU-CS-98-187.pdf"> Layered Learning in Multi-Agent Systems </a>by Peter Stone. PhD thesis, 1998. <a href="link">  </a> </summary> Multi-agent systems in complex, real-time domains require agents to act effectively both autonomously and as part of a team. This dissertation addresses multi-agent systems consisting of teams of autonomous agents acting in real-time, noisy, collaborative, and adversarial environments. Because of the inherent complexity of this type of multi-agent system, this thesis investigates the use of machine learning within multi-agent systems. The dissertation makes four main  contributions to the fields of Machine Learning and Multi-Agent Systems. First, the thesis defines a team member agent architecture within which a exible teamstructure is presented, allowing agents to decompose the task space into exible roles and allowing them to smoothly switch roles while acting. Team organization is achieved by the introduction of a locker-room agreement as a collection of conventions followed by all team members. It defines agent roles, team formations, and pre-compiled multi-agent plans. In addition, the team member agent architecture includes a communication paradigm for domains with single-channel, low-bandwidth, unreliable communication. The communication paradigm facilitates team coordination while being robust to lost messages and active interference from opponents. Second, the thesis introduces layered learning, a general-purpose machine learning paradigm for complex domains in which learning a mapping directly from agents' sensors to their actuators is intractable. Given a hierarchical task decomposition, layered learning allows for learning at each level of the hierarchy, with learning at each level directly affecting learning at the next higher level. Third, the thesis introduces a new multi-agent reinforcement learning algorithm, namely team-partitioned, opaque-transition reinforcement learning (TPOT-RL). TPOT-RL is designed for domains in which agents cannot necessarily observe the state changes when other team members act. It exploits local, action-dependent features to aggressively generalize its input representation for learning and partitions the task among the agents, allowing them to simultaneously learn collaborative policies by observing the long-term effects of their actions. Fourth, the thesis contributes a fully functioning multi-agent system that incorporates learning in a real-time, noisy domain with teammates and adversaries. Detailed algorithmic descriptions of the agents' behaviors as well as their source code are included in the thesis. Empirical results validate all four contributions within the simulated robotic soccer domain. The generality of the contributions is verified by applying them to the real robotic soccer, and network routing domains. Ultimately, this dissertation demonstrates that by learning portions of their cognitive processes, selectively communicating, and coordinating their behaviors via common knowledge, a group of independent agents can work towards a common goal in a complex, real-time, noisy, collaborative, and adversarial environment. <br> - </details>

<details> <summary> <a href="https://apps.dtic.mil/sti/pdfs/ADA461188.pdf"> Multiagent Learning in the Presence of Agents with Limitations </a>by Michael Bowling. Thesis, 2003. <a href="link">  </a> </summary> Learning to act in a multiagent environment is a challenging problem. Optimal behavior for one agent depends upon the behavior of the other agents, which are learning as well. Multiagent environments are therefore non-stationary, violating the traditional assumption underlying single-agent learning. In addition, agents in complex tasks may have limitations, such as physical constraints or designer-imposed approximations of the task that make learning tractable. Limitations prevent agents from acting optimally, which complicates the already challenging problem. A learning agent must effectively compensate for its own limitations while exploiting the limitations of the other agents. My thesis research focuses on these two challenges, namely multiagent learning and limitations, and includes four main contributions. First, the thesis introduces the novel concepts of a variable learning rate and the WoLF (Win or Learn Fast) principle to account for other learning agents. The WoLF principle is capable of making rational learning algorithms converge to optimal policies, and by doing so achieves two properties, rationality and convergence, which had not been achieved by previous techniques. The converging effect of WoLF is proven for a class of matrix games, and demonstrated empirically for a wide-range of stochastic games. Second, the thesis contributes an analysis of the effect of limitations on the game-theoretic concept of Nash equilibria. The existence of equilibria is important if multiagent learning techniques, which often depend on the concept, are to be applied to realistic problems where limitations are unavoidable. The thesis introduces a general model for the effect of limitations on agent behavior, which is used to analyze the resulting impact on equilibria. The thesis shows that equilibria do exist for a few restricted classes of games and limitations, but even well-behaved limitations do not preserve the existence of equilibria, in general. Third, the thesis introduces GraWoLF, a general-purpose, scalable, multiagent learning algorithm. GraWoLF combines policy gradient learning techniques with the WoLF variable learning rate. The effectiveness of the learning algorithm is demonstrated in both a card game with an intractably large state space, and an adversarial robot task. These two tasks are complex and agent limitations are prevalent in both. Fourth, the thesis describes the CMDragons robot soccer team strategy for adapting to an unknown opponent. The strategy uses a notion of plays as coordinated team plans. The selection of team plans is the decision point for adapting the team to its current opponent, based on the outcome of previously executed plays. The CMDragons were the first RoboCup robot team to employ online learning to autonomously alter its behavior during the course of a game. These four contributions demonstrate that it is possible to effectively learn to act in the presence of other learning agents in complex domains when agents may have limitations. The introduced learning techniques are proven effective in a class of small games, and demonstrated empirically across a wide range of settings that increase in complexity <br> - </details>

<details> <summary> <a href="http://l.academicdirect.org/Horticulture/GAs/Refs/PhD_Wiegand&Jong_2003.pdf"> An Analysis of Cooperative Coevolutionary Algorithms </a>by R. Paul Wiegand. Thesis, 2003. <a href="link">  </a> </summary> Coevolutionary algorithms behave in very complicated, often quite counterintuitive ways. Researchers and practitioners have yet to understand why this might be the case, how to change their intuition by understanding the algorithms better, and what to do about the differences. Unfortunately, there is little existing theory available to researchers to help address these issues. Further, little empirical analysis has been done at a component level to help understand intrinsic differences and similarities between coevolutionary algorithms and more traditional evolutionary algorithms. Finally, attempts to categorize coevolution and coevolutionary behaviors remain vague and poorly defined at best. The community needs directed investigations to help practitioners understand what particular coevolutionary algorithms are good at, what they are not, and why. This dissertation improves our understanding of coevolution by posing and answering the question: “Are cooperative coevolutionary algorithms (CCEAs) appropriate for static optimization tasks?” Two forms of this question are “How long do they take to reach the global optimum” and “How likely are they to get there?” The first form of the question is addressed by analyzing their performance as optimizers, both theoretically and empirically. This analysis includes investigations into the effects of coevolution-specific parameters on optimization performance in the context of particular properties of potential problem domains. The second leg of this dissertation considers the second form of the question by looking at the dynamical properties of these algorithms, analyzing their limiting behaviors again from theoretical and empirical points of view. Two common cooperative coevolutionary pathologies are explored and illustrated, in both formal and practical settings. The result is a better understanding of, and appreciation for, the fact that CCEAs are not generally appropriate for the task of static, single-objective optimization. In the end a new view of the CCEA is offered that includes analysis-guided suggestions for how a traditional CCEA might be modified to be better suited for optimization tasks, or might be applied to more appropriate tasks, given the nature of its dynamics. <br> - </details>

<details> <summary> <a href="http://robotics.stanford.edu/~guestrin/Publications/Thesis/thesis.pdf"> Planning under uncertainty in complex structured environments </a>by Guestrin CE. Stanford University, 2003. <a href="link">  </a> </summary> Many real-world tasks require multiple decision makers (agents) to coordinate their actions in order to achieve common long-term goals. Examples include: manufacturing systems, where managers of a factory coordinate to maximize profit; rescue robots that, after an earthquake, must safely find victims as fast as possible; or sensor networks, where multiple sensors collaborate to perform a large-scale sensing task under strict power constraints. All of these tasks require the solution of complex long-term multiagent planning problems in uncertain dynamic environments. <br>
Factored Markov decision processes (MDPs) allow us to represent complex uncertain dynamic systems very compactly by exploiting problem-specific structure. Specifically, the state of the system is described by a set of variables that evolve stochastically over time using a compact representation called a dynamic Bayesian network (DBN). A DBN exploits locality by assuming that the short-term evolution of a particular variable only depends on a few other variables, e.g., the state of a section of a factory is only directly affected by a few other sections. In the long-term, all variables in a DBN usually become correlated. Factored MDPs often allow for an exponential reduction in representation complexity. However, the complexity of exact solution algorithms for such MDPs grows exponentially in the number of variables, and in the number of agents. <br>
This thesis builds a formal framework and approximate planning algorithms that exploit structure in factored MDPs to solve problems with many trillions of states and actions very efficiently. <br> - </details>

<details> <summary> <a href="https://aaltodoc.aalto.fi/bitstream/handle/123456789/2486/isbn9512273594.pdf?sequence=1&isAllowed=y"> Multiagent Reinforcement Learning in Markov Games: Asymmetric and Symmetric approaches </a>by Ville Kononen. PhD dissertation, 2004. <a href="link">  </a> </summary> Modern computing systems are distributed, large, and heterogeneous. Computers, other information processing devices and humans are very tightly connected with each other and therefore it would be preferable to handle these entities more as agents than stand-alone systems. One of the goals of artificial intelligence is to understand interactions between entities, whether they are artificial or natural, and to suggest how to make good decisions while taking other decision makers into account. In this thesis, these interactions between intelligent and rational agents are modeled with Markov games and the emphasis is on adaptation and learning in multiagent systems. Markov games are a general mathematical tool for modeling interactions between multiple agents. The model is very general, for example common board games are special instances of Markov games, and particularly interesting because it forms an intersection of two distinct research disciplines: machine learning and game theory. Markov games extend Markov decision processes, a well-known tool for modeling single-agent problems, to multiagent domains. On the other hand, Markov games can be seen as a dynamic extension to strategic form games, which are standard models in traditional game theory. From the computer science perspective, Markov games provide a flexible and efficient way to describe different social interactions between intelligent agents. This thesis studies different aspects of learning in Markov games. From the machine learning perspective, the focus is on a very general learning model, i.e. reinforcement learning, in which the goal is to maximize the long-time performance of the learning agent. The thesis introduces an asymmetric learning model that is computationally efficient in multiagent systems and enables the construction of different agent hierarchies. In multiagent reinforcement learning systems based on Markov games, the space and computational requirements grow very quickly with the number of learning agents and the size of the problem instance. Therefore, it is necessary to use function approximators, such as neural networks, to model agents in many real-world applications. In this thesis, various numeric learning methods are proposed for multiagent learning problems. The proposed methods are tested with small but non-trivial example problems from different research areas including artificial robot navigation, simplified soccer game, and automated pricing models for intelligent agents. The thesis also contains an extensive literature survey on multiagent reinforcement learning and various methods based on Markov games. Additionally, game-theoretic methods and methods originated from computer science for multiagent learning and decision making are compared. <br> - </details>

### Mechanism Design

<details> <summary> <a href="https://www.aaai.org/Papers/JAIR/Vol29/JAIR-2902.pdf"> Computationally Feasible VCG Mechanisms </a> by Nisan, N; Ronen, A.. <a href="link">  </a> </summary> A major achievement of mechanism design theory is a general method for the construction of truthful mechanisms called VCG (Vickrey, Clarke, Groves). When applying this method to complex problems such as combinatorial auctions, a diﬃculty arises: VCG mechanisms are required to compute optimal outcomes and are, therefore, computationally infeasible. However, if the optimal outcome is replaced by the results of a sub-optimal algorithm, the resulting mechanism (termed VCG-based) is no longer necessarily truthful. The ﬁrst part of this paper studies this phenomenon in depth and shows that it is near universal. Speciﬁcally, we prove that essentially all reasonable approximations or heuristics for combinatorial auctions as well as a wide class of cost minimization problems yield non-truthful VCG-based mechanisms. We generalize these results for aﬃne maximizers. <br> - </details>

<details> <summary> <a href="https://www.cambridge.org/core/product/identifier/CBO9780511800481A111/type/book_part"> Introduction to Mechanism Design (for Computer Scientists) </a> by Nisan, Noam., 2007. <a href="link">  </a> </summary> Abstract We give an introduction to the micro-economic field of Mechanism Design slightly biased toward a computer-scientist's point of view. Introduction Mechanism Design is a subfield of economic theory that is rather unique within economics in having an engineering perspective. It is interested in designing economic mechanisms, just like computer scientists are interested in designing algorithms, protocols, or systems. It is best to view the goals of the designed mechanisms in the very abstract terms of social choice . A social choice is simply an aggregation of the preferences of the different participants toward a single joint decision. Mechanism Design attempts implementing desired social choices in a strategic setting – assuming that the different members of society each act rationally in a game theoretic sense. Such strategic design is necessary since usually the preferences of the participants are private. This high-level abstraction of aggregation of preferences may be seen as a common generalization of a multitude of scenarios in economics as well as in other social settings such as political science. Here are some basic classic examples: Elections : In political elections each voter has his own preferences between the different candidates, and the outcome of the elections is a single social choice. Markets : Classical economic theory usually assumes the existence and functioning of a “perfect market.” In reality, of course, we have only interactions between people, governed by some protocols. Each participant in such an interaction has his own preferences, but the outcome is a single social choice: the reallocation of goods and money. […] <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~sandholm/cs15-892F11/selfishJ.pdf"> Algorithmic Mechanism Design </a> by Nisan, Noam.. <a href="link">  </a> </summary> We consider algorithmic problems in a distributed setting where the participants cannot be assumed to follow the algorithm but rather their own self-interest. As such participants, termed agents, are capable of manipulating the algorithm, the algorithm designer should ensure in advance that the agents' interests are best served by behaving correctly. <br> - </details>

<br/>

### Applications

#### Traffic Management

<details> <summary> <a href="http://nozdr.ru/data/media/biblio/kolxoz/Cs/CsLn/L/Learning%20and%20Adaption%20in%20Multi-Agent%20Systems,%201%20conf.,%20LAMAS%202005(LNCS3898,%20Springer,%202006)(ISBN%203540330534)(224s).pdf#page=137"> Multiagent Traffic Management: Opportunities for Multiagent Learning </a>by Kurt Dresner, Peter Stone. LAMAS, 2005. <a href="link">  </a> </summary> Traffic congestion is one of the leading causes of lost productivity and decreased standard of living in urban settings. In previous work published at AAMAS, we have proposed a novel reservation-based mechanism for increasing throughput and decreasing delays at intersections. In more recent work, we have provided a detailed protocol by which two different classes of agents (intersection managers and driver agents) can use this system. We believe that the domain created by this mechanism and protocol presents many opportunities for multiagent learning on the parts of both classes of agents. In this paper, we identify several of these opportunities and offer a first-cut approach to each <br> - </details>

<details> <summary> <a href="http://timroughgarden.org/papers/optima.pdf"> Selfish Routing and the Price of Anarchy </a> by Roughgarden, Tim., 2005. <a href="link">  </a> </summary> Selfish routing is a classical mathematical model of how self-interested users might route traffic through a congested network. The outcome of selfish routing is generally inefficient, in that it fails to optimize natural objective functions. The price of anarchy is a quantitative measure of this inefficiency. We survey recent work that analyzes the price of anarchy of selfish routing. We also describe related results on bounding the worst-possible severity of a phenomenon called Braess's Paradox, and on three techniques for reducing the price of anarchy of selfish routing. This survey concentrates on the contributions of the author's PhD thesis, but also discusses several more recent results in the area. <br> - </details>

<br/>

#### Network Routing

<details> <summary> <a href="https://proceedings.neurips.cc/paper/1993/file/4ea06fbc83cdd0a06020c35d50e1e89a-Paper.pdf"> Packet Routing in Dynamically Changing Networks: A Reinforcement Learning Approach </a>by Justin A. Boyan, Michael L. Littman. NeurIPS, 1994. <a href="link">  </a> </summary> This paper describes the Q-routing algorithm for packet routing, in which a reinforcement learning module is embedded into each node of a switching network. Only local communication is used by each node to keep accurate statistics on which routing decisions  lead to minimal delivery times. In simple experiments involving a 36-node, irregularly connected network, Q-routing proves superior to a nonadaptive algorithm based on precomputed shortest paths and is able to route efficiently even when critical aspects of the simulation, such as the network load, are allowed to vary dynamically. The paper concludes with a discussion of the tradeoff between discovering shortcuts and maintaining stable policies.  <br> - </details>

<br/>

#### Scheduling

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=699030&casa_token=-I0XGrJpynsAAAAA:lnLxEYBCkeg72ZcM-a3FKtqrYXNILofFozM8nkyCjxBIMXyU2l_9J0WMaxdAAmv_1o-udOwiFch00g&tag=1"> Multi-Machine Scheduling - A Multi-Agent Learning Approach </a>by Wilfried Brauer, Gerhard WeiB. International Conference on Multi-Agent Systems, 1998. <a href="link">  </a> </summary> Multi-machine scheduling, that is, the assigment of jobs to machines such that certain performance demands like cost and time effectiveness are fualled, is a ubiquitous and complex activity in everyday lge. This paperpresents an approach to multi-machine scheduling that follows the multiagent learningparadigm known from thefield ofDistributed Artificial Intelligence. According to this approach the machines collectively and as a whole learn and iteratively refine appropriate schedules. The major characteristic of this approach is that learning is distributed over several machines, and that the individual machines carry out their learning activities in a parallel and asynchronous way <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1993/AAAI93-039.pdf"> An implementation of the contract net protocol based on marginal cost calculations </a> by Sandholm, Tuomas. AAAI Conference on Artificial Intelligence, 1993. <a href="link">  </a> </summary> This paper presents a formalization of the bidding and awarding decision process that was left undefined in the original contract net task allocation protocol. This formalization is based on marginal cost calculations based on local agent criteria. In this way, agents having very different local criteria (based on their selfinterest) can interact to distribute tasks so that the network as a whole functions more effectively. In this model, both competitive and cooperative agents can interact. In addition, the contract net protocol is extended to allow for clustering of tasks, to deal with the possibility of a large number of announcement and bid messages and to effectively handle situations, in which new bidding and awarding is being done during the period when the results of previous bids are unknown. The protocol is verified by the TRACONET (TRAnsportation Cooperation' NET) system, where dispatch centers of different companies cooperate automatically in vehicle routing. The implementation is asynchronous and truly distributed, and it provides the agents extensive autonomy. The protocol is discussed in detail and test results with real data are presented. <br> - </details>

<br/>

#### Economics

<details> <summary> <a href="https://dash.harvard.edu/bitstream/handle/1/29410143/evolut.pdf"> The Evolution of the Labor Market for Medical Interns and Residents: A Case Study in Game Theory </a> by Roth, Alvin E.. Journal of Political Economy, 1984. <a href="link">  </a> </summary> The organization of the labor market for medical interns and residents underwent a number of changes before taking its present form in 1951. The record of these changes and the problems that prompted them provides an unusual opportunity to study the forces at work in markets of this kind. The present paper begins with a brief history and then presents a game-theoretic analysis to explain the orderly operation and longevity of the current market, in contrast to the turmoil that characterized various earlier short-lived attempts to organize the market. An analysis is also given of some contemporary problems facing the market. A subsidiary theme of the paper concerns the history of ideas: the problems encountered in the organization of this market, and some of the solutions arrived at, anticipated the discussion of such issues in the literature of economics and game theory. <br> - </details>

<details> <summary> <a href="http://www.hsxresearch.com/assets/VSM_Mgmt_Sci.pdf"> Internet-Based Virtual Stock Markets for Business Forecasting </a> by Spann, Martin; Skiera, Bernd. Management Science, 2003. <a href="link">  </a> </summary> The application of Internet-based virtual stock markets (VSMs) is an additional approach that can be used to predict short- and medium-term market developments. The basic concept involves bringing a group of participants together via the Internet and allowing them to trade shares of virtual stocks. These stocks represent a bet on the outcome of future market situations, and their value depends on the realization of these market situations. In this process, a VSM elicits and aggregates the assessments of its participants concerning future market developments. The aim of this article is to evaluate the potential use and the different design possibilities as well as the forecast accuracy and performance of VSMs compared to expert predictions for their application to business forecasting. After introducing the basic idea of using VSMs for business forecasting, we discuss the different design possibilities for such VSMs. In three real-world applications, we analyze the feasibility and forecast accuracy of VSMs, compare the performance of VSMs to expert predictions, and propose a new validity test for VSM forecasts. Finally, we draw conclusions and provide suggestions for future research. <br> - </details>

<details> <summary> <a href="https://viterbi-web.usc.edu/~shaddin/cs590fa13/papers/jobmarketsignaling.pdf"> Job Market Signaling </a> by Spence, Michael. Quarterly Journal of Economics, 1973. <a href="link">  </a> </summary> 1. Introduction, 355. — 2. Hiring as investment under uncertainty, 356. — 3. Applicant signaling, 358. — 4. Informational feedback and the definition of equilibrium, 359. — 5. Properties of informational equilibria: an example, 361. — 6. The informational impact of indices, 368. — Conclusions, 374. <br> - </details>
### Commerce and Economics 

<details> <summary> <a href="https://publications.ut-capitole.fr/id/eprint/14919/1/Laffont_14919.pdf"> Characterization of satisfactory mechanisms for the revelation of preferences for public goods </a>by Green J, Laffont JJ. Econometrica: Journal of the Econometric Society, 1977. <a href="link">  </a> </summary> Problems connected with social decision processes in models with public goods have troubled economists for some time. Recently a negative result of Gibbard and Satterthwaite precluded the possibility of finding non-dictatorial deterministic mechanisms for choosing social states which have the property that individuals believe it to be impossible to manipulate the mechanism to their own advantage. They may, in particular, reveal preferences other than their own, and the resulting social choice may then be distorted away from the Pareto optimum relative to their true tastes. Gibbard and Satterthwaite's requirements are quite strong. In particular, arbitrary individual preferences are allowed. In a more specialized context, where the decision concerns the level of public goods and monetary transfers among individuals, Groves and Groves and Loeb assumed that preferences are monotonic in income and that the willingness-to-pay for alternative levels of the public good is independent of income. In such environments they found a class of mechanisms with the properties that stating one's true preferences is a dominant strategy for each individual and that a Pareto optimum is selected. In this paper, we show that the mechanisms proposed by Groves and Loeb are the only ones which have these desirable characteristics. This result enables us to concentrate the search for optimal mechanisms within this class and to use criteria other than individual incentive compatibility to distinguish among these. We have pursued this direction in other papers. In addition, we show that well-defined mechanisms which select Pareto optimal outcomes (referred to as satisfactory mechanisms), independently of the question of truthful revelation, are essentially isomorphic to the mechanisms proposed by Groves. 
  <br> - </details>

<details> <summary> <a href="link"> Combinatorial information market design </a>by Hanson R. Information Systems Frontiers, 2003.
 <a href="link">  </a> </summary> Information markets are markets created to aggregate information. Such markets usually estimate a probability distribution over the values of certain variables, via bets on those values. Combinatorial information markets would aggregate information on the entire joint probability distribution over many variables, by allowing bets on all variable value combinations. To achieve this, we want to overcome the thin market and irrational participation problems that plague standard information markets. Scoring rules avoid these problems, but instead suffer from opinion pooling problems in the thick market case. Market scoring rules avoid all these problems, by becoming automated market makers in the thick market case and simple scoring rules in the thin market case. Logarithmic versions have cost and modularity advantages. After introducing market scoring rules, we consider several design issues, including how to represent variables to support both conditional and unconditional estimates, how to avoid becoming a money pump via errors in calculating probabilities, and how to ensure that users can cover their bets, without needlessly preventing them from using previous bets as collateral for future bets. <br> - </details>

 <details> <summary> <a href="http://web.mit.edu/jnt/www/Papers/J097-04-joh-ncgame.pdf"> Efficiency loss in a network resource allocation game </a>by Johari R, Tsitsiklis JN. Mathematics of Operations Research, 2004. <a href="link">  </a> </summary> We explore the properties of a congestion game in which users of a congested resource anticipate the effect of their actions on the price of the resource. When users are sharing a single resource, we establish that the aggregate utility received by the users is at least 3/4 of the maximum possible aggregate utility. We also consider extensions to a network context, where users submit individual payments for each link in the network they may wish to use. In this network model, we again show that the selfish behavior of the users leads to an aggregate utility that is no worse than 3/4 of the maximum possible aggregate utility. We also show that the same analysis extends to a wide class of resource allocation systems where end users simultaneously require multiple scarce resources. These results form part of a growing literature on the “price of anarchy,” i.e., the extent to which selfish behavior affects system efficiency. <br> - </details>

<details> <summary> <a href="http://old.gtcenter.org/Archive/Conf07/Downloads/Conf/Guo445.pdf"> Worst-case optimal redistribution of VCG payments </a>by Guo M, Conitzer V. In Proceedings of the 8th ACM conference on Electronic commerce, 2007. <a href="link">  </a> </summary> For allocation problems with one or more items, the wellknown Vickrey-Clarke-Groves (VCG) mechanism is efficient, strategy-proof, individually rational, and does not incur a deficit. However, the VCG mechanism is not (strongly) budget balanced: generally, the agents’ payments will sum to more than 0. If there is an auctioneer who is selling the items, this may be desirable, because the surplus payment corresponds to revenue for the auctioneer. However, if the items do not have an owner and the agents are merely interested in allocating the items efficiently among themselves, any surplus payment is undesirable, because it will have to flow out of the system of agents. In 2006, Cavallo proposed a mechanism that redistributes some of the VCG payment back to the agents, while maintaining efficiency, strategy-proofness, individual rationality, and the non-deficit property. In this paper, we extend this result in a restricted setting. We study allocation settings where there are multiple indistinguishable units of a single good, and agents have unit demand. (For this specific setting, Cavallo’s mechanism coincides with a mechanism proposed by Bailey in 1997.) Here we propose a family of mechanisms that redistribute some of the VCG payment back to the agents. All mechanisms in the family are efficient, strategyproof, individually rational, and never incur a deficit. The family includes the Bailey-Cavallo mechanism as a special case. We then provide an optimization model for finding the optimal mechanism — that is, the mechanism that maximizes redistribution in the worst case — inside the family, and show how to cast this model as a linear program. We give both numerical and analytical solutions of this linear program, and the (unique) resulting mechanism shows significant improvement over the Bailey-Cavallo mechanism (in the worst case). Finally, we prove that the obtained mechanism is optimal among all anonymous deterministic mechanisms that satisfy the above properties. <br> - </details>

<details> <summary> <a href="https://nscpolteksby.ac.id/ebook/files/Ebook/Computer%20Engineering/Algorithmic%20Game%20Theory%20%282007%29/22.%20Chapter%2021%20-%20The%20Price%20of%20Anarchy%20and%20the%20Design%20of%20Scalable%20Resource%20Allocation%20Mechanisms.pdf"> The Price of Anarchy and the Design of Scalable Resource Allocation Mechanisms </a>by Johari R. Algorithmic Game Theory, 2007. <a href="link">  </a> </summary> In this chapter, we study the allocation of a single infinitely divisible resource among multiple competing users. While we aim for efficient allocation of the resource, the task is complicated by the fact that users’ utility functions are typically unknown to the resource manager. We study the design of resource allocation mechanisms that are approximately efficient (i.e., have a low price of anarchy), with low communication requirements (i.e., the strategy spaces of users are low dimensional). Our main results concern the proportional allocation mechanism, for which a tight bound on the price of anarchy can be provided. We also show that in a wide range of market mechanisms that use a single market-clearing price, the proportional allocation mechanism minimizes the price of anarchy. Finally, we relax the assumption of a single market-clearing price, and show that by extending the class of Vickrey–Clarke–Groves mechanisms all Nash equilibria can be guaranteed to be fully efficient. <br> - </details>

<br/>

### *Misc Theory and Approaches

The papers below were found to be difficult to categorise and therefore are presented under the general "catch-all" category *miscellaneous theory and approaches*.

<details> <summary> <a href="https://members.loria.fr/OBuffet/papiers/aamas02-2pages.pdf"> Learning to Weigh Basic Behaviors in Scalable Agents </a>by Olivier Buffet, Alain Dutech Francois, Charpillet. AAMAS, 2002. <a href="link">  </a> </summary> We are working on the use of Reinforcement Learning [RL](3) algorithms to design automatically reactive situated agents limited to only local perceptions. Unfortunately, as good RL algorithms suffer from combinatorial explosion, their use is generally limited to simple problems. As shown on the tile-world example of figure 1, we propose to overcome these difficulties by making the hypothesis, as in Brook’s subsumption architecture [1], that a complex problem can be efficiently dealt with if considered as a combination of simple problems. This short presentation gives basic ideas on RL algorithms (section 2). Then the three steps of our method are presented: how basic behaviors are learned for each basic motivation (sec. 3), how the scene is decomposed in key figures to find the basic behaviors currently involved (sec. 4), and how to combine them into a complex global behavior using learned weights (sec. 5). A few words are given on the experiments conducted on the tile-world problem (sec. 6) and precede a conclusion <br> - </details>

<details> <summary> <a href="https://www.kellogg.northwestern.edu/research/math/papers/590.pdf"> Multistage Games with Communication </a> by Myerson, Roger B.. Econometrica, 1986. <a href="link">  </a> </summary> This paper considers multistage games with communication mechanisms that can be implemented by a central mediator. In a communication equilibrium, no player expects ex ante to gain by manipulating his reports or actions. A sequential communication equilibrium is a communication equilibrium with a conditional probability system under which no player could ever expect to gain by manipulation, even after zero-probability events. Codominated actions are defined. It is shown that a communication equilibrium is a sequential communication equilibrium if and only if it never uses codominated actions. Predominant communication equilibria are defined by iterative elimination of codominated actions and are shown to exist. <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~sandholm/cs15-892F13/algorithmic-game-theory.pdf"> Algorithmic game theory </a> by Noam Nisan, 2007. <a href="link">  </a> </summary> Over the last few years, there has been explosive growth in the research done at the interface of computer science, game theory, and economic theory, largely motivated by the emergence of the Internet. Algorithmic Game Theory develops the central ideas and results of this new and exciting area. More than 40 of the top researchers in this field have written chapters whose topics range from the foundations to the state of the art This book contains an extensive treatmentof algorithms for equilibria in games and markets, computational auctions and mechanism design, and the “price of anarchy,” as well as applications in networks, peer-to-peer systems, security, information markets, and more. This book will be of interest to students, researchers, and practitioners in theoretical computer science, economics, networking, artificial intelligence, operations research, and discrete mathematics. <br> - </details>

<details> <summary> <a href="http://web.stanford.edu/group/cslipublications/cslipublications/pdf/1575863545.pdf"> The use of language </a> by Parikh, Prashant., 2001. <a href="link">  </a> </summary> Building on the work of J.L. Austin and Paul Grice, "The Use of Language" develops an original and systematic game-theoretic account of communication, speaker meaning and addressee interpretation, extending this analysis to conversational implicature and the Gricean maxims, illocutionary force, miscommunication, visual representation and visial implicature, and aspects of discourse. <br> - </details>

<details> <summary> <a href="https://www.cs.ubc.ca/~poole/ci.html"> Computational Intelligence: A Logical Approach </a> by Poole, David; Mackworth, Alan K.; Goebel, Randy., 1998. <a href="link">  </a> </summary> What is Computational Intelligence? Representation and Reasoning. Representation and Reasoning Systems. Extending the Language with Functional Symbols. <br> - </details>

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/rao91a.pdf"> Modeling Rational Agents within a BDI-Architecture </a> by Anand Srinivasa Rao; M. Georgeff. International Conference on Principles of Knowledge Representation and Reasoning, 1997. <a href="link">  </a> </summary> Intentions, an integral part of the mental state of an agent, play an important role in determining the behavior of rational agents as they seek to attain their goals. In this paper, a formalization of intentions based on a branching-time possible-worlds model is presented. It is shown how the formalism realizes many of the important elements of Bratman's theory of intention. In particular, the notion of intention developed here has equal status with the notions of belief and desire, and cannot be reduced to these concepts. This allows di erent types of rational agents to be modeled by imposing certain conditions on the persistence of an agent's beliefs, goals, and intentions. Finally, the formalism is compared with Bratman's theory of intention and Cohen and Levesque's formalization of intentions. <br> - </details>

<details> <summary> <a href="https://www.sciencedirect.com/science/article/abs/pii/0022053181900181"> Games of perfect information, predatory pricing and the chain-store paradox </a> by Rosenthal, Robert; Rosenthal, Robert W.. Journal of Economic Theory, 1981. <a href="link">  </a> </summary> The thesis of this paper is that finite, noncooperative games possessing both complete and perfect information ought to be treated like one-player decision problems. That is, players ought to assign at every move subjective probabilities to every subsequent choice in the game and ought to make decisions via backward induction. This view is in contrast with the gametheoretic approach of Nash equilibrium. After expanding on this view for games in the abstract in Sections 2 and 3, attention is turned in Section 4 to an example due to Reinhard Selten, called the chain-store paradox, which possesses the flavor of a situation involving a predatory-pricing monopolist. It is argued that for the chain-store game the decision-analytic approach leads, under certain assumptions, to more realistic outcomes than the standard Nash-equilibrium approach. <br> - </details>

<details> <summary> <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=552180b3e11784e3edf177b0de607555992b8078"> Contract Types for Satisficing Task Allocation:I Theoretical Results </a> by Sandholm, Tuomas., 2002. <a href="link">  </a> </summary> We analyze task reallocation where individually rational ([R) agents (re)contract tasks among themselves based on marginal costs. A task allocation graph is introduced as a tool for analyzing contract types. Traditional single task contracts always have a short path (sequence of contracts) to the optimal task allocation but an IR path may not exist, or it may not be short. We analyze an algorithm for finding the shortest IR path. Next we introduce cluster contracts, swaps, and multiagent contracts. Each of the four contract types avoids some local optima that the others do not. Even if the protocol is equipped with all four types, local optima exist. To attack this problem, we introduce OCSMcontracts which combine the ideas behind the four earlier types into an atomic contract type. If the protocol is equipped with OCSM-contracts, any sequence of IR contracts leads to the optimal task allocation in a finite number of steps: an oracle--or speculation--is not needed for choosing the path (no subset of OCSMcontracts suffices even with an oracle). This means that the multiagent search does not need to backtrack. This is a powerful result for small problem instances. For large ones, the anytime feature of our multicontract-type algorithm--with provably monotonic improvement of each agent’s solution--is more important. 1 <br> - </details>

<details> <summary> <a href="https://www.tau.ac.il/~schmeid/PDF/D_Schmeidler_Eq_Nonatomic_Games_a.pdf"> EQUILIBRIUM POINTS OF NONATOMIC GAMES </a> by O. A. B. Space; Ali Khan; M. A. Kahn., 2010. <a href="link">  </a> </summary> Schmeidler's results on the equilibrium points of nonatomic games with strategy sets in Euclidean «-space are generalized to nonatomic games with strategy sets in a Banach space. Our results also extend previous work of the author which assumed the Banach space to be separable and its dual to possess the Radon-Nikodym property. Our proofs use recent results in functional analysis. <br> - </details>

<details> <summary> <a href="https://linkinghub.elsevier.com/retrieve/pii/S0004370207000495"> If multi-agent learning is the answer, what is the question? </a> by Shoham, Yoav; Powers, Rob; Grenager, Trond. Artificial Intelligence, 2007. <a href="link">  </a> </summary> If multi-agent learning is the answer, what is the question? <br> - </details>

<details> <summary> <a href="https://pdf.sciencedirectassets.com/271585/1-s2.0-S0004370200X00197/1-s2.0-000437029400007N/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIB5oYMrMQHbmc5KKoUHwQ54sUTt8SqxFtm7Alsq8qonxAiEAi2udjvDXQ0VMo9DqQOf2%2BqypF31EnpEztv2DRpxiul0q1QQI0v%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDJ%2FCmVnsu6ASJXbJdCqpBESJdwhGfByTDB1pofwZFi2omzVPkpdVLxQrv9dKL%2BTq7JtE10QskDsiBOVE0EhUqs6DnrlvThIUjJWOUysy7%2BUmJdgat9KiC%2BV%2FLkd8PXIMc9v%2Fm%2BQb%2BoaYvoka14hJyfrCGCP0%2B2LMgqE%2FixX8ZvbMstO9AToI%2BjAk9oeQSZVq5E%2F2ONBAktvOTm1uK%2Bec7HxXjvL7q9i6NMWK14NLf5aJ4rrwqnSQUOuVHwxc0Y7cv%2BRFZETEzFLL1NG%2BC5iGqN2wk%2B9qMwnK0BfZpx2w%2BixpoP1ywLNH85Knch6LC4oATmJLx0%2FkgLXVJ4x7JABBPuMY9faCztbbhWS4KspacgljLlEAjUSggQX2yb895t7l5brEEz8QBgPzOnQ3FCpzkuytmQDD%2BcsHVMhkeQzhbcncbt0kYJw%2FaupzwVkFM8bncaKLW8dGmv5C92EB0el5Oc%2BzEiFsbztTPxlHNoA4oXw6nEbCleiRtx6qnbGCLwP0LGoX6BsoTgVyDrFPyy76c81w3yHTV3%2Fob0H2ESkuSe72iY0l0RVIYQfEd4d2LBDi4SLYwqBtYeiadT8tWD%2BZakwwsINvSjuj%2FzC0zffD0BVN2Q3NYD%2BxgIG1uKSz6rqyWdrm8vKw9lHKdJ0P%2BLd3n%2F5PpllV7I8SAcZIZt3ZzwAKfwn27Ea2HkMimzxi2xXV2EV9np4TIZR4EjSbfg%2FYMUIO6ZtW04%2BjO007GGHH395jFeZTfqofzS0whZLmnAY6qQFbAZ5T28ji5lstXlPwrRFO5%2FQwqMyoYiHvPvK3IWyXwZTTxXGbtzxR3k3LAecWAnNgiBt8QqrKF2RPbOt%2Bs5z1pWQ729JIcIV%2FtN3HsYibqwnxZQ8Ff%2FBjwUReCg2KbsqVIIkb8wSOlPD%2BXUnJCQtdkTWnLWEa5OrwQGaw7zESLoWdJjEFp9ND5f%2BCmSjxSHhCrEx3DUBnqOr8EW8WfscD2cM9oWzuv2qK&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221214T090916Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYZO6262GY%2F20221214%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=951973215bcbca117cc919d2d277c95acd6879e1b0652466f304754bc9a3c835&hash=dd8061ba4767598b05aa0939d4a66b292d53ec5a9b9e1df459164174f829eb6e&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=000437029400007N&tid=spdf-be792225-b3b7-45c1-8a0c-88094d0e9fbd&sid=8d0ac65c64f75049b90b0765521a3dc2f2dfgxrqb&type=client&ua=51535d500303055c005f&rr=7795ccb9d92a06c7"> On social laws for artificial agent societies: off-line design </a> by Shoham, Yoav; Tennenholtz, Moshe. Artificial Intelligence, 1995. <a href="link">  </a> </summary> Abstract   We are concerned with the utility of social laws in a computational environment, laws which guarantee the successful coexistence of multiple programs and programmers. In this paper we are interested in the off-line design of social laws, where we as designers must decide ahead of time on useful social laws. In the first part of this paper we suggest the use of social laws in the domain of mobile robots, and prove analytic results about the usefulness of this approach in that setting. In the second part of this paper we present a general model of social law in a computational system, and investigate some of its properties. This includes a definition of the basic computational problem involved with the design of multi-agent systems, and an investigation of the automatic synthesis of useful social laws in the framework of a model which refers explicitly to social laws. <br> - </details>

<details> <summary> <a href="https://www.lirmm.fr/~ferber/M2R/biblio/ai1462.pdf"> On the emergence of social conventions: modeling, analysis, and simulations </a> by Shoham, Yoav; Tennenholtz, Moshe. Artificial Intelligence, 1997. <a href="link">  </a> </summary> Abstract   We define the notion of  social conventions  in a standard game-theoretic framework, and identify various criteria of consistency of such conventions with the principle of individual rationality. We then investigate the emergence of such conventions in a stochastic setting; we do so within a stylized framework currently popular in economic circles, namely that of  stochastic games . This framework comes in several forms; in our setting agents interact with each other through a random process, and accumulate information about the system. As they do so, they continually reevaluate their current choice of strategy in light of the accumulated information. We introduce a simple and natural strategy-selection rule, called  highest cumulative reward  (HCR). We show a class of games in which HCR guarantees eventual convergence to a rationally acceptable social convention. Most importantly, we investigate the efficiency with which such social conventions are achieved. We give an analytic lower bound on this rate, and then present results about how HCR works out in practice. Specifically, we pick one of the most basic games, namely a basic  coordination  game (as defined by Lewis), and through extensive computer simulations determine not only the effect of applying HCR, but also the subtle effects of various system parameters, such as the amount of memory and the frequency of update performed by all agents. <br> - </details>

<br/>

<!-- <details> <summary> <a href="link"> title </a>by authors. Conference, year. <a href="link">  </a> </summary> abstract <br> - </details> -->
