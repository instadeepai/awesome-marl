This is a collection of older papers on the topic of learning in multi-agent systems. We sort papers by publication date and survey subtopic. Any additions to this repo are welcome.

### Auctions, bidding and negotiation

<details> <summary> <a href="https://authors.library.caltech.edu/80874/1/2118073.pdf">  A Note on Sequential Auctions </a>by DAN BERNHARDT, DAVID SCOONES. American Economic Review, 1994. <a href="link">  </a> </summary> This note explores multiobject, sequential, private-value auctions. <br> - </details>

<details> <summary> <a href="http://www2.econ.iastate.edu/tesfatsi/binmore1.pdf"> Applying Game Theory to Automated Negotiation </a>by Ken Binmore and Nir Vulkan. Workshop on Economics, Game Theory and the Internet, 1997. <a href="link">  </a> </summary> With existing technology, it is already possible for personal agents to schedule meetings for their users, to write the small print of an agreement, and for agents to search the Internet for the cheapest price. But serious negotiation cranks the difficulty of the problem up several notches. In this paper, we review what game theory has to offer in the light of experience gained in programming automated agents within the ADEPT (Advance Decision Environment for Process Tasks) project, which is currently being used by British Telecom for some purposes <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/256867/1/aamas02-byde.pdf"> Decision Procedures for Multiple Auctions </a>by Andrew Byde, Chris Preist, Nicholas R Jennings. AAMAS, 2002. <a href="link">  </a> </summary> This paper presents a decision theoretic framework that an autonomous agent can use to bid effectively across multiple, simultaneous auctions. Specifically, our framework enables an agent to make rational decisions about purchasing multiple goods from a series of auctions that operate different protocols (we deal with the English, Dutch, First-Price Sealed Bid and Vickrey cases). The framework is then used to characterize the optimal decision that an agent should take. Finally, we develop a practical algorithm that provides a heuristic approximation to this ideal <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/258572/1/pat-toit.pdf"> Developing a Bidding Agent for Multiple Heterogeneous Auctions </a>by P Anthony, NR Jennings. ACM TOIT, 2003. <a href="link">  </a> </summary> Due to the proliferation of online auctions, there is an increasing need to monitor and bid in multiple auctions in order to procure the best deal for the desired good. To this end, this paper reports on the development of a heuristic decision making framework that an autonomous agent can exploit to tackle the problem of bidding across multiple auctions with varying start and end times and with varying protocols (including English, Dutch and Vickrey). The framework is flexible, configurable, and enables the agent to adopt varying tactics and strategies that attempt to ensure that the desired item is delivered in a manner consistent with the user’s preferences. Given this large space of possibilities, we employ a genetic algorithm to search (offline) for effective strategies in common classes of environment. The strategies that emerge from this evolution are then codified into the agent’s reasoning behaviour so that it can select the most appropriate strategy to employ in its prevailing circumstances. The proposed framework has been implemented in a simulated marketplace environment and its effectiveness has been empirically demonstrated. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/263434/1/marketACM_draft.pdf"> Market-based Recommendation: Agents that Compete for Consumer Attention </a>by SANDER M. BOHTE, ENRICO GERDING, HAN LA POUTRE. ACM TOIT, 2004. <a href="link">  </a> </summary> The amount of attention space available for recommending suppliers to consumers on e-commerce sites is typically limited. We present a competitive distributed recommendation mechanism based on adaptive software agents for efficiently allocating the “consumer attention space”, or banners. In the example of an electronic shopping mall, the task is delegated to the individual shops, each of which evaluates the information that is available about the consumer and his or her interests (e.g. keywords, product queries, and available parts of a profile). Shops make a monetary bid in an auction where a limited amount of “consumer attention space” for the arriving consumer is sold. Each shop is represented by a software agent that bids for each consumer. This allows shops to rapidly adapt their bidding strategy to focus on consumers interested in their offerings. For various basic and simple models for on-line consumers, shops, and profiles, we demonstrate the feasibility of our system by evolutionary simulations as in the field of agent-based computational economics (ACE). We also develop adaptive software agents that learn bidding-strategies, based on neural networks and strategy exploration heuristics. Furthermore, we address the commercial and technological advantages of this distributed market-based approach. The mechanism we describe is not limited to the example of the electronic shopping mall, but can easily be extended to other domains. <br> - </details>

<br/>

### Bounded rationality

<details> <summary> <a href="https://dl.acm.org/doi/pdf/10.1145/195058.195445"> On complexity as bounded rationality  </a>by C.H. Papadimitriou, M. Yannakakis. STOC, 1994. <a href="link">  </a> </summary> It has been hoped that computational approaches can help resolve some well-known paradoxes in game theory. We prove that tf the repeated prisoner’s dilemma M played by finite automata with less than exponentially (in the number of rounds) many states, then cooperation can be achieved an equilibrium (while with exponentially many states, defection is the only equilibrium). We furthermore prove a generalization to arbitrary games and Pareto optimal points. Finally, we present a general model of polynomially computable games, and characterize in terms of fami!iar complexity classes ranging from NP to NEXP the natural problems that arise in relation with such games. <br> - </details>

<details> <summary> <a href="http://sfi-edu.s3.amazonaws.com/sfi-edu/production/uploads/sfi-com/dev/uploads/filer/eb/8d/eb8d7d4d-8bc0-49ac-8feb-e73c5a141a12/94-03-014.pdf"> Inductive Reasoning, Bounded Rationality and the Bar Problem </a>by W. Brian Arthur. American Economic Review, 1994. <a href="link">  </a> </summary> This paper draws on modem psychology to argue that as humans, in economic decision contexts that are complicated or ill-defined, we use not deductive, but inductive reasoning. That is, in such contexts we induce a variety of working hypotheses or mental models, act upon the most credible, and replace hypotheses with new ones if they cease to work. Inductive reasoning leads to a rich psychological world in which an agent's hypotheses or mental models compete for survival against each other, in an environment formed by other other agents' hypotheses or mental models-a world that is both evolutionary and complex. Inductive reasoning can be modeled in a variety of ways. The main body of the paper introduces and models a coordination problem-"the bar problem"-in which agents' expectations are forced to be subjective and to differ. It shows that while agents' beliefs never settle down, collectively they form an "ecology" that does converge to an equilibrium pattern. <br> - </details>

<details> <summary> <a href="https://mitpress.mit.edu/9780262681001/modeling-bounded-rationality/"> Modeling Bounded Rationality </a>by Ariel Rubinstein. MIT Press,
1998. <a href="link">  </a> </summary> The notion of bounded rationality was initiated in the 1950s by Herbert Simon; only recently has it influenced mainstream economics. In this book, Ariel Rubinstein defines models of bounded rationality as those in which elements of the process of choice are explicitly embedded. The book focuses on the challenges of modeling bounded rationality, rather than on substantial economic implications. In the first part of the book, the author considers the modeling of choice. After discussing some psychological findings, he proceeds to the modeling of procedural rationality, knowledge, memory, the choice of what to know, and group decisions.In the second part, he discusses the fundamental difficulties of modeling bounded rationality in games. He begins with the modeling of a game with procedural rational players and then surveys repeated games with complexity considerations. He ends with a discussion of computability constraints in games. The final chapter includes a critique by Herbert Simon of the author's methodology and the author's response. <br> - </details>

<br/>

### Collective Intelligence

<details> <summary> <a href="https://proceedings.neurips.cc/paper/1998/file/5129a5ddcd0dcd755232baa04c231698-Paper.pdf"> USING COLLECTIVE INTELLIGENCE TO ROUTE INTERNET TRAFFIC </a>by David H. Wolpert, Kagan Turner, Jeremy Frank. NeurIPS, 1998. <a href="link">  </a> </summary> A COllective INtelligence (COIN) is a set of interacting reinforcement learning (RL) algorithms designed in an automated fashion so that their collective behavior optimizes a global utility function. We summarize the theory of COINs, then present experiments using that theory to design COINs to control internet traffic routing. These experiments indicate that COINs outperform all previously investigated RL-based, shortest path routing algorithms.  <br> - </details>

<details> <summary> <a href="link"> GENERAL PRINCIPLES OF LEARNING-BASED MULTI-AGENT SYSTEMS </a>by David H. Wolpert, Kevin R. Wheeler, Kagan Turner. International Conference on Autonomous Agents, 1999. <a href="link">  </a> </summary> We consider the problem of how to design large decentralized multi-agent systems (MAS’s) in an automated fashion, with little or no hand-tuning. Our approach has each agent run a reinforcement learning algorithm. This converts the problem into one of how to automatically set/update the reward functions for each of the agents so that the global goal is achieved. In particular we don not want the agents to “work at cross-purposes” as far as the global goal is concerned. We use the term artificial COllective INtelligence (COIN) to refer to systems that embody solutions to this problem. In this paper we present a summary of a mathematical framework for COINS. We then investigate the real-world applicability of the core concepts of that framework via two computer experiments: we show that our COINS perform near optimally in a difficult variant of Arthur’s bar problem [l] (and in psrtitular avoid the tragedy of the commons for that problem), and we also illustrate optimal performance for our COINS in the leader-follower problem.  <br> - </details>

<details> <summary> <a href="http://faculty.washington.edu/paymana/swarm/bonabeau03-etcon.pdf"> Swarm Intelligence </a>by Eric Bonabeau. Emergent technology conference, 2003. <a href="link">  </a> </summary> Ants, Bees or Termites - all social insects - show impressive collective problem-solving capabilities. Properties associated with their group behaviour like self-organisation, robustness and flexibility are seen as characteristics that artificial systems for optimisation, control or task execution should exhibit. In the last decade, diverse efforts have been made to take social insects as an example and develop algorithms inspired by their strictly self-organised behaviour. These approaches can be subsumed under the concept of "Swarm Intelligence". <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-030-77584-1.pdf"> A Primer on Multiple Intelligences </a>by Matthew N. O. Sadiku, Sarhan M. Musa. Springer, 2021. <a href="link">  </a> </summary> The concept of intelligence is interesting and is discussed every day. It has been central to the feld of psychology and remains hotly debated. It was frst introduced by Francis Galton in 1885. Intelligence or cognitive development is a biopsychological potential to process information that can be activated to solve problems. It is the capacity of the individual to act purposefully, to think rationally, and to deal effectively with his environment. The characteristic of intelligence is usually attributed to humans. Traditionally, intelligence is often regarded as a person’s intellectual capacity; something one is born with and that cannot be changed. It is the ability to learn, to proft from experience, the ability to adapt to new situations, and the ability to solve problems. It was generally believed that intelligence was a single entity that was inherited. It is fxed and can be measured. Historically, intelligence has been measured using the IQ test which is a general measure of one’s cognitive function. Other views of intelligence have emerged in recent years. Today, an increasing number of researchers, believe that there exists a multiple of intelligences, quite independent of each other. People have a unique blend of intelligences. The concept of multiple intelligences represents an effort to reframe the traditional conception of intelligence. Professor Howard Gardner at Harvard’s Graduate School of Education argued that there are better or alternative ways to measure intelligence than standard IQ tests. He frst proposed nine intelligences and insisted that all people are born with one or more intelligences. He believed human intelligence was multidimensional. This spectrum of intelligence indicates that people can be smart in a number of different ways. It implies that we are all intelligent in different ways, but there is always a primary, or more dominant, intelligence in each of us. Multiple intelligences theory states that everyone has all intelligences at varying degrees of profciency, while most will experience more dominant intelligences that impact the way they learn and interact with others. Each type of intelligence presents a distinct component of our total competence. There is no clear consensus about how many intelligences there are. In this book, we consider 19 multiple intelligences. The intelligences are possessed by everyone and vary in degree of development within each individual. <br> - </details>

<br/>

### Convergent Learning

<details> <summary> <a href="http://www.dklevine.com/archive/refs4415.pdf"> Learning Mixed Equilibria </a> by Drew Fundenberg, David M. Kreps. Journal of Games and Economic Behvaior, 1993.  </summary> We study learning processes for finite strategic-form games, in which players use the history of past play to forecast play in the current period. In a generalization of fictitious play, we assume only that players asymptotically choose best responses to the historical frequencies of opponents′ past play. This implies that if the stage-game strategies converge, the limit is a Nash equilibrium. In the basic model, plays seems unlikely to converge to a mixed-strategy equilibrium, but such convergence is natural when the stage game is perturbed in the manner of Harsanyi′s purification theorem.  <br> - </details>

<details> <summary> <a href="http://www.eecs.harvard.edu/cs286r/courses/spring06/papers/kalailehrer93.pdf"> Rational learning leads to nash equilibrium </a>by Ehud Kalai, Ehud Lehrer. Econometrica, 1993. <a href="link">  </a> </summary> Each of n players, in an infinitely repeated game, starts with subjective beliefs about his opponents' strategies. If the individual beliefs are compatible with the true strategies chosen, then Bayesian updating will lead in the long run to accurate prediction of the future play of the game. It follows that individual players, who know their own payoff matrices and choose strategies to maximize their expected utility, must eventually play according to a Nash equilibrium of the repeated game. An immediate corollary is that, when playing a Harsanyi-Nash equilibrium of a repeated game of incomplete information about opponents' payoff matrices, players will eventually play a Nash equilibrium of the real game, as if they had complete information <br> - </details>

<details> <summary> <a href="https://cs.brown.edu/~mlittman/papers/ml96-generalized.pdf"> A generalized reinforcement-learning model: Convergence and applications </a>by Michael L. Littman, C. Szepesvari. ICML, 1996. <a href="link">  </a> </summary> Reinforcement learning is the process by which an autonomous agent uses its experience interacting with an environment to improve its behavior. The Markov decision process (MDP) model is a popular way of formalizing the reinforcement learning problem but it is by no means the only way. In this paper we show how many of the important theoretical results concerning reinforcement learning in MDPs extend to a generalized MDP model that includes MDPs two-player games and MDPs under a worst-case optimality criterion as special cases. The basis of this extension is a stochastic approximation theorem that reduces asynchronous convergence to synchronous con <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf"> The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems </a> by Caroline Claus, Craig Boutilier. AAAI, 1998.  </summary> Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.  <br> - </details>

<details> <summary> <a href="https://econweb.ucsd.edu/~jandreon/Econ264/papers/Erev%20Roth%20AER%201998.pdf"> Predicting how people play games: reinforcement leaning in experimental games with unique, mixed strategy equilibria </a> by Ido Erev, Alvin E. Roth. The American Economic Review, 1998. </summary> We examine learning in all experiments we could locate involving 100 periods or more of games with a unique equilibrium in mixed strategies, and in a new experiment. We study both the ex post ("best fit") descriptive power of learning models, and their ex ante predictive power, by simulating each experiment using parameters estimated from the other experiments. Even a one-parameter reinforcement learning model robustly outperforms the equilibrium predictions. Predictive power is improved by adding "forgetting" and "experimentation," or by allowing greater rationality as in probabilistic fictitious play. Implications for developing a low-rationality, cognitive game theory are discussed. <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~mmv/papers/01ijcai-mike.pdf"> Rational and Convergent Learning in Stochastic Games </a>by Michael Bowling, Manuela Veloso. 2001. </summary> This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we briefly overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm, WoLF policy hillclimbing, that is based on a simple principle: “learn quickly while losing, slowly while winning.” The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.  <br> - </details>

<details> <summary> <a href="https://dspace.mit.edu/bitstream/handle/1721.1/3688/CS023.pdf?sequence=2&isAllowed=y"> Playing is believing: the role of beliefs in multi-agent learning </a> by Yu-Han Chang, Leslie Pack Kaelbling. NeurIPS, 2001. </summary> We propose a new classification for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classification, we review the optimality of existing algorithms and discuss some insights that can be gained. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the long-run against fair opponents. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/3-540-44795-4_33.pdf"> Social agents playing a periodical policy </a>by Ann Now´e, Johan Parent, and Katja Verbeeck. ECML, 2001. <a href="link">  </a> </summary> Coordination is an important issue in multiagent systems. Within the stochastic game framework this problem translates to policy learning in a joint action space. This technique however suffers some important drawbacks like the assumption of the existence of a unique Nash equilibrium and synchronicity, the need for central control, the cost of communication, etc. Moreover in general sum games it is not always clear which policies should be learned. Playing pure Nash equilibria is often unfair to at least one of the players, while playing a mixed strategy doesn’t give any guarantee for coordination and usually results in a sub-optimal payoff for all agents. In this work we show the usefulness of periodical policies, which arise as a side effect of the fairness conditions used by the agents. We are interested in games which assume competition between the players, but where the overall performance can only be as good as the performance of the poorest player. Players are social distributed reinforcement learners, who have to learn to equalize their payoff. Our approach is illustrated on synchronous one-step games as well as on asynchronous job scheduling games. <br> - </details>

<details> <summary> <a href="https://papers.nips.cc/paper/2002/file/0d73a25092e5c1c9769a9f3255caa65a-Paper.pdf"> Efficient learning equilibrium </a>by Ronen I. Brafman, Moshe Tennenholtz. NeurIPS, 2002. <a href="link">  </a> </summary> We introduce efficient learning equilibrium (ELE), a normative approach to learning in noncooperative settings. In ELE, the learning algorithms themselves are required to be in equilibrium. In addition, the learning algorithms must arrive at a desired value after polynomial time, and a deviation from the prescribed ELE becomes irrational after polynomial time. We prove the existence of an ELE (where the desired value is the expected payoff in a Nash equilibrium) and of a Pareto-ELE (where the objective is the maximization of social surplus) in repeated games with perfect monitoring. We also show that an ELE does not always exist in the imperfect monitoring case. Finally, we discuss the extension of these results to general-sum stochastic games <br> - </details>

<br/>

### Coordination

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1994/AAAI94-065.pdf"> Learning to coordinate without sharing information </a>by Sandip Sen, Mahendra Sekaran, John Hale. National Conference on Artificial Intelligence, 1994. <a href="link">  </a> </summary> Researchers in the field of Distributed Artificial Intelligence (DAI) have been developing efficient mechanisms to coordinate the activities of multiple autonomous agents. The need for coordination arises because agents have to share resources and expertise required to achieve their goals. Previous work in the area includes using sophisticated information exchange protocols, investigating heuristics for negotiation, and developing formal models of possibilities of conflict and cooperation among agent interests. In order to handle the changing requirements of continuous and dynamic environments, we propose learning as a means to provide additional possibilities for effective coordination. We use reinforcement learning techniques on a block pushing problem to show that agents can learn complimentary policies to follow a desired path without any knowledge about each other. We theoretically analyze and experimentally verify the effects of learning rate on system convergence, and demonstrate benefits of using learned coordination knowledge on similar problems. Reinforcement learning based coordination can be achieved in both cooperative and non-cooperative domains, and in domains with noisy communication channels and other stochastic characteristics that present a formidable challenge to using other coordination schemes. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/publication/226319923_A_Distributed_Approach_for_Coordination_of_Traffic_Signal_Agents"> A game-theoretic approach to coordination of traffic signal agents </a>by Bazzan A. L. C. PhD thesis, 1997. <a href="link">  </a> </summary> Innovative control strategies are needed to cope with the increasing urban traffic chaos. In most cases, the currently used strategies are based on a central traffic-responsive control system which can be demanding to implement and maintain. Therefore, a functional and spatial decentralization is desired. For this purpose, distributed artificial intelligence and multi-agent systems have come out with a series of techniques which allow coordination and cooperation. However, in many cases these are reached by means of communication and centrally controlled coordination processes, giving little room for decentralized management. Consequently, there is a lack of decision-support tools at managerial level (traffic control centers) capable of dealing with decentralized policies of control and actually profiting from them. In the present work a coordination concept is used, which overcomes some disadvantages of the existing methods. This concept makes use of techniques of evolutionary game theory: intersections in an arterial are modeled as individually-motivated agents or players taking part in a dynamic process in which not only their own local goals but also a global one has to be taken into account. The role of the traffic manager is facilitated since s/he has to deal only with tactical ones, leaving the operational issues to the agents. Thus the system ultimately provides support for the traffic manager to decide on traffic control policies. Some application in traffic scenarios are discussed in order to evaluate the feasibility of transferring the responsibility of traffic signal coordination to agents. The results show different performances of the decentralized coordination process in different scenarios (e.g. the flow of vehicles is nearly equal in both opposing directions, one direction has a clearly higher flow, etc.). Therefore, the task of the manager is facilitate once s/he recognizes the scenario and acts accordingly. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2002/AAAI02-050.pdf"> Reinforcement Learning of Coordination in Cooperative Multiagent Systems </a>by Kapetanakis S. and Kudenko D. AAAI, 2002. <a href="link">  </a> </summary> We report on an investigation of reinforcement learning techniques for the learning of coordination in cooperative multiagent systems. Specifically, we focus on a novel action selection strategy for Q-learning (Watkins 1989). The new technique is applicable to scenarios where mutual observation of actions is not possible. To date, reinforcement learning approaches for such independent agents did not guarantee convergence to the optimal joint action in scenarios with high miscoordination costs. We improve on previous results (Claus & Boutilier 1998) by demonstrating empirically that our extension causes the agents to converge almost always to the optimal joint action even in these difficult cases. <br> - </details>

<details> <summary> <a href="https://link.springer.com/chapter/10.1007/3-540-36187-1_36"> Learning to Reach the Pareto Optimal Nash Equilibrium as a Team </a>by Katja Verbeeck, Ann Nowe, Tom Lenaerts, Johan Parent.  LNAI, 2002. <a href="link">  </a> </summary> Coordination is an important issue in multi-agent systems when agents want to maximize their revenue. Often coordination is achieved through communication, however communication has its price. We are interested in finding an approach where the communication between the agents is kept low, and a global optimal behavior can still be found. In this paper we report on an efficient approach that allows independent reinforcement learning agents to reach a Pareto optimal Nash equilibrium with limited communication. The communication happens at regular time steps and is basicallya signal for the agents to start an exploration phase. During each exploration phase, some agents exclude their current best action so as to give the team the opportunityto look for a possiblyb etter Nash equilibrium. This technique of reducing the action space byexclusions was onlyrecen tlyin troduced for finding periodical policies in games of conflicting interests. Here, we explore this technique in repeated common interest games with deterministic or stochastic outcomes. <br> - </details>

<details> <summary> <a href="https://core.ac.uk/download/pdf/54045748.pdf"> Coordination of independent learners in cooperative Markov games </a>by La¨etitia Matignon, Guillaume J. Laurent, Nadine Le Fort-Piat. Technical Report, 2009. <a href="link">  </a> </summary> In the framework of fully cooperative multi-agent systems, independent agents learning by reinforcement must overcome several difficulties as the coordination or the impact of exploration. The study of these issues allows first to synthesize the characteristics of existing reinforcement learning decentralized methods for independent learners in cooperative Markov games. Then, given the difficulties encountered by these approaches, we focus on two main skills: optimistic agents, which manage the coordination in deterministic environments, and the detection of the stochasticity of a game. Indeed, the key difficulty in stochastic environment is to distinguish between various causes of noise. The SOoN algorithm is so introduced, standing for “Swing between Optimistic or Neutral”, in which independent learners can adapt automatically to the environment stochasticity. Empirical results on various cooperative Markov games notably show that SOoN overcomes the main factors of non-coordination and is robust face to the exploration of other agents <br> - </details>

<br/>

### Cooperation

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839146&casa_token=Qb2aIlZRh9IAAAAA:1BrtA4ESZr_iJJJ4vhG6HZstOeQ-qfuqfs3iIGu2HutZb5bXpR9KyJTslQNF4YeYJ9l-Jw-mGac5BDk&tag=1"> Advantages of Cooperation Between Reinforcement Learning Agents in Difficult Stochastic Problems </a>by Hamid R. Berenji, David Vengerov. International Conference on Fuzzy Systems, 2000. <a href="link">  </a> </summary> This paper presents the first results in understanding the reasons for cooperative advantage between reinforcement learning agents. We consider a cooporation method which consists of using and updating a common policy. We tested this method on a complex fuzzy reinforcement learning problem and found that cooperation brings larger than expected benefits. More precisely, we found that K cooperative agents oach learning for N time steps outperform K independent agents each learning in a separate world for K*N time steps. In this paper, we explain the observed phenomenon and determine the necessary conditions for its presence in a wide class of reinforccment learning problems.  <br> - </details>

<br/>

### Decision theory

<details> <summary> <a href="https://www.ijcai.org/Proceedings/91-1/Papers/011.pdf"> A decision-theoretic approach to coordinating multiagent interactions </a>by Piotr J. Gmytrasiewicz, Edmund H. Durfee, David K. Weh. IJCAI, 1991. <a href="https://www.Summary.so/instadeep/Multiagent-Learning-Basics-Challenges-and-Prospect-21cb7b4294b84a4188cafd184a3deed8">  </a> </summary> We describe a decision-theoretic method that an autonomous agent can use to model multiagent situations and behave rationally based on its model. Our approach, which we call the Recursive Modeling Method, explicitly accounts for the recursive nature of multiagent reasoning. Our method lets an agent recursively model another agent's decisions based on probabilistic views of how that agent perceives the multiagent situation, which in turn are derived from hypothesizing how that other agent perceives the initial agent's possible decisions, and so on. Further, we show how the possibility of multiple interactions can affect the decisions of agents, allowing cooperative behavior to emerge as a rational choice of selfish agents that otherwise might behave uncooperatively <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/1301.3836.pdf"> The Complexity of Decentralized Control of Markov Decision Processes </a>by Daniel S. Bernstein, Shlomo Zilberstein, Neil Immerman. UAI, 2000. <a href="link">  </a> </summary> Planning for distributed agents with partial state information is considered from a decision theoretic perspective. We describe generalizations of both the MDP and POMDP models that allow for decentralized control. For even a small number of agents, the finite-horizon problems corresponding to both of our models are complete for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov processes. In contrast to the MDP and POMDP problems, the problems we consider provably do not admit polynomialtime algorithms and most likely require doubly exponential time to solve in the worst case. We have thus provided mathematical evidence corresponding to the intuition that decentralized planning problems cannot easily be reduced to centralized problems and solved exactly using established techniques.  <br> - </details>

<br/>

### Dispersion games

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2002/AAAI02-061.pdf"> Dispersion games: general definitions and some specific learning results </a>by T. Grenager, R. Powers, Y. Shoham. AAAI, 2002. <a href="link">  </a> </summary> Dispersion games are the generalization of the anticoordination game to arbitrary numbers of agents and actions. In these games agents prefer outcomes in which the agents are maximally dispersed over the set of possible actions. This class of games models a large number of natural problems, including load balancing in computer science, niche selection in economics, and division of roles within a team in robotics. Our work consists of two main contributions. First, we formally define and characterize some interesting classes of dispersion games. Second, we present several learning strategies that agents can use in these games, including traditional learning rules from game theory and artificial intelligence, as well as some special purpose strategies. We then evaluate analytically and empirically the performance of each of these strategies. <br> - </details>

<br/>

### Extensive form games

<details> <summary> <a href="https://www.tau.ac.il/~samet/papers/learning-to-play.pdf">  Learning to play games in extensive form by valuation </a>by Phillipe Jehiel, Dov Samet. NAJ Economics, 2001. <a href="link">  </a> </summary> Game theoretic models of learning which are based on the strategic form of the game cannot explain learning in games with large extensive form. We study learning in such games by using valuation of moves. A valuation for a player is a numeric assessment of her moves that purports to reflect their desirability. We consider a myopic player, who chooses moves with the highest valuation. Each time the game is played, the player revises her valuation by assigning the payoff obtained in the play to each of the moves she has made. We show for a repeated win–lose game that if the player has a winning strategy in the stage game, there is almost surely a time after which she always wins. When a player has more than two payoffs, a more elaborate learning procedure is required. We consider one that associates with each move the average payoff in the rounds in which this move was made. When all players adopt this learning procedure, with some perturbations, then, with probability 1 there is a time after which strategies that are close to subgame perfect equilibrium are played. A single player who adopts this procedure can guarantee only her individually rational payoff <br> - </details>

<br/>

### Evolutionary game theory

<details> <summary> <a href="http://etherplan.com/the-logic-of-animal-conflict.pdf"> The logic of animal conflict </a>by Maynard Smith, J., Price, G.R. Nature, 1973. <a href="link">  </a> </summary> Conflicts between animals of the same species usually are of “limited war” type, not causing serious injury. This is often explained as due to group or species selection for behaviour benefiting the species rather than individuals. Game theory and computer simulation analyses show, however, that a “limited war” strategy benefits individual animals as well as the species. <br> - </details>

<details> <summary> <a href="https://www.reed.edu/biology/courses/BIO342/2012_syllabus/2012_readings/smith_1976_games.pdf"> Evolution and the Theory of Games </a>by J. Maynard Smith. American Scientist, 1976. <a href="link">  </a> </summary> n situations characterized by conflict of interest, the best strategy to adopt depends on what others are doing. <br> - </details>

<details> <summary> <a href="http://math.uchicago.edu/~shmuel/Modeling/Axelrod%20and%20Hamilton.pdf"> The Evolution of Cooperation </a>by Robert Axelrod, William D. Hamilton. Science, 1981. <a href="link">  </a> </summary> Cooperation in organisms, whether bacteria or primates, has been a difficulty for evolutionary theory since Darwin. On the assumption that interactions between pairs of individuals occur on a probabilistic basis, a model is developed based on the concept of an evolutionarily stable strategy in the context of the Prisoner's Dilemma game. Deductions from the model, and the results of a computer tournament show how cooperation based on reciprocity can get started in an asocial world, can thrive while interacting with a wide range of other strategies, and can resist invasion once fully established. Potential applications include specific aspects of territoriality, mating, and disease. <br> - </details>

<details> <summary> <a href="https://www.econstor.eu/bitstream/10419/95028/1/wp347.pdf"> An Introduction to Evolutionary Game Theory </a>by Weibull, Jörgen W. IFN, 1992. <a href="link">  </a> </summary> Please note that these lecture notes are incomplete and may contain typos and errors. I hope to have a more full-fledged version in a few months, and appreciate in the meantime comments and suggestions.  <br> - </details>

<details> <summary> <a href="https://www.econstor.eu/bitstream/10419/94705/1/wp407.pdf"> Nash Equilibrium and Evolution by Imitation </a>by Bjornerstedt J., and Weibull, J. The Rational Foundations of Economic Behavior, 1995. <a href="link">  </a> </summary> Nash's "mass action" interpretation of his equilibrium concept does not presume that the players know the game or are capable of sophisticated calculations. Instead, players are repeatedly and randomly drawn from large populations to play the game, one population for each player position, and base their strategy choice on observed payoffs. The present paper examines in some detail such an interpretation in a dass of population dynamics based on adaptation by way of imitation of successful behaviors. Drawing from results in evolutionary game theory, implications of dynamic stability for aggregate Nash equilibrium play are discussed.  <br> - </details>

<details> <summary> <a href="https://www.econstor.eu/bitstream/10419/94851/1/wp487.pdf"> What have we learned from Evolutionary Game Theory so far? </a>by Weibull, Jörgen W. IFN, 1997. <a href="link">  </a> </summary> Evolutionary theorizing has a long tradition in economics. Only recently has this approach been brought into the framework of noncooperative game theory. Evolutionary game theory studies the robustness of strategic behavior with respect to evolutionary forces in the context of games played many times in large populations of boundedly rational agents. This new strand in economic theory has lead to new predictions and opened up doors to other social sciences. The discussion will be focused on the following questions: What distinguishes the evolutionary approach from the rationalistic? What are the most important ndings in evolutionary game theory so far? What are the next challenges for evolutionary game theory in economics? <br> - </details>

<details> <summary> <a href="https://core.ac.uk/download/pdf/6518937.pdf"> Learning through Reinforcement and Replicator Dynamics </a>by Borgers, T., Sarin, R. Journal of Economic Theory, 1997. <a href="link">  </a> </summary> This paper considers a version of R. R. Bush and F. Mosteller's stochastic learning theory in the context of games. We show that in a continuous time limit the learning model converges to the replicator dynamics of evolutionary game theory. Thus we provide a non-biological interpretation of evolutionary game theory. <br> - </details>

<details> <summary> <a href="http://www.fulviofrisone.com/attachments/article/412/Hofbauer%20Evolutionary%20Games%20and%20Population%20Dynamics.pdf"> Evolutionary Games and Population Dynamics </a>by Hofbauer, J., Sigmund, K. Cambridge University Press, 1998. <a href="link">  </a> </summary> Every form of behaviour is shaped by trial and error. Such stepwise adaptation can occur through individual learning or through natural selection, the basis of evolution. Since the work of Maynard Smith and others, it has been realised how game theory can model this process. Evolutionary game theory replaces the static solutions of classical game theory by a dynamical approach centred not on the concept of rational players but on the population dynamics of behavioural programmes. In this book the authors investigate the nonlinear dynamics of the self-regulation of social and economic behaviour, and of the closely related interactions between species in ecological communities. Replicator equations describe how successful strategies spread and thereby create new conditions which can alter the basis of their success, i.e. to enable us to understand the strategic and genetic foundations of the endless chronicle of invasions and extinctions which punctuate evolution. In short, evolutionary game theory describes when to escalate a conflict, how to elicit cooperation, why to expect a balance of the sexes, and how to understand natural selection in mathematical terms. <br> - </details>

<details> <summary> <a href="https://watermark.silverchair.com/282794.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsYwggLCBgkqhkiG9w0BBwagggKzMIICrwIBADCCAqgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMBRcVXHaCrhh9DPsJAgEQgIICecvIjI-AyHPqDEtoqxG45etk8Kr3qZnjlgYcF9HmV5Zgk3mbn6FJ-OzHZWLMlN-KdRwXGkIjDVCEOB8A-qFzdZz484BYJeQqM-d1usHshJkVd8nuz2mGVsBsS1PT9YPlw68yuUWF1UzLK41IyXZmZk4Pu58zMpQSken2DXuBZzC5R0btcbkHVhC_PuRR2Empi2m-xjW8dxWV5R3sbNiDnQQFLjm7BZZXTEE1qV1GvUCW0DdIDhcWJpa-LP6NZ1G0Evo4nlcVC4h1ZNzgizSe5oKTU3hJjeltMD35akait35q5EiPh6xMFqhykSJAAu-krMio05VNfHqAZ-n9vrW5g2W6z38eSBMCPmOx4VaLh9-vCyxHTIosjdbrsKISJ1F2t1AZitrUPniIOLr07aPZBVQHT1lxegrDKEQ-CzvB5Zg7cyVgyBRHzYKswHkCNij_3teAAB7Wi8DSmJWVKOovllkEuXayDtwEy8cktVZsgCGunE9Mz1wZKB0Pj6UGiccWtbGTS63nTNBgFh4ZntPAChV6x3olsXbzqcn0snIVvDtUzS_ziOyIGSD1WrHcZCQUFcWiZXPg059UQHKxunkIJCoZTqEhq5Aicg0MBOG7RPfBMIMv4GZtEPtNRuo3Ax1kFbdmlN2_FwnbSAtMYnh5an1Z3SjVeWkKCko2FBP6yigjm2vOIJBs0Q1s3sCNAFIoa7fSIH2jgmykzWsFQdRe-0ke7GHPs0UhzzDIYogT569IVE7FUeyY3lAHMFTcH3k8XlSCtnCu84J3nXoghqfHArjcDNZnTEg9mxu5W17_tnJzHhuU-viP7hzQu0gFaej7yaaCLRKkClTkuQ"> Evolution of biological information </a>by Thomas D. Schneider. Nucleic Acids Research, 2000. <a href="link">  </a> </summary> How do genetic systems gain information by evolutionary processes? Answering this question precisely requires a robust, quantitative measure of information. Fortunately, 50 years ago Claude Shannon defined information as a decrease in the uncertainty of a receiver. For molecular systems, uncertainty is closely related to entropy and hence has clear connections to the Second Law of Thermodynamics. These aspects of information theory have allowed the development of a straightforward and practical method of measuring information in genetic control systems. Here this method is used to observe information gain in the binding sites for an artificial ‘protein’ in a computer simulation of evolution. The simulation begins with zero information and, as in naturally occurring genetic systems, the information measured in the fully evolved binding sites is close to that needed to locate the sites in the genome. The transition is rapid, demonstrating that information gain can occur by punctuated equilibrium. <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/30801544/Tuybnaic02-with-cover-page-v2.pdf?Expires=1668008453&Signature=D2CzKtWwSHhSIlxHOl7NcfNf3Be7IomezobJzZPLDBT2~3nA28zKf7xPQkQq13XFBfGgEIhx3IvzL9OHu4abVVSf9TxdFZwCaNg7JODf81a8~bBg2y9CITtTYBtmpw8gxQw9mXc4dpHBEc9dKwjLi18zC47x2e9gr4ZX3uYeRu6JflBxR6FmqwvlNzR4VxPvTv0DwgKdnALkVedwDLaGUlE7iQEd5VQgNhy8ZF-76bZ8qhGWv4FNdrFY5bjVAbJ2nz4vYcM2AAc6qNE~if9VjBARd1hkg0-3U7WLUDk2UnRzBz2rn9Z7ra75pN2MQB0VtpXAQHuh8gG5Od~MBOIfuA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> Towards a relation between learning agents and evolutionary dynamics </a>by Karl Tuyls, Tom Lenaerts, Katja Verbeeck, Sam Maes. BNAIC, 2002. <a href="link">  </a> </summary> Modeling learning agents in the context of Multi-agent Systems requires insight in the type and form of interactions with the environment and other agents in the system. Usually, these agents are modeled similar to the different players in a standard game theoretical model. In this paper we examine whether evolutionary game theory, and more specifically the replicator dynamics, is an adequate theoretical model for the study of the dynamics of reinforcement learning agents in a multi-agent system. As a first step in this direction we extend the results of [1, 9] to a more general reinforcement learning framework, i.e. Learning Automata. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Karl-Tuyls/publication/221471451_On_a_Dynamical_Analysis_of_Reinforcement_Learning_in_Games_Emergence_of_Occam%27s_Razor/links/0c9605203e5ba30157000000/On-a-Dynamical-Analysis-of-Reinforcement-Learning-in-Games-Emergence-of-Occams-Razor.pdf"> On a Dynamical Analysis of Reinforcement Learning in Games: Emergence of Occam’s Razor </a>by Karl Tuyls, Katja Verbeeck, Sam Maes. Lecture Notes in Computer Science, 2003. <a href="link">  </a> </summary> Modeling learning agents in the context of Multi-agent Systems requires an adequate understanding of their dynamic behaviour. Usually, these agents are modeled similar to the different players in a standard game theoretical model. Unfortunately traditional Game Theory is static and limited in its usefelness. Evolutionary Game Theory improves on this by providing a dynamics which describes how strategies evolve over time. In this paper, we discuss three learning models whose dynamics are related to the Replicator Dynamics(RD). We show how a classical Reinforcement Learning(RL) technique, i.e. Qlearning relates to the RD. This allows to better understand the learning process and it allows to determine how complex a RL model should be. More precisely, Occam’s Razor applies in the framework of games, i.e. the simplest model (Cross) suffices for learning equilibria. An experimental verification in all three models is presented. <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/30888627/p693-with-cover-page-v2.pdf?Expires=1668008714&Signature=c6qzlgxHi0g~03AfAJcY0D1i7Pi0TbjmgXXrMbaE-mYV52qJJLFcEMYIjkqj59wHIJyC69sEI6dMenm8neQP9IORV-tpCNigyRWlRS5b8WL4gIFLqBodbIlr10KlLaY6~zNRHY-shOzVixDraeDdIT3qCqh-Kjb~S3uSnoIRRgKSV8p5XzeW1SAnJEcXRnM1ZZY6VTWiVejZoH02f-g9Tx1LiDmvp8XI2FJK0FuMrR-iFpcFcafc44q8bSu9HxhPBW3OPll4~vT4H3U~dMyv2h-lo-vp6gmuoQLPpym~Gc1lqBQsQb8ngo0ZDEdESlUz-ayTFD8CS6cgWm17HHO~IQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> A selection-mutation model for Q-learning in multi-agent systems </a>by Karl Tuyls, Katja Verbeeck, Tom Lenaerts. AAMAS, 2003. <a href="link">  </a> </summary> Although well understood in the single-agent framework, the use of traditional reinforcement learning (RL) algorithms in multi-agent systems (MAS) is not always justified. The feedback an agent experiences in a MAS, is usually influenced by the other agents present in the system. Multi agent environments are therefore non-stationary and convergence and optimality guarantees of RL algorithms are lost. To better understand the dynamics of traditional RL algorithms we analyze the learning process in terms of evolutionary dynamics. More specifically we show how the Replicator Dynamics (RD) can be used as a model for Q-learning in games. The dynamical equations of Q-learning are derived and illustrated by some well chosen experiments. Both reveal an interesting connection between the exploitationexploration scheme from RL and the selection-mutation mechanisms from evolutionary game theory. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-540-39857-8_38.pdf"> Extended Replicator Dynamics as a Key to Reinforcement Learning in Multi-agent Systems </a>by Karl Tuyls, Dries Heytens, Ann Nowe, Bernard Manderick.  ECML, 2003. <a href="link">  </a> </summary> Modeling learning agents in the context of Multi-agent Systems requires an adequate understanding of their dynamic behaviour. Evolutionary Game Theory provides a dynamics which describes how strategies evolve over time. B¨orgers et al. [1] and Tuyls et al. [11] have shown how classical Reinforcement Learning (RL) techniques such as Cross-learning and Q-learning relate to the Replicator Dynamics (RD). This provides a better understanding of the learning process. In this paper, we introduce an extension of the Replicator Dynamics from Evolutionary Game Theory. Based on this new dynamics, a Reinforcement Learning algorithm is developed that attains a stable Nash equilibrium for all types of games. Such an algorithm is lacking for the moment. This kind of dynamics opens an interesting perspective for introducing new Reinforcement Learning algorithms in multi-state games and MultiAgent Systems. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-540-30115-8_18.pdf"> Analyzing Multi-agent Reinforcement Learning Using Evolutionary Dynamics </a>by ’t Hoen, P.J., Tuyls, K. ECML, 2004. <a href="link">  </a> </summary> In this paper, we show how the dynamics of Q-learning can be visualized and analyzed from a perspective of Evolutionary Dynamics (ED). More specifically, we show how ED can be used as a model for Qlearning in stochastic games. Analysis of the evolutionary stable strategies and attractors of the derived ED from the Reinforcement Learning (RL) application then predict the desired parameters for RL in MultiAgent Systems (MASs) to achieve Nash equilibriums with high utility. Secondly, we show how the derived fine tuning of parameter settings from the ED can support application of the COllective INtelligence (COIN) framework. COIN is a proved engineering approach for learning of cooperative tasks in MASs. We show that the derived link between ED and RL predicts performance of the COIN framework and visualizes the incentives provided in COIN toward cooperative behavior. <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1555000&casa_token=CmWmYz9F4AUAAAAA:7bw2f3NZZdUZlt1kFoEIWEVVD__xdLeBOvnp6Ebemfmkgj4wbhX5ntu2JREjL0cMdb0EmQ7gYpdW9qI&tag=1"> On Social Learning and Robust Evolutionary Algorithm Design in Economic Games </a>by Floortje Alkemade, Han La Poutre, Hans Amman. Congress on Evolutionary Computation, 2005. <a href="link">  </a> </summary> Agent-based computational economics (ACE) combines elements from economics and computer science. In this paper, we focus on the relation between the evolutionary technique that is used and the economic problem that is modeled. In the field of ACE, economic simulations often derive parameter settings for the genetic algorithm directly from the values of the economic model parameters. In this paper we compare two important approaches that are dominating in ACE and show that the above practice may hinder the performance of the GA and thereby hinder agent learning. More specifically, we show that economic model parameters and evolutionary algorithm parameters should be treated separately by comparing the two widely used approaches to social learning with respect to their convergence properties and robustness. This leads to new considerations for the methodological aspects of evolutionary algorithm design within the field of ACE. We also present improved social (ACE) simulation results for the Cournot oligopoly game, yielding (higher profit) Cournot-Nash equilibria instead of the competitive equilibria. <br> - </details>

<br/>

### Foundational Game Theory

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/neumann44a.pdf"> Theory of Games and Economic Behaviour </a>by von Neumann, J., Morgenstern, O. Princeton University Press, 1944. <a href="link">  </a> </summary> This is the classic work upon which modern-day game theory is based. What began more than sixty years ago as a modest proposal that a mathematician and an economist write a short paper together blossomed, in 1944, when Princeton University Press published Theory of Games and Economic Behavior. In it, John von Neumann and Oskar Morgenstern conceived a groundbreaking mathematical theory of economic and social organization, based on a theory of games of strategy. Not only would this revolutionize economics, but the entirely new field of scientific inquiry it yielded--game theory--has since been widely used to analyze a host of real-world phenomena from arms races to optimal policy choices of presidential candidates, from vaccination policy to major league baseball salary negotiations. And it is today established throughout both the social sciences and a wide range of other sciences. <br> - </details>

<details> <summary> <a href="https://arielrubinstein.tau.ac.il/books/GT.pdf"> A Course in Game Theory </a>by Martin J. Osborne and Ariel Rubinstein.  MIT Press, 1994. <a href="link">  </a> </summary> A Course in Game Theory presents the main ideas of game theory at a level suitable for graduate students and advanced undergraduates, emphasizing the theory's foundations and interpretations of its basic concepts. The authors provide precise definitions and full proofs of results, sacrificing generalities and limiting the scope of the material in order to do so. The text is organized in four parts: strategic games, extensive games with perfect information, extensive games with imperfect information, and coalitional games. It includes over 100 exercises. <br> - </details>

<br/>

### General-sum games

<details> <summary> <a href="https://webdocs.cs.ualberta.ca/~bowling/papers/00icml.pdf"> Convergence Problems of General-Sum Multiagent Reinforcement Learning </a>by Michael Bowling. ICML, 2000. </summary> Stochastic games are a generalization of MDPs to multiple agents, and can be used as a framework for investigating multiagent learning. Hu and Wellman (1998) recently proposed a multiagent Q-learning method for general-sum stochastic games. In addition to describing the algorithm, they provide a proof that the method will converge to a Nash equilibrium for the game under specified conditions. The convergence depends on a lemma stating that the iteration used by this method is a contraction mapping. Unfortunately the proof is incomplete. In this paper we present a counterexample and flaw to the lemma’s proof. We also introduce strengthened assumptions under which the lemma holds, and examine how this affects the classes of games to which the theoretical result can be applied  <br> - </details>

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/littman01a.pdf"> Friend-or-foe Q-learning in general-sum games </a>by Michael L. Littman. ICML, 2001. <a href="link">  </a> </summary> This paper describes an approach to reinforcement learning in multiagent general-sum games in which a learner is told to treat each other agent as either a friend" or foe". This Q-learning-style algorithm provides strong convergence guarantees compared to an existing Nash-equilibrium-based learning rule. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-02/SS02-02-012.pdf"> Correlated-Q Learning </a>by Amy Greenwald, Keith Hall, Roberto Serrano. NeurIPS workshop on multi-agent learning, 2002. <a href="link">  </a> </summary> Bowling named two desiderata for multiagent learning algorithms: rationality and convergence. This paper introduces correlated-Q learning, a natural generalization of Nash-Q and FF-Q that satisfies these criteria. Nash-Q satisfies rationality, but in general it does not converge. FF-Q satisfies convergence, but in general it is not rational. Correlated-Q satisfies rationality by construction. This papers demonstrates the empirical convergence of correlated-Q on a standard testbed of general-sum Markov games. <br> - </details>

<details> <summary> <a href="https://www.jmlr.org/papers/volume4/hu03a/hu03a.pdf"> Nash Q-Learning for General-Sum Stochastic Games </a>by Junling Hu, Michael Wellman. JMLR, 2003. <a href="link">  </a> </summary> We extend Q-learning to a noncooperative multiagent context, using the framework of general-sum stochastic games. A learning agent maintains Q-functions over joint actions, and performs updates based on assuming Nash equilibrium behavior over the current Q-values. This learning protocol provably converges given certain restrictions on the stage games (defined by Q-values) that arise during learning. Experiments with a pair of two-player grid games suggest that such restrictions on the game structure are not necessarily required. Stage games encountered during learning in both grid environments violate the conditions. However, learning consistently converges in the first grid game, which has a unique equilibrium Q-function, but sometimes fails to converge in the second, which has three different equilibrium Q-functions. In a comparison of offline learning performance in both games, we find agents are more likely to reach a joint optimal path with Nash Q-learning than with a single-agent Q-learning method. When at least one agent adopts Nash Q-learning, the performance of both agents is better than using single-agent Q-learning. We have also implemented an online version of Nash Q-learning that balances exploration with exploitation, yielding improved performance. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Sandip-Sen-2/publication/2831062_Towards_a_Pareto-optimal_solution_in_general-sum_games/links/54dbfb250cf28d3de65e2dc8/Towards-a-Pareto-optimal-solution-in-general-sum-games.pdf"> Towards a Pareto Optimal Solution in general-sum games </a>by Sandip Sen, Stephane Airiau, Rajatish Mukherjee. AAMAS, 2003. <a href="link">  </a> </summary> Multiagent learning literature has investigated iterated two-player games to develop mechanisms that allow agents to learn to converge on Nash Equilibrium strategy profiles. Such equilibrium configuration implies that there is no motivation for one player to change its strategy if the other does not. Often, in general sum games, a higher payoff can be obtained by both players if one chooses not to respond optimally to the other player. By developing mutual trust, agents can avoid iterated best responses that will lead to a lesser payoff Nash Equilibrium. In this paper we work with agents who select actions based on expected utility calculations that incorporates the observed frequencies of the actions of the opponent(s). We augment this stochastically-greedy agents with an interesting action revelation strategy that involves strategic revealing of one's action to avoid worst-case, pessimistic moves. We argue that in certain situations, such apparently risky revealing can indeed produce better payoff than a non-revealing approach. In particular, it is possible to obtain Pareto-optimal solutions that dominate Nash Equilibrium. We present results over a large number of randomly generated payoff matrices of varying sizes and compare the payoffs of strategically revealing learners to payoffs at Nash equilibrium. <br> - </details>

<br/>

### Hierachical learning

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/a:1022140919877.pdf"> Recent Advances in Hierarchical Reinforcement Learning </a>by Andrew G. Barto, Sridhar Mahadevan. Discrete Event Dynamic Systems, 2003. <a href="link">  </a> </summary> Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting. <br> - </details>

<br/>

### Incomplete information games

<details> <summary> <a href="http://www.ma.huji.ac.il/~zamir/papers/22_IJGT85.pdf"> Formulation of bayesian analysis for games with incomplete information </a>by J-F. Mertens, S. Zamir. International Journal of Game Theory, 1985. <a href="link">  </a> </summary> A formal model is given of Harsanyi's infinite hierarchies of beliefs. It is shown that the model closes with some Bayesian game with incomplete information, and that any such game can be approximated by one with a finite number of states of world. <br> - </details>

<br/>

### No-regret learning

<details> <summary> <a href="http://static.cs.brown.edu/people/amy/papers/icml.pdf"> On no-regret learning, fictitious play, and nash equilibrium </a>by Jafari, C., Greenwald, A., Gondek, D., Ercal, G. ICML, 2001. <a href="link">  </a> </summary> This paper addresses the question what is the outcome of multi-agent learning via no-regret algorithms in repeated games? Specifically, can the outcome of no-regret learning be characterized by traditional game-theoretic solution concepts, such as Nash equilibrium? The conclusion of this study is that no-regret learning is reminiscent of fictitious play: play converges to Nash equilibrium in dominancesolvable, constant-sum, and generalsum 2  2 games, but cycles exponentially in the Shapley game. Notably, however, the information required of fictitious play far exceeds that of noregret learning. <br> - </details>

<br/>

### Opponent modelling

<details> <summary> <a href="http://strategicreasoning.org/wp-content/uploads/2010/03/csr01.pdf"> Learning about other agents in a dynamic multiagent system </a>by Junling Hu, Michael Wellman. Journal of Cognitive Systems Research, 2001. <a href="link">  </a> </summary> We analyze the problem of learning about other agents in a class of dynamic multiagent systems, where performance of the primary agent depends on behavior of the others. We consider an online version of the problem, where agents must learn models of the others in the course of continual interactions. Various levels of recursive models are implemented in a simulated double auction market. Our experiments show learning agents on average outperform non-learning agents who do not use information about others. Among learning agents, those with minimum recursion assumption generally perform better than the agents with more complicated, though often wrong assumptions. <br> - </details>

<br/>

### Repeated games

<details> <summary> <a href="http://www-stat.wharton.upenn.edu/~steele/Resources/Projects/SequenceProject/Hannan.pdf"> Approximation to bayes risk in repeated plays </a>by James Hannan. Contributions to the Theory of Games, 1959. <a href="link">  </a> </summary> This paper is concerned with the development of a dynamic theoryof decision under uncertainty. The results obtained are directly applicableto the development of a dynamic theory of games in which at least one play­er is, at each stage, fully informed on the joint empirical distribution ofthe past choices of strategies of the rest. Since the decision problem canbe Imbedded in a sufficiently unspecified game theoretic model, the paperis written in the language and notation of the general two person game, in which, however, player  I’s motivation is completely unspecified. <br> - </details>

<details> <summary> <a href="https://scholars.huji.ac.il/sites/default/files/abrahamn/files/bounded.pdf"> Bounded complexity justifies cooperation in finitely repeated prisoner’s dilemma </a>by Abraham Neyman. Economic Letters, 1985. <a href="link">  </a> </summary> Cooperation in the finitely repeated prisoner's dilemma is justified, without departure from strict utility maximization or complete information, but under the assumption that there are bounds (possibly very large) to the complexity of the strategies that the players may use. <br> - </details>

<details> <summary> <a href="http://www.econ.ucla.edu/workingpapers/wp735.pdf"> Noncomputable strategies and discounted repeated games </a>by John H. Nachbar, William R. Zame. Economic Theory, 1996. <a href="link">  </a> </summary> A number of authors have used formal models of computation to capture the idea of “bounded rationality” in repeated games. Most of this literature has used computability by a finite automaton as the standard. A conceptual difficulty with this standard is that the decision problem is not “closed.” That is, for every strategy implementable by an automaton, there is some best response implementable by an automaton, but there may not exist any algorithm forfinding such a best response that can be implemented by an automaton. However, such algorithms can always be implemented by a Turing machine, the most powerful formal model of computation. In this paper, we investigate whether the decision problem can be closed by adopting Turing machines as the standard of computability. The answer we offer is negative. Indeed, for a large class of discounted repeated games (including the repeated Prisoner's Dilemma) there exist strategies implementable by a Turing machine for whichno best response is implementable by a Turing machine. <br> - </details>

<details> <summary> <a href="https://www.cs.utexas.edu/users/pstone/Papers/bib2html-links/threats-ATAL2001.pdf"> Implicit negotiation in repeated games </a>by Peter Stone and Michael L. Littman. ATAL, 2001. <a href="link">  </a> </summary> In business-related interactions such as the on-going high-stakes FCC spectrum auctions, explicit communication among participants is regarded as collusion, and is therefore illegal. In this paper, we consider the possibility of autonomous agents engaging in implicit negotiation via their tacit interactions. In repeated general-sum games, our testbed for studying this type of interaction, an agent using a ``best response'' strategy maximizes its own payoff assuming its behavior has no effect on its opponent. This notion of best response requires some degree of learning to determine the fixed opponent behavior. Against an unchanging opponent, the best-response agent performs optimally, and can be thought of as a ``follower, '' since it adapts to its opponent. However, pairing two best-response agents in a repeated game can result in suboptimal behavior. We demonstrate this suboptimality in several different games using variants of Q-learning as an example of a best-response strategy. We then examine two ``leader'' strategies that induce better performance from opponent followers via stubbornness and threats. These tactics are forms of implicit negotiation in that they aim to achieve a mutually beneficial outcome without using explicit communication outside of the game. <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~mmv/papers/01ijcai-mike.pdf"> Sophisticated EWA Learning and Strategic Teaching in Repeated Games </a>by Colin F. Camerer, Teck-Hua Ho, Juin-Kuan Chong. Journal of Economic Theory, 2002. <a href="https://www.Summary.so/instadeep/Multiagent-Learning-Basics-Challenges-and-Prospect-21cb7b4294b84a4188cafd184a3deed8">   </a> </summary> Most learning models assume players are adaptive (i.e., they respond only to their own previous experience and ignore others' payo® information) and behavior is not sensitive to the way in which players are matched. Empirical evidence suggests otherwise. In this paper, we extend our adaptive experienceweighted attraction (EWA) learning model to capture sophisticated learning and strategic teaching in repeated games. The generalized model assumes there is a mixture of adaptive learners and sophisticated players. An adaptive learner adjusts his behavior the EWA way. A sophisticated player rationally best-responds to her forecasts of all other behaviors. A sophisticated player can be either myopic or farsighted. A farsighted player develops multiple-period rather than single-period forecasts of others' behaviors and chooses to `teach' the other players by choosing a strategy scenario that gives her the highest discounted net present value. We estimate the model using data from p-beauty contests and repeated trust games with incomplete information. The generalized model is better than the adaptive EWA model in describing and predicting behavior. Including teaching also allows an empirical learning-based approach to reputation formation which predicts better than a quantal-response extension of the standard typebased approach. <br> - </details>

<details> <summary> <a href="https://digitalcommons.montclair.edu/cgi/viewcontent.cgi?article=1586&context=compusci-facpubs"> The Role of Reactivity in Multiagent Learning </a>by Bikramjit Banerjee, Jing Peng. AAMAS, 2004. <a href="link">  </a> </summary> In this paper we take a closer look at a recently proposed classification scheme for multiagent learning algorithms. Based on this scheme an exploitation mechanism (we call it the Exploiter) was developed that could beat various Policy Hill Climbers (PHC) and other fair opponents in some repeated matrix games. We show on the contrary that some fair opponents may actually beat the Exploiter in repeated games. This clearly indicates a deficiency in the original classification scheme which we address. Specifically, we introduce a new measure called Reactivity that measures how fast a learner can adapt to an unexpected hypothetical change in the opponent’s policy. We show that in some games, this new measure can approximately predict the performance of a player, and based on this measure we explain the behaviors of various algorithms in the Matching Pennies game, which was inexplicable by the original scheme. Finally we show that under certain restrictions, a player that consciously tries to avoid exploitation may be unable to do so. <br> - </details>

<br/>

### Robotic teams

<details> <summary> <a href="https://www.aaai.org/Papers/Workshops/1997/WS-97-03/WS97-03-002.pdf"> Learning Roles: Behavioral Diversity in Robot Teams </a>by Tucker Balch. AAAI, 1997. <a href="">  </a> </summary> This paper describes research investigating behavioral specialization in learning robot teams. Each agent is provided a common set of skills (motor schema-based behavioral assemblages) from which it builds a taskachieving strategy using reinforcement learning. The agents learn individually to activate particular behavioral assemblages given their current situation and a reward signal. The experiments, conducted in robot soccer simulations, evaluate the agents in terms of performance, policy convergence, and behavioral diversity. The results show that in many cases, robots will autorustically diversify by choosing heterogeneous behaviors. The degree of diversification and the performance of the team depend on the reward structure. When the entire team is jointly rewarded or penalized (global reinforcement), teams tend towards heterogeneous behavior. When agents are provided feedback individually (local reinforcement), they converge to identical policies. <br> - </details>

<details> <summary> <a href="https://smartech.gatech.edu/bitstream/handle/1853/21573/alaa99.pdf"> Reward and Diversity in Multirobot Foraging </a>by Tucker Balch. IJCAI, 1999. <a href="link">  </a> </summary> This research seeks to quantify the impact of the choice of reward function on behavioral diversity in learning robot teams. The methodology developed for this work has been applied to multirobot foraging soccer and cooperative movement. This paper focuses specifically on results in multirobot foraging. In these experiments three types of reward are used with Qlearning to train a multirobot team to forageing a local performancebased reward a global performancebased reward and a heuristic strategy referred to as shaped reinforcement. Local strategies provide each agent a specific reward according to its own behavior while global rewards provide all the agents on the team the same reward simultaneously. Shaped reinforcement provides a heuristic reward for an agent's action given its situation. The experiments indicate that local performance based rewards and shaped reinforcement generate statistically similar results, they both provide the best performance and the least diversity. Finally learned policies are demonstrated on a team of Nomadic Technologies' Nomad-150 robots. <br> - </details>

<br/>

### Theoretical frameworks for MARL

<details> <summary> <a href="https://courses.cs.duke.edu/spring07/cps296.3/littman94markov.pdf"> Markov games as a framework for multiagent reinforcement learning  </a>by Michael L. Littman. ICML, 1994. <a href="link">  </a> </summary> In the Markov decision process(MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic. <br> - </details>

<details> <summary> <a href="https://www.lirmm.fr/~jq/Cours/3cycle/module/HuWellman98icml.pdf"> Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm </a>by Junling Hu, Michael P. Wellman. ICML, 1998. <a href="link">  </a> </summary> In this paper, we adopt general-sum stochastic games as a framework for multiagent reinforcement learning. Our work extends previous work by Littman on zero-sum stochastic games to a broader framework. We design a multiagent Q-learning method under this framework, and prove that it converges to a Nash equilibrium under specified conditions. This algorithm is useful for finding the optimal strategy when there exists a unique Nash equilibrium in the game. When there exist multiple Nash equilibria in the game, this algorithm should be combined with other learning techniques to find optimal strategies. <br> - </details>

<details> <summary> <a href="https://www.semanticscholar.org/paper/Multiagent-Reinforcement-Learning-in-Stochastic-Hu-Wellman/7ce14dbb9add4d9656746703babd00d8f765b22a"> Multiagent reinforcement learning in stochastic games </a>by Hu, J., Wellman, M.P. Cambridge University Press, 1999. <a href="link">  </a> </summary> We adopt stochastic games as a general framework for dynamic noncooperative systems. This framework provides a way of describing the dynamic interactions of agents in terms of individuals' Markov decision processes. By studying this framework, we go beyond the common practice in the study of learning in games, which primarily focus on repeated games or extensive-form games. For stochastic games with incomplete information, we design a multiagent reinforcement learning method which allows agents to learn Nash equilibrium strategies. We show in both theory and experiments that this algorithm converges. From the viewpoint of machine learning research, our work helps to establish the theoretical foundation for applying reinforcement learning, originally deened for single-agent systems, to multiagent systems. <br> - </details>

<br/>

### Theses

<details> <summary> <a href="http://reports-archive.adm.cs.cmu.edu/anon/1998/CMU-CS-98-187.pdf"> Layered Learning in Multi-Agent Systems </a>by Peter Stone. PhD thesis, 1998. <a href="link">  </a> </summary> Multi-agent systems in complex, real-time domains require agents to act effectively both autonomously and as part of a team. This dissertation addresses multi-agent systems consisting of teams of autonomous agents acting in real-time, noisy, collaborative, and adversarial environments. Because of the inherent complexity of this type of multi-agent system, this thesis investigates the use of machine learning within multi-agent systems. The dissertation makes four main  contributions to the fields of Machine Learning and Multi-Agent Systems. First, the thesis defines a team member agent architecture within which a exible teamstructure is presented, allowing agents to decompose the task space into exible roles and allowing them to smoothly switch roles while acting. Team organization is achieved by the introduction of a locker-room agreement as a collection of conventions followed by all team members. It defines agent roles, team formations, and pre-compiled multi-agent plans. In addition, the team member agent architecture includes a communication paradigm for domains with single-channel, low-bandwidth, unreliable communication. The communication paradigm facilitates team coordination while being robust to lost messages and active interference from opponents. Second, the thesis introduces layered learning, a general-purpose machine learning paradigm for complex domains in which learning a mapping directly from agents' sensors to their actuators is intractable. Given a hierarchical task decomposition, layered learning allows for learning at each level of the hierarchy, with learning at each level directly affecting learning at the next higher level. Third, the thesis introduces a new multi-agent reinforcement learning algorithm, namely team-partitioned, opaque-transition reinforcement learning (TPOT-RL). TPOT-RL is designed for domains in which agents cannot necessarily observe the state changes when other team members act. It exploits local, action-dependent features to aggressively generalize its input representation for learning and partitions the task among the agents, allowing them to simultaneously learn collaborative policies by observing the long-term effects of their actions. Fourth, the thesis contributes a fully functioning multi-agent system that incorporates learning in a real-time, noisy domain with teammates and adversaries. Detailed algorithmic descriptions of the agents' behaviors as well as their source code are included in the thesis. Empirical results validate all four contributions within the simulated robotic soccer domain. The generality of the contributions is verified by applying them to the real robotic soccer, and network routing domains. Ultimately, this dissertation demonstrates that by learning portions of their cognitive processes, selectively communicating, and coordinating their behaviors via common knowledge, a group of independent agents can work towards a common goal in a complex, real-time, noisy, collaborative, and adversarial environment. <br> - </details>

<details> <summary> <a href="https://aaltodoc.aalto.fi/bitstream/handle/123456789/2486/isbn9512273594.pdf?sequence=1&isAllowed=y"> Multiagent Reinforcement Learning in Markov Games: Asymmetric and Symmetric approaches </a>by Ville Kononen. PhD dissertation, 2004. <a href="link">  </a> </summary> Modern computing systems are distributed, large, and heterogeneous. Computers, other information processing devices and humans are very tightly connected with each other and therefore it would be preferable to handle these entities more as agents than stand-alone systems. One of the goals of artificial intelligence is to understand interactions between entities, whether they are artificial or natural, and to suggest how to make good decisions while taking other decision makers into account. In this thesis, these interactions between intelligent and rational agents are modeled with Markov games and the emphasis is on adaptation and learning in multiagent systems. Markov games are a general mathematical tool for modeling interactions between multiple agents. The model is very general, for example common board games are special instances of Markov games, and particularly interesting because it forms an intersection of two distinct research disciplines: machine learning and game theory. Markov games extend Markov decision processes, a well-known tool for modeling single-agent problems, to multiagent domains. On the other hand, Markov games can be seen as a dynamic extension to strategic form games, which are standard models in traditional game theory. From the computer science perspective, Markov games provide a flexible and efficient way to describe different social interactions between intelligent agents. This thesis studies different aspects of learning in Markov games. From the machine learning perspective, the focus is on a very general learning model, i.e. reinforcement learning, in which the goal is to maximize the long-time performance of the learning agent. The thesis introduces an asymmetric learning model that is computationally efficient in multiagent systems and enables the construction of different agent hierarchies. In multiagent reinforcement learning systems based on Markov games, the space and computational requirements grow very quickly with the number of learning agents and the size of the problem instance. Therefore, it is necessary to use function approximators, such as neural networks, to model agents in many real-world applications. In this thesis, various numeric learning methods are proposed for multiagent learning problems. The proposed methods are tested with small but non-trivial example problems from different research areas including artificial robot navigation, simplified soccer game, and automated pricing models for intelligent agents. The thesis also contains an extensive literature survey on multiagent reinforcement learning and various methods based on Markov games. Additionally, game-theoretic methods and methods originated from computer science for multiagent learning and decision making are compared. <br> - </details>

<br/>

<!-- BREAK -->

<!-- <details> <summary> <a href="link"> title </a>by authors. Conference, year. <a href="link">  </a> </summary> abstract <br> - </details> -->

<details> <summary> <a href="http://www.cs.toronto.edu/kr/papers/auctions.pdf"> Sequential Auctions for the Allocation of Resources with Complementarities </a>by Craig Boutilier, Moises Goldszmidt, Bikash Sabata. IJCAI, 1999. <a href="link">  </a> </summary> Market-based mechanisms such as auctions are being studied as an appropriate means for resource allocation in distributed and multiagent decision problems. When agents value resources in combination rather than in isolation, one generally relies on combinatorial auctions where agents bid for resource bundles, orsimultaneous auctionsfor all resources. We develop a different model, where agents bid for required resourcessequentially. This model has the advantage that it can be applied in settings where combinatorial and simultaneous models are infeasible (e.g., when resources are made available at different points in time by different parties), as well as certain benefits in settings where combinatorial models are applicable. We develop a dynamic programming model for agents to compute bidding policies based on estimated distributions over prices. We also describe how these distributions are updated to provide a learning model for bidding behavior <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/2004/file/88fee0421317424e4469f33a48f50cb0-Paper.pdf"> Convergence and No-Regret in Multiagent Learning </a>by Michael Bowling. NeurIPS, 2004. <a href="link">  </a> </summary> Learning in a multiagent system is a challenging problem due to two key factors. First, if other agents are simultaneously learning then the environment is no longer stationary, thus undermining convergence guarantees. Second, learning is often susceptible to deception, where the other agents may be able to exploit a learner’s particular dynamics. In the worst case, this could result in poorer performance than if the agent was not learning at all. These challenges are identifiable in the two most common evaluation criteria for multiagent learning algorithms: convergence and regret. Algorithms focusing on convergence or regret in isolation are numerous. In this paper, we seek to address both criteria in a single algorithm by introducing GIGA-WoLF, a learning algorithm for normalform games. We prove the algorithm guarantees at most zero average regret, while demonstrating the algorithm converges in many situations of self-play. We prove convergence in a limited setting and give empirical results in a wider variety of situations. These results also suggest a third new learning criterion combining convergence and regret, which we call negative non-convergence regret (NNR). <br> - </details>

<details> <summary> <a href="https://apps.dtic.mil/sti/pdfs/ADA385122.pdf"> An Analysis of Stochastic Game Theory for Multiagent Reinforcement Learning </a>by Michael Bowling, Manuela Veloso. Technical Report, 2000. <a href="link">  </a> </summary> Learning behaviors in a multiagent environmentis crucial for developing and adapting multiagent systems. Reinforcement learning techniques have addressed this problem for a single agent acting in a stationary environment, which is modeled as a Markov decision process (MDP). But, multiagent environments are inherently non-stationary since the other agents are free to change their behavior as they also learn and adapt. Stochastic games, first studied in the game theory community, are a natural extension of MDPs to include multiple agents. In this paper we contribute a comprehensive presentation ofthe relevanttechniques for solving stochastic games from both the game theory community and reinforcement learning communities. We examine the assumptions and limitations of these algorithms, and identify similarities between these algorithms, single agent reinforcement learners, and basic game theory techniques <br> - </details>

<details> <summary> <a href="https://www.sciencedirect.com/science/article/pii/S0004370202001212/pdf?crasolve=1&r=76e929ed589406db&ts=1669198737508&rtype=https&vrr=UKN&redir=UKN&redir_fr=UKN&redir_arc=UKN&vhash=UKN&host=d3d3LnNjaWVuY2VkaXJlY3QuY29t&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&iv=7f135f0e0222bf00fdd4304ee2a1ae9f&token=36303334373233626432363262646163646435326538633139323566646439336462623432323537373965623737383261653335393965373534323332333532616365373333346337623830666338373735616139396537333962383a333465363031303533343635303962346333666262613232&text=9e36f3db92abeb589d5da8a52095e27f9670f9d838b2ad4862938be7f725337583a19749fa0cc538c097a5baf504157a6cefe332eae16f6f272bc064381f4524219bd0363a96bdf58d6e26627fa730ff6cec4c9ad6d794be01e48ff4e223857fa5f9011bcb6a38bcfb560984c7bef5f934b93e74793f2a1547243e01e692b606979614b88b57ba5f4833e30854615a2d02124eae78f47ac7001997788775368d12924e437912cf828d5431f7a3e769024e7459bbd1997839f8b098ba49c013f6876d4d9d1a2b78a1705aeadb49566b37ec1f3f3a816efba12093a43af51508f89b60bbbc41bb6d3c2ca20e5dd140af278c1436b5ea7c507e8053624890df9b6f12f39a3dbf7cac42ab4dca5b8356300411cf58a5c91c81d91bd4811d65cd14aa4b3959746ebaac377a8115b4801ee33a&original=3f6d64353d3236333032333763363663366136323039316333326433373234316337393064267069643d312d73322e302d53303030343337303230323030313231322d6d61696e2e706466"> Multiagent learning using a variable learning rate </a>by Michael Bowling, Manuela Veloso. Artificial Intelligence, 2002. <a href="link">  </a> </summary> Learning to act in a multiagent environment is a difficult problem since the normal definition of an optimal policy no longer applies. The optimal policy at any moment depends on the policies of the other agents. This creates a situation of learning a moving target. Previous learning algorithms have one of two shortcomings depending on their approach. They either converge to a policy that may not be optimal against the specific opponents’ policies, or they may not converge at all. In this article we examine this learning problem in the framework of stochastic games. We look at a number of previous learning algorithms showing how they fail at one of the above criteria. We then contribute a new reinforcement learning technique using a variable learning rate to overcome these shortcomings. Specifically, we introduce the WoLF principle, “Win or Learn Fast”, for varying the learning rate. We examine this technique theoretically, proving convergence in self-play on a restricted class of iterated matrix games. We also present empirical results on a variety of more general stochastic games, in situations of self-play and otherwise, demonstrating the wide applicability of this method <br> - </details>

<details> <summary> <a href="https://dl.acm.org/doi/pdf/10.1145/336992.337000"> Automated Strategy Searches in an Electronic Goods Market: Learning and Complex Price Schedules </a>by Christopher H. Brooks, Scott Fay, Rajarshi Das, Jeffrey K. MacKie-Masons,
Jeffrey Kephartt, Edmund H. Durfee. ACM-EC, 1999. <a href="link">  </a> </summary> Markets for electronic goods provide the possibility of exploring new and more complex pricing schemes, due to the flexibility of information goods and negligible marginal cost. In this paper we compare dynamic performance across price schedules of varying complexity. We provide a monopolist producer with two machine learning methods which implement a strategy that balances exploitation to maximize current profits against exploration to improve future profits. We find that the complexity of the price schedule affects both the amount of exploration necessary and the aggregate profit received by a producer. In general, simpler price schedules are more robust and give up less profit during the learning periods even though the more complex schedules have higher long-run profits. These results hold for both learning methods, even though the relative performance of the methods is quite sensitive to differences in the smoothness of the profit landscape for different price schedules. Our results have implications for automated learning and strategic pricing in non-stationary environments, which arise when the consumer population changes, individuals change their preferences, or competing firms change their strategies. <br> - </details>

<details> <summary> <a href="http://shiftleft.com/mirrors/www.hpl.hp.com/techreports/2002/HPL-2002-321.pdf"> Applying Evolutionary Game Theory to Auction Mechanism Design </a>by Andrew Byde. ACM-EC, 2003. <a href="link">  </a> </summary> In this paper we describe an evolution-based method for evaluating auction mechanisms, and apply it to a space of mechanisms including the standard first- and second-price sealed bid auctions. We replicate results known already in the Auction Theory literature regarding the suitability of different mechanisms for different bidder environments, and extend the literature by establishing the superiority of novel mechanisms over standard mechanisms, for commonly occurring scenarios. Thus this paper simultaneously extends Auction Theory, and provides a systematic method for further such extensions. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Georgios-Chalkiadakis/publication/2553318_Coordination_in_Multiagent_Reinforcement_Learning_A_Bayesian_Approach/links/09e4151363949e7f80000000/Coordination-in-Multiagent-Reinforcement-Learning-A-Bayesian-Approach.pdf"> Coordination in Multiagent Reinforcement Learning: A Bayesian Approach </a>by Georgios Chalkiadakis, Craig Boutilier. AAMAS, 2003. <a href="link">  </a> </summary> Much emphasis in multiagent reinforcement learning (MARL) research is placed on ensuring that MARL algorithms (eventually) converge to desirable equilibria. As in standard reinforcement learning, convergence generally requires sufficient exploration of strategy space. However, exploration often comes at a price in the form of penalties or foregone opportunities. In multiagent settings, the problem is exacerbated by the need for agents to “coordinate” their policies on equilibria. We propose a Bayesian model for optimal exploration in MARL problems that allows these exploration costs to be weighed against their expected benefits using the notion of value of information. Unlike standard RL models, this model requires reasoning about how one’s actions will influence the behavior of other agents. We develop tractable approximations to optimal Bayesian exploration, and report on experiments illustrating the benefits of this approach in identical interest games. <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/2003/file/c8067ad1937f728f51288b3eb986afaa-Paper.pdf"> All learning is local: Multi-agent learning in global reward games </a>by Yu-Han Chang, Tracey Ho, Leslie Pack Kaelbling. NeurIPS, 2003. <a href="link">  </a> </summary> In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efficient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman filtering to allow an agent to construct a good training signal and learn an effective policy <br> - </details>

<details> <summary> <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=1046&context=sis_research"> Walverine: A walrasian trading agent </a>by Shih-Fen Cheng, Evan Leung, Kevin M. Lochner, Kevin O’Malley, Daniel M. Reeves,Julian L. Schvartzman, Michael P. Wellman. AAMAS, 2003. <a href="link">  </a> </summary> TAC-02 was the third in a series of Trading Agent Competition events fostering research in automating trading strategies by showcasing alternate approaches in an open-invitation market game. TAC presents a challenging travel-shopping scenario where agents must satisfy client preferences for complementary and substitutable goods by interacting through a variety of market types. Michigan’s entry, Walverine, bases its decisions on a competitive (Walrasian) analysis of the TAC travel economy. Using this Walrasian model, we construct a decision-theoretic formulation of the optimal bidding problem, which Walverine solves in each round of bidding for each good. Walverine’s optimal bidding approach, as well as several other features of its overall strategy, are potentially applicable in a broad class of trading environments. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/s10994-006-0143-1.pdf"> AWESOME: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents </a>by Vincent Conitzer, Tuomas Sandholm. Machine Learning, 2007. <a href="link">  </a> </summary> Two minimal requirements for a satisfactory multiagent learning algorithm are that it 1. learns to play optimally against stationary opponents and 2. converges to a Nash equilibrium in self-play. The previous algorithm that has come closest, WoLF-IGA, has been proven to have these two properties in 2-player 2-action (repeated) games—assuming that the opponent’s mixed strategy is observable. Another algorithm, ReDVaLeR (which was introduced after the algorithm described in this paper), achieves the two properties in games with arbitrary numbers of actions and players, but still requires that the opponents’ mixed strategies are observable. In this paper we present AWESOME, the first algorithm that is guaranteed to have the two properties in games with arbitrary numbers of actions and players. It is still the only algorithm that does so while only relying on observing the other players’ actual actions (not their mixed strategies). It also learns to play optimally against opponents that eventually become stationary. The basic idea behind AWESOME (Adapt When Everybody is Stationary, Otherwise Move to Equilibrium) is to try to adapt to the others’ strategies when they appear stationary, but otherwise to retreat to a precomputed equilibrium strategy. We provide experimental results that suggest that AWESOME converges fast in practice. The techniques used to prove the properties of AWESOME are fundamentally different from those used for previous algorithms, and may help in analyzing future multiagent learning algorithms as well. <br> - </details>

<details> <summary> <a href="http://nozdr.ru/data/media/biblio/kolxoz/Cs/CsLn/L/Learning%20and%20Adaption%20in%20Multi-Agent%20Systems,%201%20conf.,%20LAMAS%202005(LNCS3898,%20Springer,%202006)(ISBN%203540330534)(224s).pdf#page=137"> Multiagent Traffic Management: Opportunities for Multiagent Learning </a>by Kurt Dresner, Peter Stone. LAMAS, 2005. <a href="link">  </a> </summary> Traffic congestion is one of the leading causes of lost productivity and decreased standard of living in urban settings. In previous work published at AAMAS, we have proposed a novel reservation-based mechanism for increasing throughput and decreasing delays at intersections. In more recent work, we have provided a detailed protocol by which two different classes of agents (intersection managers and driver agents) can use this system. We believe that the domain created by this mechanism and protocol presents many opportunities for multiagent learning on the parts of both classes of agents. In this paper, we identify several of these opportunities and offer a first-cut approach to each <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/B:MACH.0000039779.47329.3a.pdf"> Integrating Guidance into Relational Reinforcement Learning </a>by KURT DRIESSENS, SASO DZEROSKI. Machine Learning, 2004. <a href="link">  </a> </summary> Reinforcement learning, and Q-learning in particular, encounter two major problems when dealing with large state spaces. First, learning the Q-function in tabular form may be infeasible because of the excessive amount of memory needed to store the table, and because the Q-function only converges after each state has been visited multiple times. Second, rewards in the state space may be so sparse that with random exploration they will only be discovered extremely slowly. The first problem is often solved by learning a generalization of the encountered examples (e.g., using a neural net or decision tree). Relational reinforcement learning (RRL) is such an approach; it makes Q-learning feasible in structural domains by incorporating a relational learner into Q-learning. The problem of sparse rewards has not been addressed for RRL. This paper presents a solution based on the use of “reasonable policies” to provide guidance. Different types of policies and different strategies to supply guidance through these policies are discussed and evaluated experimentally in several relational domains to show the merits of the approach. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/a:1007694015589.pdf"> Relational Reinforcement Learning </a>by SASO DZEROSKI, LUC DE RAEDT, KURT DRIESSENS. Machine Learning, 2001. <a href="link">  </a> </summary> Relational reinforcement learning is presented, a learning technique that combines reinforcement learning with relational learning or inductive logic programming. Due to the use of a more expressive representation language to represent states, actions and Q-functions, relational reinforcement learning can be potentially applied to a new range of learning tasks. One such task that we investigate is planning in the blocks world, where it is assumed that the effects of the actions are unknown to the agent and the agent has to learn a policy. Within this simple domain we show that relational reinforcement learning solves some existing problems with reinforcement learning. In particular, relational reinforcement learning allows us to employ structural representations, to abstract from specific goals pursued and to exploit the results of previous learning phases when addressing new (more complex) situations. <br> - </details>

<details> <summary> <a href="http://nozdr.ru/data/media/biblio/kolxoz/Cs/CsLn/L/Learning%20and%20Adaption%20in%20Multi-Agent%20Systems,%201%20conf.,%20LAMAS%202005(LNCS3898,%20Springer,%202006)(ISBN%203540330534)(224s).pdf#page=147"> Dealing with Errors in a Cooperative Multi-agent Learning System </a>by Constanca Oliveira e Sousa and Luis Custodio. LAMAS, 2005. <a href="link">  </a> </summary> This paper presents some methods of dealing with the problem of cooperative learning in a multi-agent system, in error prone environments. A system is developed that learns by reinforcement and is robust to errors that can come from the agents sensors, from another agent that shares wrong information or even from the communication channel. <br> - </details>

<details> <summary> <a href="http://iebi.nctu.edu.tw/course_old/9515492/Final_Report/9534530/9534530-3.pdf"> The Importance of Ordering in Sequential Auctions </a>by Wedad Elmaghraby. Management Science, 2003. <a href="link">  </a> </summary> To date, the largest part of literature on multi-unit auctions has assumed that there are k homogeneous objects being auctioned, where each bidder wishes to win exactly one or all of k units. These modeling assumptions have made the examination of ordering in sequential auctions inconsequential. The aim of this paper is to introduce and highlight the critical influence that ordering can have on the efficiency of an auction. We study a buyer who outsources via sequential 2nd-price auctions two heterogeneous jobs, and faces a diverse set of suppliers with capacity constraints <br> - </details>

<details> <summary> <a href="https://digital.csic.es/bitstream/10261/160955/1/RAS24(1998)_159-82.pdf"> Negotiation Decision Functions for Autonomous Agents </a>by Peyman Faratin, Carles Sierra, Nick R Jenning. Conference, year. <a href="link">  </a> </summary>  We present a formal model of negotiation between autonomous agents The purpose of the negotiation is to reach an agreement about the provision of a service by one agent for another The model defines a range of strategies and tactics that agents can employ to generate initial offers, evaluate proposals and offer counter proposals. The model is based on computationally tractable assumptions, demonstrated in the domain of business process management and empirically evaluate <br> - </details>

<details> <summary> <a href="https://pdf.sciencedirectassets.com/271585/1-s2.0-S0004370200X00963/1-s2.0-S0004370202002904/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIGFP1%2Bs3XmZFMMz356aNwAmhMuGI9k1k7jf9Har%2FKjOTAiEAp31KCbp0yCHszItK5g5OpUYeeE4yIjBe9A%2FnJss4LyEq1QQIv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDAJhRmh%2FklRoMSMnWCqpBGypJb7gu7MTorfY6Hnc3oMb%2F2dFRfOlhT1Y3uEje99X%2FixlxlioaCFUcWucjZWBNb4U56DGXJwkJUQlUbm8JIv232PXklf%2B%2BYzpVtgB5jgBWizuCzEC1nGHT3jic%2BtWmsI7%2Fw6ZH8Zy4BaW34r4IlMxtIt2cBSPqk2%2B1DIEr3NRLOPV%2B0vZeDFgpU8gCgqQAFJp%2BfW5C%2FUodnBSWNkLV2nHGzL7PeXpBRpUojCqLXqC20BgFjb%2F9nsTfwxkDtrYWpzV8deMmWrBcuMJJWK2y5oBwvDHE9T0lup2DH%2BqWBkwcPXGLsyyM03tBgQjjoNw903SQagtCDDkRILKDgzUrj%2FYX4TjDRn3mSwUk9me5jMS8YB2yexIW6p5STYEMwqcJ4l7v%2BboFeS%2F1321ADrE77zx06hamEEP2uPtTKYORKGk0WyGi4cnh1JgDorV5wIMaHNYN5vBFxSVp9uEpIVABv7ZJpzj0%2FvL7k%2B47UHe68Mzjmsk1pgTuV0ccAPRy6nAlaxcR3PsgI8egXmZ4sEZ2o9xht3Yhw1P8YXmE7ZR1vDkSqWVst1mCrJ0WXq01pZSfz6%2BDYi3EcQPQr%2FEl%2FG7auzznum89Dx25zaiHw%2BhT8iJ3VmN1oAJ9KkjQbM7cseO4KpS0akFkniEDMDYfBt%2BQYNPJ07b2M6xEA98lyzwiiVCEuS%2FgXx%2Fw1UpSmAW1yCiFDPYtFloqE7LVU2gIuzZ6jxlOMYR3TYKMUgwtsf4mwY6qQHJRvMhFn1auKaTVI0w14zxYddvAr7RxGsDW1R9XvZBRm%2BYCJ%2Fhe5h82uWaGUJAwIqorYqpoE3Zke4VBJTiebdscWaNoKDWh9Na7Jhq0QEyaF9CqrkTyeYROqGe1OT2yOyoAG8%2FzOBWHnvQfFSKXfTVllBPvpTPdtlL%2Fu160kbIxjReqSMVNgkjmKdcgCVOT0guvZReygFelqRn%2FmWRSf8TUfCSGfClQ4aO&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221123T143152Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6ZQP7CUN%2F20221123%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=366f562e104e1ce07189adf5a3b637ef9d279201d7968aaea2b89f3746e10b68&hash=99f87d2a61c2835d7e9af934f74013b81ca0f23dafe0c8db633e045f298ac604&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0004370202002904&tid=spdf-ea80df82-46a9-4cd0-9633-76313cf3bffa&sid=a340001a289239432d6b3090763952e8abdegxrqb&type=client&ua=515902055c0556585506&rr=76ea9c674f613f1f"> Using similarity criteria to make issue trade-offs in automated negotiations </a>by P. Faratin, C. Sierra, N.R. Jennings. Artificial Intelligence, 2002. <a href="link">  </a> </summary> Automated negotiation is a key form of interaction in systems that are composed of multiple autonomous agents. The aim of such interactions is to reach agreements through an iterative process of making offers. The content of such proposals are, however, a function of the strategy of the agents. Here we present a strategy called the trade-off strategy where multiple negotiation decision variables are traded-off against one another (e.g., paying a higher price in order to obtain an earlier delivery date or waiting longer in order to obtain a higher quality service). Such a strategy is commonly known to increase the social welfare of agents. Yet, to date, most computational work in this area has ignored the issue of trade-offs, instead aiming to increase social welfare through mechanism design. The aim of this paper is to develop a heuristic computational model of the trade-off strategy and show that it can lead to an increased social welfare of the system. A novel linear algorithm is presented that enables software agents to make trade-offs for multi-dimensional goods for the problem of distributed resource allocation. Our algorithm is motivated by a number of real-world negotiation applications that we have developed and can operate in the presence of varying degrees of uncertainty. Moreover, we show that on average the total time used by the algorithm is linearly proportional to the number of negotiation issues under consideration. This formal analysis is complemented by an empirical evaluation that highlights the operational effectiveness of the algorithm in a range of negotiation scenarios. The algorithm itself operates by using the notion of fuzzy similarity to approximate the preference structure of the other negotiator and then uses a hill-climbing technique to explore the space of possible trade-offs for the one that is most likely to be acceptable <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/258568/1/amec03-shaheen.pdf"> Comparing Equilibria for Game-Theoretic and Evolutionary Bargaining Models </a>by Shaheen Fatima, Michael Wooldridge, Nicholas R. Jennings. Conference, year. <a href="link">  </a> </summary> Game-theoretic models of bargaining are typically based on the assumption that players have perfect rationality and that they always play an equilibrium strategy. In contrast, research in experimental economics shows that in bargaining between human subjects, participants do not always play the equilibrium strategy. Such agents are said to be boundedly rational. In playing a game against a boundedly rational opponent, a player’s most effective strategy is not the equilibrium strategy, but the one that is the best reply to the opponent’s actual strategy. Against this background, this paper studies the bargaining behavior of boundedly rational agents by using genetic algorithms. Since bargaining involves players with different utility functions, we have two subpopulations – one represents the buyer, and the other represents the seller (i.e., the population is asymmetric). We study the competitive co-evolution of strategies in the two subpopulations for an incomplete information setting, and compare the results with those prescribed by game theory. Our analysis leads to two main conclusions. Firstly, our study shows that although each agent in the game-theoretic model has a strategy that is dominant at every period at which it makes a move, the stable state of the evolutionary model does not always match the game-theoretic equilibrium outcome. Secondly, as the players mutually adapt to each other’s strategy, the stable outcome depends on the initial population <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/3-540-45356-3.pdf"> A Game-Theoretic Approach to the Simple Coevolutionary Algorithm </a>by Sevan G. Ficici, Jordan B. Pollack. LNCS, 2000. <a href="link">  </a> </summary> The fundamental distinction between ordinary evolutionary algorithms (EA) and co-evolutionary algorithms lies in the interaction between coevolving entities. We believe that this property is essentially game-theoretic in nature. Using game theory, we describe extensions that allow familiar mixing-matrix and Markov-chain models of EAs to address coevolutionary algorithm dynamics. We then employ concepts from evolutionary game theory to examine design aspects of conventional coevolutionary algorithms that are poorly understood. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-1-4419-8909-3.pdf"> Selection in Coevolutionary Algorithms and the Inverse Problem </a>by Sevan Ficici, Ofer Melnik, Jordan Pollack. Springer, 2004. <a href="link">  </a> </summary> The inverse problem in the collective intelligence framework concerns how the private utility functions of agents can be engineered so that their selfish behaviors collectively give rise to a desired world state. In this chapter we examine several selection and fitnesssharing methods used in coevolution and consider their operation with respect to the inverse problem. The methods we test are truncation and linear-rank selection and competitive and similarity-based fitness sharing. Using evolutionary game theory to establish the desired world state, our analyses show that variable-sum games with polymorphic Nash are problematic for these methods. Rather than converge to polymorphic Nash, the methods we test produce cyclic behavior, chaos, or attractors that lack game-theoretic justification and therefore fail to solve the inverse problem. The private utilities of the evolving agents may thus be viewed as poorly factored—improved private utility does not correspond to improved world utility. <br> - </details>

<details> <summary> <a href="link"> title </a>by authors. Conference, year. <a href="link">  </a> </summary> abstract <br> - </details>

<details> <summary> <a href="link"> title </a>by authors. Conference, year. <a href="link">  </a> </summary> abstract <br> - </details>

<details> <summary> <a href="link"> title </a>by authors. Conference, year. <a href="link">  </a> </summary> abstract <br> - </details>

<details> <summary> <a href="link"> title </a>by authors. Conference, year. <a href="link">  </a> </summary> abstract <br> - </details>

<details> <summary> <a href="link"> title </a>by authors. Conference, year. <a href="link">  </a> </summary> abstract <br> - </details>
