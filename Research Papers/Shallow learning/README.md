This is a collection of older papers on the topic of learning in multi-agent systems. We sort papers by publication date and survey subtopic. Any additions to this repo are welcome.

### Auctions, Bidding and Negotiation

<details> <summary> <a href="http://59.90.80.165:8080/jspui/bitstream/123456789/64/1/downloadme%20%282%29.pdf"> Bargaining and Markets </a>by Martin J. Osborne, Ariel Rubinstein. Academic Press, 1990. <a href="link">  </a> </summary> The formal theory of bargaining originated with John Nash's work in the early 1950s. This book discusses two recent developments in this theory. The first uses the tool of extensive games to construct theories of bargaining in which time is modeled explicitly. The second applies the theory of bargaining to the study of decentralized markets. Rather than surveying the field, the authors present a select number of models, each of which illustrates a key point. In addition, they give detailed proofs throughout the book. It uses a small number of models, rather than a survey of the field, to illustrate key points, and includes detailed proofs given as explanations for the models. The text has been class-tested in a semester-long graduate course. <br> - </details>

<details> <summary> <a href="https://www.jair.org/index.php/jair/article/view/10106/23927"> A Market-Oriented Programming Environment and its Application to Distributed Multicommodity Flow Problems </a>by M. P. Wellman. JAIR, 1993. <a href="link">  </a> </summary> Market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem solving, we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy. WALRAS provides basic constructs for defining computational market structures, and protocols for deriving their corresponding price equilibria. In a particular realization of this approach for a form of multicommodity flow problem, we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation, and that the behavior of the system can be meaningfully analyzed in economic terms. <br> - </details>

<details> <summary> <a href="https://authors.library.caltech.edu/80874/1/2118073.pdf">  A Note on Sequential Auctions </a>by DAN BERNHARDT, DAVID SCOONES. American Economic Review, 1994. <a href="link">  </a> </summary> This note explores multiobject, sequential, private-value auctions. <br> - </details>

<details> <summary> <a href="http://www.cs.ox.ac.uk/people/michael.wooldridge/pubs/paam96.pdf"> Production sequencing as negotiation </a>by M. Wooldridge, S. Bussmann, and M. Klosterberg. In Proceedings of the First International Conference on the Practical Application of Intelligent Agents and Multi-Agent Technology (PAAM-96),1996. <a href="link">  </a> </summary> 
The production sequencing problem involves a factory generating a product sequence such that when processed, the sequence will both satisfy current orders, and minimize overall costs. In this paper, we argue that production sequencing may fruitfully be considered as a negotiation problem, in which production cells within a factory negotiate over product sequences in order to fairly distribute costs. We begin by describing and formally defining the production sequencing problem; we then investigate its complexity, and present an analysis of it using the tools of game and negotiation theory. We then define a negotiation algorithm for production sequencing, and discuss what assumptions and simplifications must be made in order to make this algorithm suitable for implementation. We conclude by discussing issues for future wor <br> - </details>

<details> <summary> <a href="http://www2.econ.iastate.edu/tesfatsi/binmore1.pdf"> Applying Game Theory to Automated Negotiation </a>by Ken Binmore and Nir Vulkan. Workshop on Economics, Game Theory and the Internet, 1997. <a href="link">  </a> </summary> With existing technology, it is already possible for personal agents to schedule meetings for their users, to write the small print of an agreement, and for agents to search the Internet for the cheapest price. But serious negotiation cranks the difficulty of the problem up several notches. In this paper, we review what game theory has to offer in the light of experience gained in programming automated agents within the ADEPT (Advance Decision Environment for Process Tasks) project, which is currently being used by British Telecom for some purposes <br> - </details>

<details> <summary> <a href="https://digital.csic.es/bitstream/10261/160955/1/RAS24(1998)_159-82.pdf"> Negotiation Decision Functions for Autonomous Agents </a>by Peyman Faratin, Carles Sierra, Nick R Jenning. Journal of Robotics and Autonomous Systems, 1998. <a href="link">  </a> </summary>  We present a formal model of negotiation between autonomous agents The purpose of the negotiation is to reach an agreement about the provision of a service by one agent for another The model defines a range of strategies and tactics that agents can employ to generate initial offers, evaluate proposals and offer counter proposals. The model is based on computationally tractable assumptions, demonstrated in the domain of business process management and empirically evaluate <br> - </details>

<details> <summary> <a href="http://www.cs.toronto.edu/kr/papers/auctions.pdf"> Sequential Auctions for the Allocation of Resources with Complementarities </a>by Craig Boutilier, Moises Goldszmidt, Bikash Sabata. IJCAI, 1999. <a href="link">  </a> </summary> Market-based mechanisms such as auctions are being studied as an appropriate means for resource allocation in distributed and multiagent decision problems. When agents value resources in combination rather than in isolation, one generally relies on combinatorial auctions where agents bid for resource bundles, orsimultaneous auctionsfor all resources. We develop a different model, where agents bid for required resourcessequentially. This model has the advantage that it can be applied in settings where combinatorial and simultaneous models are infeasible (e.g., when resources are made available at different points in time by different parties), as well as certain benefits in settings where combinatorial models are applicable. We develop a dynamic programming model for agents to compute bidding policies based on estimated distributions over prices. We also describe how these distributions are updated to provide a learning model for bidding behavior <br> - </details>

<details> <summary> <a href="https://dl.acm.org/doi/pdf/10.1145/336992.337000"> Automated Strategy Searches in an Electronic Goods Market: Learning and Complex Price Schedules </a>by Christopher H. Brooks, Scott Fay, Rajarshi Das, Jeffrey K. MacKie-Masons,
Jeffrey Kephartt, Edmund H. Durfee. ACM-EC, 1999. <a href="link">  </a> </summary> Markets for electronic goods provide the possibility of exploring new and more complex pricing schemes, due to the flexibility of information goods and negligible marginal cost. In this paper we compare dynamic performance across price schedules of varying complexity. We provide a monopolist producer with two machine learning methods which implement a strategy that balances exploitation to maximize current profits against exploration to improve future profits. We find that the complexity of the price schedule affects both the amount of exploration necessary and the aggregate profit received by a producer. In general, simpler price schedules are more robust and give up less profit during the learning periods even though the more complex schedules have higher long-run profits. These results hold for both learning methods, even though the relative performance of the methods is quite sensitive to differences in the smoothness of the profit landscape for different price schedules. Our results have implications for automated learning and strategic pricing in non-stationary environments, which arise when the consumer population changes, individuals change their preferences, or competing firms change their strategies. <br> - </details>

<details> <summary> <a href="https://hpi.de/fileadmin/user_upload/fachgebiete/plattner/teaching/Dynamic_Pricing/kep00.pdf"> Dynamic pricing by software agents </a>by Jeffrey O. Kephart, James E. Hanson, Amy R. Greenwald. Computer Networks, 2000. <a href="link">  </a> </summary> We envision a future in which the global economy and the Internet will merge, evolving into an information economy bustling with billions of economically motivated software agents that exchange information goods and services with humans and other agents. Economic software agents will differ in important ways from their human counterparts, and these dierences may have significant beneficial or harmful effects upon the global economy. It is therefore important to consider the economic incentives and behaviors of economic software agents, and to use every available means to anticipate their collective interactions. We survey research conducted by the Information Economies group at IBM Research aimed at understanding collective interactions among agents that dynamically price information goods or services. In particular, we study the potential impact of widespread shopbot usage on prices, the price dynamics that may ensue from various mixtures of automated pricing agents (or ``pricebots''), the potential use of machine-learning algorithms to improve profits, and more generally the interplay among learning, optimization, and dynamics in agentbased information economies. These studies illustrate both beneficial and harmful collective behaviors that can arise in such systems, suggest possible cures for some of the undesired phenomena, and raise fundamental theoretical issues, particularly in the realms of multi-agent learning and dynamic optimization. <br> - </details>

<details> <summary> <a href="https://deepblue.lib.umich.edu/bitstream/handle/2027.42/50440/DynamicBundling.pdf?sequence=1"> Pricing information bundles in a dynamic environment </a>by J. Kephart, C. Brooks, and R. Das. ACMEC, 2001. <a href="link">  </a> </summary> We explore a scenario in which a monopolist producer of information goods seeks to maximize its profits in a market where consumer demand shifts frequently and unpredictably. The producer may set an arbitrarily complex price schedule---a function that maps the set of purchased items to a price. However, lacking direct knowledge of consumer demand, it cannotcompute the optimal schedule. Instead, it attempts to optimize profits via trial and error. By means of a simple model of consumer demand and a modified version of a simple nonlinear optimization routine, we study a variety of parametrizations of the price schedule and quantify some of the relationships among learnability, complexity, and profitability. In particular, we show that fixed pricing or simple two-parameter dynamic pricing schedules are preferred when demand shifts frequently, but that dynamic pricing based on more complex schedules tends to be most profitable when demand shifts very infrequently. <br> - </details>

<details> <summary> <a href="https://www.cs.ox.ac.uk/people/michael.wooldridge/pubs/gdn2001.pdf"> Automated negotiation: prospects, methods, and challenges </a>by N. Jennings, P. Faratin, A. Lomuscio, S. Parsons, C. Sierra, and M. Wooldrigde. International Journal of Group Decision and Negotiation, 2001. <a href="link">  </a> </summary> This paper is to examine the space of negotiation opportunities for autonomous agents, to identify and evaluate some of the key techniques, and to highlight some of the major challenges for future automated negotiation research. This paper is not meant as a survy of the field of automated negotiation. Rather, the descriptions and assessments of the various approaches are generally undertaken with particular reference to work in which the authors have been involved. However, the specific issues raised should be viewed as being broadly applicable. <br> - </details>

<details> <summary> <a href="http://econ2.econ.iastate.edu/tesfatsi/ACEIntroSpecialIssue.JEDC2001.LT.pdf"> Introduction to the special issue on agent-based computational economics </a>by Leigh Tesfatsion. Journal of Economic Dynamics & Control, 2001. <a href="link">  </a> </summary> A brief overview of agent-based computational economics (ACE) is given, followed by a synopsis of the articles included in this special issue on ACE and in a companion special issue on ACE scheduled to appear in Computational Economics. <br> - </details>

<details> <summary> <a href="https://www.hpl.hp.com/techreports/2001/HPL-2001-107.pdf"> Economic Dynamics of Agents in Multiple Auctions </a>by Chris Preist, Andrew Byde, Claudio Bartolini. Autonomous Agents, 2001. <a href="link">  </a> </summary> Over the last few years, electronic auctions have become an increasingly important aspect of e-commerce, both in the business to business and business to consumer domains. As a result of this, it is often possible to find many auctions selling similar goods on the web. However, when an individual is attempting to purchase such a good, they will usually bid in one, or a small number, of such auctions. This results in two forms of inefficiency. Firstly, the individual may pay more for the good than would be expected in an ideal market. Secondly, some sellers may fail to make a sale that could take place in an ideal market. In this paper, we present an agent that is able to participate in multiple auctions for a given good, placing bids appropriately to secure the cheapest price. We present experiments to show; 1. Current auction markets on the web are inefficient, with trades taking place away from equilibrium price, and not all benefit from trade being extracted. 2. Our agent is able to exploit these inefficiencies, resulting in it making higher profits than the simple strategy of bidding in a small number of auctions. 3. As more participants use our agent, the market becomes more efficient. When all participants use the agent, all trades take place close to equilibrium price, and the market approaches ideal behaviour <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/cs/0205066.pdf"> Effectiveness of preference elicitation in combinatorial auctions </a>by Benoıt Hudson, Tuomas Sandholm. AMEC IV, 2002. <a href="link">  </a> </summary> Combinatorial auctions where agents can bid on bundles of items are desirable because they allow the agents to express complementarity and substitutability between the items. However, expressing one’s preferences can require bidding on all bundles. Selective incremental preference elicitation by the auctioneer was recently proposed to address this problem [4], but the idea was not evaluated. In this paper we show, experimentally and theoretically, that automated elicitation provides a drastic benefit. In all of the elicitation schemes under study, as the number of items for sale increases, the amount of information elicited is a vanishing fraction of the information collected in traditional “direct revelation mechanisms” where bidders reveal all their valuation information. Most of the elicitation schemes also maintain the benefit as the number of agents increases. We develop more effective elicitation policies for existing query types. We also present a new query type that takes the incremental nature of elicitation to a new level by allowing agents to give approximate answers that are refined only on an as-needed basis. In the process, we present methods for evaluating different types of elicitation policies. <br> - </details>

<details> <summary> <a href="https://epdf.tips/auction-theory1da4f2acfd65f4df0ff99f80a90282e777638.html"> Auction Theory </a>by Vijay Krishna. Academic press, 2002. <a href="link">  </a> </summary> Auction Theory, Second Edition improves upon his 2002 bestseller with a new chapter on package and position auctions as well as end-of-chapter questions and chapter notes. Complete proofs and new material about collusion complement Krishna’s ability to reveal the basic facts of each theory in a style that is clear, concise, and easy to follow. With the addition of a solutions manual and other teaching aids, the 2e continues to serve as the doorway to relevant theory for most students doing empirical work on auctions. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/256867/1/aamas02-byde.pdf"> Decision Procedures for Multiple Auctions </a>by Andrew Byde, Chris Preist, Nicholas R Jennings. AAMAS, 2002. <a href="link">  </a> </summary> This paper presents a decision theoretic framework that an autonomous agent can use to bid effectively across multiple, simultaneous auctions. Specifically, our framework enables an agent to make rational decisions about purchasing multiple goods from a series of auctions that operate different protocols (we deal with the English, Dutch, First-Price Sealed Bid and Vickrey cases). The framework is then used to characterize the optimal decision that an agent should take. Finally, we develop a practical algorithm that provides a heuristic approximation to this ideal <br> - </details>

<details> <summary> <a href="https://pdf.sciencedirectassets.com/271585/1-s2.0-S0004370200X00963/1-s2.0-S0004370202002904/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjELb%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIGFP1%2Bs3XmZFMMz356aNwAmhMuGI9k1k7jf9Har%2FKjOTAiEAp31KCbp0yCHszItK5g5OpUYeeE4yIjBe9A%2FnJss4LyEq1QQIv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDAJhRmh%2FklRoMSMnWCqpBGypJb7gu7MTorfY6Hnc3oMb%2F2dFRfOlhT1Y3uEje99X%2FixlxlioaCFUcWucjZWBNb4U56DGXJwkJUQlUbm8JIv232PXklf%2B%2BYzpVtgB5jgBWizuCzEC1nGHT3jic%2BtWmsI7%2Fw6ZH8Zy4BaW34r4IlMxtIt2cBSPqk2%2B1DIEr3NRLOPV%2B0vZeDFgpU8gCgqQAFJp%2BfW5C%2FUodnBSWNkLV2nHGzL7PeXpBRpUojCqLXqC20BgFjb%2F9nsTfwxkDtrYWpzV8deMmWrBcuMJJWK2y5oBwvDHE9T0lup2DH%2BqWBkwcPXGLsyyM03tBgQjjoNw903SQagtCDDkRILKDgzUrj%2FYX4TjDRn3mSwUk9me5jMS8YB2yexIW6p5STYEMwqcJ4l7v%2BboFeS%2F1321ADrE77zx06hamEEP2uPtTKYORKGk0WyGi4cnh1JgDorV5wIMaHNYN5vBFxSVp9uEpIVABv7ZJpzj0%2FvL7k%2B47UHe68Mzjmsk1pgTuV0ccAPRy6nAlaxcR3PsgI8egXmZ4sEZ2o9xht3Yhw1P8YXmE7ZR1vDkSqWVst1mCrJ0WXq01pZSfz6%2BDYi3EcQPQr%2FEl%2FG7auzznum89Dx25zaiHw%2BhT8iJ3VmN1oAJ9KkjQbM7cseO4KpS0akFkniEDMDYfBt%2BQYNPJ07b2M6xEA98lyzwiiVCEuS%2FgXx%2Fw1UpSmAW1yCiFDPYtFloqE7LVU2gIuzZ6jxlOMYR3TYKMUgwtsf4mwY6qQHJRvMhFn1auKaTVI0w14zxYddvAr7RxGsDW1R9XvZBRm%2BYCJ%2Fhe5h82uWaGUJAwIqorYqpoE3Zke4VBJTiebdscWaNoKDWh9Na7Jhq0QEyaF9CqrkTyeYROqGe1OT2yOyoAG8%2FzOBWHnvQfFSKXfTVllBPvpTPdtlL%2Fu160kbIxjReqSMVNgkjmKdcgCVOT0guvZReygFelqRn%2FmWRSf8TUfCSGfClQ4aO&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221123T143152Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6ZQP7CUN%2F20221123%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=366f562e104e1ce07189adf5a3b637ef9d279201d7968aaea2b89f3746e10b68&hash=99f87d2a61c2835d7e9af934f74013b81ca0f23dafe0c8db633e045f298ac604&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0004370202002904&tid=spdf-ea80df82-46a9-4cd0-9633-76313cf3bffa&sid=a340001a289239432d6b3090763952e8abdegxrqb&type=client&ua=515902055c0556585506&rr=76ea9c674f613f1f"> Using similarity criteria to make issue trade-offs in automated negotiations </a>by P. Faratin, C. Sierra, N.R. Jennings. Artificial Intelligence, 2002. <a href="link">  </a> </summary> Automated negotiation is a key form of interaction in systems that are composed of multiple autonomous agents. The aim of such interactions is to reach agreements through an iterative process of making offers. The content of such proposals are, however, a function of the strategy of the agents. Here we present a strategy called the trade-off strategy where multiple negotiation decision variables are traded-off against one another (e.g., paying a higher price in order to obtain an earlier delivery date or waiting longer in order to obtain a higher quality service). Such a strategy is commonly known to increase the social welfare of agents. Yet, to date, most computational work in this area has ignored the issue of trade-offs, instead aiming to increase social welfare through mechanism design. The aim of this paper is to develop a heuristic computational model of the trade-off strategy and show that it can lead to an increased social welfare of the system. A novel linear algorithm is presented that enables software agents to make trade-offs for multi-dimensional goods for the problem of distributed resource allocation. Our algorithm is motivated by a number of real-world negotiation applications that we have developed and can operate in the presence of varying degrees of uncertainty. Moreover, we show that on average the total time used by the algorithm is linearly proportional to the number of negotiation issues under consideration. This formal analysis is complemented by an empirical evaluation that highlights the operational effectiveness of the algorithm in a range of negotiation scenarios. The algorithm itself operates by using the notion of fuzzy similarity to approximate the preference structure of the other negotiator and then uses a hill-climbing technique to explore the space of possible trade-offs for the one that is most likely to be acceptable <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1004452&casa_token=_y29arx7OywAAAAA:ut4x7dKOsZeIwPZeANENUCx4AHZJw5eqnqBX6dK8u5k-CcJXyYzv3i4qMvM_7P-Zhp_KwOK_qD3Ee8M&tag=1"> Co-evolving automata negotiate with a variety of opponents </a>by authors. CEC, 2002. <a href="link">  </a> </summary> Real-life negotiations typically involve multiple parties with (i) different preferences for the different issues and (ii) bargaining strategies which change over time. Such a dynamic environment (with imperfect information) is addressed in this paper with a multi-population evolutionary algorithm (EA). Each population represents an evolving collection of bargaining strategies in our setup. The bargaining strategies are represented by a special kind of finite automata, which require only two transitions per state. We show that such automata (with a limited complexity) are a suitable choice in a computational setting. We furthermore describe an EA which generates highly-efficient bargaining automata in the course of time. A series of computational experiments shows that co-evolving automata are able to discriminate successfully between different opponents, although they receive no explicit information about the identity or preferences of their opponents. These results are important for the further development of evolving automata for real-life (agent system) applications <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/A:1023068821218.pdf"> Negotiating Complex Contracts </a>by MARK KLEIN, PEYMAN FARATIN, HIROKI SAYAMA, YANEER BAR-YAM. Group Decision and Negotiation, 2003. <a href="link">  </a> </summary> Work to date on computational models of negotiation has focused almost exclusively on defining contracts consisting of one or a few independent issues and tractable contract spaces. Many real-world contracts, by contrast, are much more complex, consisting of multiple inter-dependent issues and intractably large contract spaces. This paper describes a simulated annealing based approach appropriate for negotiating such complex contracts that achieves near-optimal social welfares for negotiations with binary issue dependencies. <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245277&casa_token=b0X5M38ekKsAAAAA:rA1HV26lSrVBoxk78BNyYE_g08ZR8TeU--aBCB149XCEV3MgVGVKTCuBSMKtqgOQJ4DdWYIo5aTbAg&tag=1"> A Fuzzy-Logic Based Bidding Strategy for Autonomous Agents in Continuous Double Auctions </a>by Minghua He, Ho-fung Leung, Nicholas R. Jennings. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2003. <a href="link">  </a> </summary> Increasingly, many systems are being conceptualized, designed, and implemented as marketplaces in which autonomous software entities (agents) trade services. These services can be commodities in e-commerce applications or data and knowledge services in information economies. In many of these cases, there are both multiple agents that are looking to procure services and multiple agents that are looking to sell services at any one time. Such marketplaces are termed continuous double auctions (CDAs). Against this background, this paper develops new algorithms that buyer and seller agents can use to participate in CDAs. These algorithms employ heuristic fuzzy rules and fuzzy reasoning mechanisms in order to determine the best bid to make given the state of the marketplace. Moreover, we show how an agent can dynamically adjust its bidding behavior to respond effectively to changes in the supply and demand in the marketplace. We then show, by empirical evaluations, how our agents outperform four of the most prominent algorithms previously developed for CDAs (several of which have been shown to outperform human bidders in experimental studies). <br> - </details>

<details> <summary> <a href="https://pdf.sciencedirectassets.com/271585/1-s2.0-S0004370200X02639/1-s2.0-S0004370203000158/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIBP5GP7dmGqi7wAg5DR0bgqcpijcuNrCmVR5%2FJjNLEyUAiAaY2zWzg3ZHyA91biTlAPOFcqnKl%2Feq9nMY7FfR6jYfirVBAju%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMW7KnF2I7cCo00gM6KqkETOGplfimfDfqVd0lLkHb01ZxcR6F1mYpd9Q1Zcwvef0DyPMYWZHYt5YzASAlUAyMt0Msak7eJ8ipxVA%2B9FN3KFOwXiuvT%2BSfbOIuHbZdxq2karkRJK3LNWyuRjm7nRUfihqbuLhAQXnDWMfi0Y8MeqTyT8o5KqY5dQ5RzABntbg2igH19KT1r95A3MOvVwcdA5nXqhtooCcwZ8gQogVWpOyGce%2BREqS7qWqFnABSxSyQyz6fBG3WWBD229TAYdew4n45HP%2FmT3hNIz9SUZxTkEsrh7T2QU46XaqCCDq3A2qlCOLwS%2FvcA96aj7Gr9UNhjHlBbg0Rvwfvbn2kXfpe72uLbCUVrTzCE%2FXYfubVL2Y2FtOdqjZ5pWbcDv2MCWDmOFa0OdfXWDM%2BGm0I0fq2IfhOG59JM2jl7vNqElnTB6sdY0MiG6f7rOwBi6C75q4IpHAgax8C0OjG61kgY%2BBDqQGxH3mCx3m9q%2BleYKUHt7O1DPNqHN1gUui0ODRdrFyCXgIlLrnQO6L7a0BYuM3DQGyon9a1RgzgiRNMnAvVb%2FJO%2BHGx7oKjuqTUWNHGB7VIgNlhnSVtMmPsqEG1EcOCel2PF2jwu194lM3YlHqfdgawRH0pHIGmqAV26o9FhlpOJ5hz6ZQnexjrWzQ6tpAfBtDCvmSJrL0WkOQNY1yCOCPfaZuruBmkMnhYN3x14S4hZHH0u6qtjfy2rIFvl7Ix77ubwul8WnvAojCt8oKcBjqqAbUuga7WlnDbaiM370oOD9WjaWRhhY%2BdsPGONab%2FhkSTMVmEQWk6MJycF5Xz82ad%2BhCEU3k%2FPvdJDUy3IODVhfUACSQKUMwLLZUO%2FQbLIW2WGDyaWxh45brO7Lrmw0%2BltMZgPKOsbydk2I2Hmdwx%2Bi6Fi%2FEJ8xw1cUuxnN1hvJhN8MbMGJ08FYhmgQ%2B7eX0BK%2FSN3W2Yf1Z0z3RpB4EZYksaN1xW8P8Rm8f2&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221125T135532Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY7K43AS7V%2F20221125%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8c035dfdf2564023776054990ff28b4847fe04c4ed5a994dde23c527cbf2063a&hash=81d6bc119090610a45419d033c87de9f4cb2b93713f5cddf3659a379db625870&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0004370203000158&tid=spdf-588e879f-6d1e-40a5-9540-50758ec7b35a&sid=9109dace770f5647b7390367bbe152f21328gxrqb&type=client&ua=515901050057050c0401&rr=76fae1ecea430536"> BOB: Improved winner determination in combinatorial auctions and generalizations </a>by Tuomas Sandholm, Subhash Suri. Artificial Intelligence, 2003. <a href="link">  </a> </summary> Combinatorial auctions can be used to reach efficient resource and task allocations in multiagent systems where the items are complementary or substitutable. Determining the winners is NP-complete and inapproximable, but it was recently shown that optimal search algorithms do very well on average. This paper presents a more sophisticated search algorithm for optimal (and anytime) winner determination, including structural improvements that reduce search tree size, faster data structures, and optimizations at search nodes based on driving toward, identifying and solving tractable special cases. We also uncover a more general tractable special case, and design algorithms for solving it as well as for solving known tractable special cases substantially faster. We generalize combinatorial auctions to multiple units of each item, to reserve prices on singletons as well as combinations, and to combinatorial exchanges. All of these generalizations support both complementarity and substitutability of the items. Finally, we present algorithms for determining the winners in these generalizations <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/258568/1/amec03-shaheen.pdf"> Comparing Equilibria for Game-Theoretic and Evolutionary Bargaining Models </a>by Shaheen Fatima, Michael Wooldridge, Nicholas R. Jennings. AMEC, 2003. <a href="link">  </a> </summary> Game-theoretic models of bargaining are typically based on the assumption that players have perfect rationality and that they always play an equilibrium strategy. In contrast, research in experimental economics shows that in bargaining between human subjects, participants do not always play the equilibrium strategy. Such agents are said to be boundedly rational. In playing a game against a boundedly rational opponent, a player’s most effective strategy is not the equilibrium strategy, but the one that is the best reply to the opponent’s actual strategy. Against this background, this paper studies the bargaining behavior of boundedly rational agents by using genetic algorithms. Since bargaining involves players with different utility functions, we have two subpopulations – one represents the buyer, and the other represents the seller (i.e., the population is asymmetric). We study the competitive co-evolution of strategies in the two subpopulations for an incomplete information setting, and compare the results with those prescribed by game theory. Our analysis leads to two main conclusions. Firstly, our study shows that although each agent in the game-theoretic model has a strategy that is dominant at every period at which it makes a move, the stable state of the evolutionary model does not always match the game-theoretic equilibrium outcome. Secondly, as the players mutually adapt to each other’s strategy, the stable outcome depends on the initial population <br> - </details>

<details> <summary> <a href="https://www.sciencedirect.com/science/article/pii/S0004370203000419/pdf?crasolve=1&r=76f041591e3d2124&ts=1669273097150&rtype=https&vrr=UKN&redir=UKN&redir_fr=UKN&redir_arc=UKN&vhash=UKN&host=d3d3LnNjaWVuY2VkaXJlY3QuY29t&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&iv=742ebc324f7984dca1c1f6de0cf4a665&token=62646465663638356136353730393536643266623265343633336439643732323833613663623764353764643062636363383434303733323165383133613666346236383034646231666337633131643464313334363834383939623a623961643531653033613038666332653237333533346436&text=0bf166264e06189f27d06630256a1c2218ed109869cfd04cdd7b694f679a1287661a3053a5dc6c2e3e962d6cc5e7d241af10350003b03173d40bb11d07ff26b5dbc280e72cfd625493395b6015d5b5960aef862798df7827df3586b4c57b018f077057472eb1d10a023fc01fbb1b2f1def6c6823b9c802a46361b12e75ffd215e7114522d13622b262cbbb25eff773da04dbcceaff497663b6bc3280c76b93b2f4b8d8caa1b9298047813637fc08246717f7cdd9b57d6aa305647e891557004d2261c623e64078ada293c7502d9b3308d488047cd87ae0454aae5501f294ee713b0c8291db3c5694ba5c068f0a0d4346c68503cedba06b7867b8111b52c2ca6eb2ac434d4333c1471c5ac85f26a0222a65e3a3a85d99783852cb8224d2ad29dfbe0609a3a97813306531270891016352&original=3f6d64353d3661363231646162316230373731366264323864663530636261623139646336267069643d312d73322e302d53303030343337303230333030303431392d6d61696e2e706466265f76616c636b3d31"> A fuzzy constraint based model for bilateral, multi-issue negotiations in semi-competitive environments </a>by Xudong Luo, Nicholas R. Jennings, Nigel Shadbolt, Ho-fung Leung, Jimmy Ho-man Lee. Artificial Intelligence, 2003. <a href="link">  </a> </summary> This paper develops a fuzzy constraint based model for bilateral multi-issue negotiation in trading environments. In particular, we are concerned with the principled negotiation approach in which agents seek to strike a fair deal for both parties, but which, nevertheless, maximises their own payoff. Thus, there are elements of both competition and cooperation in the negotiation (hence semicompetitive environments). One of the key intuitions of the approach is that there is often more than one option that can satisfy the interests of both parties. So, if the opponent cannot accept an offer then the proponent should endeavour to find an alternative that is equally acceptable to it, but more acceptable to the opponent. That is, the agent should make a trade-off. Only if such a trade-off is not possible should the agent make a concession. Against this background, our model ensures the agents reach a deal that is fair (Pareto-optimal) for both parties if such a solution exists.Moreover, this is achieved by minimising the amount of private information that is revealed. The model uses prioritised fuzzy constraints to represent trade-offs between the different possible values of the negotiation issues and to indicate how concessions should be made when they are necessary. Also by using constraints to express negotiation proposals, the model can cover the negotiation space more efficiently since each exchange covers a region rather than a single point (which is what most existing models deal with). In addition, by incorporating the notion of a reward into our negotiation model, the agents can sometimes reach agreements that would not otherwise be possible. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/258572/1/pat-toit.pdf"> Developing a Bidding Agent for Multiple Heterogeneous Auctions </a>by P Anthony, NR Jennings. ACM TOIT, 2003. <a href="link">  </a> </summary> Due to the proliferation of online auctions, there is an increasing need to monitor and bid in multiple auctions in order to procure the best deal for the desired good. To this end, this paper reports on the development of a heuristic decision making framework that an autonomous agent can exploit to tackle the problem of bidding across multiple auctions with varying start and end times and with varying protocols (including English, Dutch and Vickrey). The framework is flexible, configurable, and enables the agent to adopt varying tactics and strategies that attempt to ensure that the desired item is delivered in a manner consistent with the user’s preferences. Given this large space of possibilities, we employ a genetic algorithm to search (offline) for effective strategies in common classes of environment. The strategies that emerge from this evolution are then codified into the agent’s reasoning behaviour so that it can select the most appropriate strategy to employ in its prevailing circumstances. The proposed framework has been implemented in a simulated marketplace environment and its effectiveness has been empirically demonstrated. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/a:1024592607487.pdf"> Multi-Issue Negotiation Processes by Evolutionary Simulation, Validation and Social Extensions </a>by ENRICO GERDING, DAVID VAN BRAGT and HAN LA POUTRE. Computational Economics, 2003. <a href="link">  </a> </summary> We describe a system for bilateral negotiations in which artificial agents are generated by an evolutionary algorithm (EA). The negotiations are governed by a finite-horizon version of the alternating-offers protocol. Several issues are negotiated simulataneously. We first analyse and validate the outcomes of the evolutionary system, using the game-theoretic subgame-perfect equilibrium as a benchmark. We then present two extensions of the negotiation model. In the first extension agents take into account the fairness of the obtained payoff. We find that when the fairness norm is consistently applied during the negotiation, agents reach symmetric outcomes which are robust and rather insensitive to the actual fairness settings. In the second extension we model a competitive market situation where agents have multiple bargaining opportunities before reaching the final agreement. Symmetric outcomes are now also obtained, even when the number of bargaining opportunities is small. We furthermore study the influence of search or negotiation costs in this game <br> - </details>

<details> <summary> <a href="http://shiftleft.com/mirrors/www.hpl.hp.com/techreports/2002/HPL-2002-321.pdf"> Applying Evolutionary Game Theory to Auction Mechanism Design </a>by Andrew Byde. ACM-EC, 2003. <a href="link">  </a> </summary> In this paper we describe an evolution-based method for evaluating auction mechanisms, and apply it to a space of mechanisms including the standard first- and second-price sealed bid auctions. We replicate results known already in the Auction Theory literature regarding the suitability of different mechanisms for different bidder environments, and extend the literature by establishing the superiority of novel mechanisms over standard mechanisms, for commonly occurring scenarios. Thus this paper simultaneously extends Auction Theory, and provides a systematic method for further such extensions. <br> - </details>

<details> <summary> <a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=1046&context=sis_research"> Walverine: A walrasian trading agent </a>by Shih-Fen Cheng, Evan Leung, Kevin M. Lochner, Kevin O’Malley, Daniel M. Reeves,Julian L. Schvartzman, Michael P. Wellman. AAMAS, 2003. <a href="link">  </a> </summary> TAC-02 was the third in a series of Trading Agent Competition events fostering research in automating trading strategies by showcasing alternate approaches in an open-invitation market game. TAC presents a challenging travel-shopping scenario where agents must satisfy client preferences for complementary and substitutable goods by interacting through a variety of market types. Michigan’s entry, Walverine, bases its decisions on a competitive (Walrasian) analysis of the TAC travel economy. Using this Walrasian model, we construct a decision-theoretic formulation of the optimal bidding problem, which Walverine solves in each round of bidding for each good. Walverine’s optimal bidding approach, as well as several other features of its overall strategy, are potentially applicable in a broad class of trading environments. <br> - </details>

<details> <summary> <a href="http://iebi.nctu.edu.tw/course_old/9515492/Final_Report/9534530/9534530-3.pdf"> The Importance of Ordering in Sequential Auctions </a>by Wedad Elmaghraby. Management Science, 2003. <a href="link">  </a> </summary> To date, the largest part of literature on multi-unit auctions has assumed that there are k homogeneous objects being auctioned, where each bidder wishes to win exactly one or all of k units. These modeling assumptions have made the examination of ordering in sequential auctions inconsequential. The aim of this paper is to introduce and highlight the critical influence that ordering can have on the efficiency of an auction. We study a buyer who outsources via sequential 2nd-price auctions two heterogeneous jobs, and faces a diverse set of suppliers with capacity constraints <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-642-17045-4_14.pdf"> Bargaining with Posterior Opportunities: An Evolutionary Social Simulation </a>by E. H. Gerding, J.A. La Poutre. LNEMS, 2003. <a href="link">  </a> </summary> Negotiations have been extensively studied theoretically throughout the years. A well-known bilateral approach is the ultimatum game, where two agents negotiate on how to split a pie or a "dollar": the proposer makes an offer and responder can choose to accept or reject. In this paper a natural extension of the ultimatum game is presented, in which both agents can negotiate with other opponents in case of a disagreement. This way the basics of a compe titive market are modelled where for instance a buyer can try several sellers before making a purchase decision. The game is investigated using an evolutionary simulation. The outcomes appear to depend largely on the information available to the agents. We find that if the agents' number of future bargaining opportunities is commonly known, the proposer has the advantage. Ifthis information is held private, however, the responder can obtain a larger share of the pie. For the first case we also provide a game-theoretic analysis and compare the outcome with evolutionary results. Furthermore, the effects of search costs and allowing multiple issues to be negotiated simultaneously are investigated. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/258564/1/minghua-toit.pdf"> SouthamptonTAC: An Adaptive Autonomous
Trading Agent </a>by MINGHUA HE, NICHOLAS R. JENNINGS. ACM Transactions on Internet Technology, 2003. <a href="link">  </a> </summary> Software agents are increasingly being used to represent humans in on-line auctions. Such agents have the advantages of being able to systematically monitor a wide variety of auctions and then make rapid decisions about what bids to place in what auctions. They can do this continuously and repetitively without losing concentration. Moreover, in complex multiple auction settings, agents may need to modify their behavior in one auction depending on what is happening in another. To provide a means of evaluating and comparing (benchmarking) research methods in this area, the Trading Agent Competition (TAC) was established. This competition involves a number of agents bidding against one another in a number of related auctions (operating different protocols) to purchase travel packages for customers. Against this background, this artcle describes the design, implementation and evaluation of our adaptive autonomous trading agent, SouthamptonTAC, one of the most successful participants in TAC 2002 <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/David-Bragt/publication/225315578_Why_Agents_for_Automated_Negotiations_Should_Be_Adaptive/links/0c960525bab1154003000000/Why-Agents-for-Automated-Negotiations-Should-Be-Adaptive.pdf"> Why agents for automated negotiations should be adaptive </a>by D.D.B. van Bragt, J.A. La Poutre. Netnomics, 2003. <a href="link">  </a> </summary> We show that adaptive agents on the Internet can learn to exploit bidding agents who use a (limited) number of fixed strategies. These learning agents can be generated by adapting a special kind of finite automata with evolutionary algorithms (EAs). Our approach is especially powerful if the adaptive agent participates in frequently occurring micro-transactions, where there is sufficient opportunity for the agent to learn online from past negotiations. More in general, results presented in this paper provide a solid basis for the further development of adaptive agents for Internet applications. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/cs/0412106.pdf"> Online Learning of Aggregate Knowledge about Non-linear Preferences Applied to Negotiating Prices and Bundles </a>by D.J.A. Somefun, T.B. Klos, J.A. La Poutre. ICEC, 2004. <a href="link">  </a> </summary> In this paper, we consider a form of multi-issue negotiation where a shop negotiates both the contents and the price of bundles of goods with his customers. We present some key insights about, as well as a procedure for, locating mutually beneficial alternatives to the bundle currently under negotiation. The essence of our approach lies in combining aggregate (anonymous) knowledge of customer preferences with current data about the ongoing negotiation process. The developed procedure either works with already obtained aggregate knowledge or, in the absence of such knowledge, learns the relevant information online. We conduct computer experiments with simulated customers that have nonlinear preferences. We show how, for various types of customers, with distinct negotiation heuristics, our procedure (with and without the necessary aggregate knowledge) increases the speed with which deals are reached, as well as the number and the Pareto efficiency of the deals reached compared to a benchmark. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/ICAPS/2004/ICAPS04-030.pdf"> Price Prediction Strategies for Market-Based Scheduling </a>by Jeffrey K. MacKie-Mason, Anna Osepayshvili, Daniel M. Reeves, Michael P. Wellman. AAAI, 2004. <a href="link">  </a> </summary> In a market-based scheduling mechanism, the allocation of time-specific resources to tasks is governed by a competitive bidding process. Agents bidding for multiple, separately allocated time slots face the risk that they will succeed in obtaining only part of their requirement, incurring expenses for potentially worthless slots. We investigate the use of price prediction strategies to manage such risk. Given an uncertain price forecast, agents follow simple rules for choosing whether and on which time slots to bid. We find that employing price predictions can indeed improve performance over a straightforward baseline in some settings. Using an empirical game-theoretic methodology, we establish Nash equilibrium profiles for restricted strategy sets. This allows us to confirm the stability of price-predicting strategies, and measure overall efficiency. We further experiment with variant strategies to analyze the source of prediction’s power, demonstrate the existence of self-confirming predictions, and compare the performance of alternative prediction methods. <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1460738&casa_token=wKwgrRN8AGQAAAAA:i8kMBXhVePv1z3n_64vnE1I0aBe0-bFVpB6zgbbIVpmQCsWoc1YVc0u1xfLHlnG_x0cJHHX36FP-3KA"> An Efficient Turnkey Agent for Repeated Trading with Overall Budget and Preferences </a>by I.B. Vermeulen, D.J.A. Somefun, J.A. La Poutre. Cybernetics and Intelligent Systems, 2004. <a href="link">  </a> </summary> For various e-commerce applications autonomous agents can do the actual trading on behalf of their users. We consider an agent whu trades rcpentcdly on hchalf of his uscr, given an overall budget and prcfcrcnccs per time stcp, both specified at thc start. For many e-commerce settings such an agent has limited computationaI resources, limited prior information concerning price fluctuations, and little time for online learning. We therefore develop an efficient heuristic that requires little prior information to work well from the start, even for very roughed nonsmooth problem instanccs. Extensive computer experiments conducted for a wide variety of customer preferences show virtually no difference in performance betwcen a dynamic prugramming (IW) approach and the developed heuristic carrying out the agent’s task. The DP approach ha$, however, thc important drawback of generally being too computationally intensive. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/263434/1/marketACM_draft.pdf"> Market-based Recommendation: Agents that Compete for Consumer Attention </a>by SANDER M. BOHTE, ENRICO GERDING, HAN LA POUTRE. ACM TOIT, 2004. <a href="link">  </a> </summary> The amount of attention space available for recommending suppliers to consumers on e-commerce sites is typically limited. We present a competitive distributed recommendation mechanism based on adaptive software agents for efficiently allocating the “consumer attention space”, or banners. In the example of an electronic shopping mall, the task is delegated to the individual shops, each of which evaluates the information that is available about the consumer and his or her interests (e.g. keywords, product queries, and available parts of a profile). Shops make a monetary bid in an auction where a limited amount of “consumer attention space” for the arriving consumer is sold. Each shop is represented by a software agent that bids for each consumer. This allows shops to rapidly adapt their bidding strategy to focus on consumers interested in their offerings. For various basic and simple models for on-line consumers, shops, and profiles, we demonstrate the feasibility of our system by evolutionary simulations as in the field of agent-based computational economics (ACE). We also develop adaptive software agents that learn bidding-strategies, based on neural networks and strategy exploration heuristics. Furthermore, we address the commercial and technological advantages of this distributed market-based approach. The mechanism we describe is not limited to the example of the electronic shopping mall, but can easily be extended to other domains. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/1207.4108"> Bidding under Uncertainty: Theory and Experiments </a>by Amy Greenwald, Justin Boyan. UAI, 2004. <a href="link">  </a> </summary> This paper describes a study of agent bidding strategies, assuming combinatorial valuations for complementary and substitutable goods, in three auction environments: sequential auctions, simultaneous auctions, and the Trading Agent Competition (TAC) Classic hotel auction design, a hybrid of sequential and simultaneous auctions. The problem of bidding in sequential auctions is formulated as an MDP, and it is argued that expected marginal utility bidding is the optimal bidding policy. The problem of bidding in simultaneous auctions is formulated as a stochastic program, and it is shown by example that marginal utility bidding is not an optimal bidding policy, even in deterministic settings. Two alternative methods of approximating a solution to this stochastic program are presented: the first method, which relies on expected values, is optimal in deterministic environments; the second method, which samples the nondeterministic environment, is asymptotically optimal as the number of samples tends to infinity. Finally, experiments with these various bidding policies are described in the TAC Classic setting. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/265624/1/autombarg2004a.pdf"> Automated Bilateral Bargaining about Multiple Attributes in a One-to-Many Setting </a>by E.H. Gerding, D.J.A. Somefun, J.A. La Poutre. ICEC, 2004. <a href="link">  </a> </summary> Negotiations are an important way of reaching agreements between selfish autonomous agents. In this paper we focus on one-to-many bargaining within the context of agentmediated electronic commerce. We consider an approach where a seller agent negotiates over multiple interdependent attributes with many buyer agents in a bilateral fashion. In this setting, “fairness,” which corresponds to the notion of envy-freeness in auctions, may be an important business constraint. For the case of virtually unlimited supply (such as information goods), we present a number of one-to-many bargaining strategies for the seller agent, which take into account the fairness constraint, and consider multiple attributes simultaneously. We compare the performance of the bargaining strategies using an evolutionary simulation, especially for the case of impatient buyers. Several of the developed strategies are able to extract almost all the surplus; they utilize the fact that the setting is one-to-many, even though bargaining is bilateral. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/259560/1/aamas04duong.pdf"> Coordinating multiple concurrent negotiations </a>by Thuc Duong Nguyen, Nicholas R. Jennings. AAMAS, 2004. <a href="link">  </a> </summary> To secure good deals, an agent may engage in multiple concurrent negotiations for a particular good or service. However for this to be effective, the agent needs to carefully coordinate its negotiations. At a basic level, such coordination should ensure the agent does not procure more of the good than is needed. But to really derive benefit from such an approach, the agent needs the concurrent encounters to mutually influence one another (e.g. a good price with one opponent should enable an agent to negotiate more strongly in the other interactions). To this end, this paper presents a novel heuristic model for coordinating multiple bilateral negotiations. The model is empirically evaluated and shown to be effective and robust in a range of negotiation scenarios. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/B:AGNT.0000038029.82331.c0.pdf"> Agent-Mediated Electronic Commerce </a>by CARLES SIERRA. AAMAS, 2004. <a href="link">  </a> </summary> Electronic commerce has been one of the traditional arenas for agent technology. The complexity of these applications has been a challenge for researchers that have developed methodologies, products and systems, having in mind the specificities of trade, the interaction particularities of commerce, the strict notion of commitment and contract, and the clearly shaped conventions and norms that structure the field. In this paper I survey some key areas for agent technology which, although general, are of special importance in electronic commerce, namely, solid development methodologies, negotiation technologies and trust-building mechanisms. I give examples of systems in which I have directly participated, although I also try to refer to the work of other AgentLink Special Interest Group members over the last few years <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/265630/1/bilateralbarga.pdf"> Bilateral Bargaining in a One-to-Many Bargaining Setting </a>by Enrico Gerding, Koye Somefun, Han La Poutre. AAMAS, 2004. <a href="link">  </a> </summary> Electronic markets are becoming increasingly transparent with low search cost, strong price competition, and low margins. Automated negotiation enables a business to go beyond price competition. Through the use of autonomous agents, which negotiate on behalf of their owners, a business can obtain flexibility in prices and goods, distinguish between groups of buyers based on their preferences, and even personalize complex goods according to the demands of individual buyers without significantly increasing transaction costs. We focus here on one-to-many bargaining, where a seller agent negotiates, on behave of a seller, with many buyer agents individually in a bilateral fashion. In many cases, auctions can be used to effectively organize one-to-many bargaining. For various situations, however, auctions may not be the preferred protocol for bargainers. In situations of, for example, flexible or virtually unlimited supply, multiple issues, and/or continuous sale the appropriate auction protocol becomes, at best, much more complex. Consequently, businesses may opt for the intuitive and flexible bilateral bargaining protocol, where the seller agent negotiates bilaterally with one or more buyers simultaneously by exchanging offers and counter offers. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/265638/1/neg_bundle.pdf"> Automated Negotiation and Bundling of Information Goods </a>by Koye Somefun, Enrico Gerding, Sander Bohte, Han La Poutre. LNAI, 2004. <a href="link">  </a> </summary> In this paper, we present a novel system for selling bundles of news items. Through the system, customers bargain with the seller over the price and quality of the delivered goods. The advantage of the developed system is that it allows for a high degree of flexibility in the price, quality, and content of the offered bundles. The price, quality, and content of the delivered goods may, for example, differ based on daily dynamics and personal interest of customers. Autonomous “software agents” execute the negotiation on behalf of the users of the system. To perform the actual negotiation these agents make use of bargaining strategies. We present the novel approach of decomposing bargaining strategies into concession strategies and Pareto efficient search strategies. Additionally, we introduce the orthogonal and orthogonal-DF strategy: two Pareto search strategies. We show through computer experiments that the use of these Pareto search strategies will result in very efficient bargaining outcomes. Moreover, the system is setup such that it is actually in the best interest of the customer to have their agent adhere to this approach of disentangling the bargaining strategy. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/1207.1400.pdf"> Self-Confirming Price Prediction for Bidding in Simultaneous Ascending Auctions </a>by Anna Osepayshvili, Michael P. Wellman, Daniel M. Reeves, Jeffrey K. MacKie-Mason. UAI, 2005. <a href="link">  </a> </summary> Simultaneous ascending auctions present agents with the exposure problem: bidding to acquire a bundle risks the possibility of obtaining an undesired subset of the goods. Auction theory provides little guidance for dealing with this problem. We present a new family of decision-theoretic bidding strategies that use probabilistic predictions of final prices. We focus on selfconfirming price distribution predictions, which by definition turn out to be correct when all agents bid decision-theoretically based on them. Bidding based on these is provably not optimal in general, but our experimental evidence indicates the strategy can be quite effective compared to other known methods. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/268206/1/prima2005.pdf"> Learning the Structure of Utility Graphs Used in Multi-Issue Negotiation through Collaborative Filtering </a>by Valentin Robu, Han La Poutre. PRIMA, 2005. <a href="link">  </a> </summary> Graphical utility models represent powerful formalisms for modeling complex agent decisions involving multiple issues. In the context of negotiation, it has been shown that using utility graphs enables reaching Paretoefficient agreements with a limited number of negotiation steps, even for highdimensional negotiations involving complex complementarity/ substitutability dependencies between multiple issues. This paper considerably extends the results of Valentin et al., (2005), by proposing a method for constructing the utility graphs of buyers automatically, based on previous negotiation data. Our method is based on techniques inspired from item-based collaborative filtering, used in online recommendation algorithms. Experimental results show that our approach is able to retrieve the structure of utility graphs online, with a high degree of accuracy, even for highly non-linear settings and even if a relatively small amount of data about concluded negotiations is available. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Valentin-Robu/publication/221456988_Modeling_complex_multi-issue_negotiations_using_utility_graphs/links/004635267b150953b9000000/Modeling-complex-multi-issue-negotiations-using-utility-graphs.pdf"> Modeling Complex Multi-Issue Negotiations
Using Utility Graphs </a>by Valentin Robu, D.J.A. Somefun, J.A. La Poutre. AAMAS, 2005. <a href="link">  </a> </summary> This paper presents an agent strategy for complex bilateral negotiations over many issues with inter-dependent valuations. We use ideas inspired by graph theory and probabilistic influence networks to derive efficient heuristics for negotiations about multiple issues. Experimental results show — under relatively weak assumptions with respect to the structure of the utility functions – that the developed approach leads to Pareto-efficient outcomes. Moreover, Pareto-efficiency can be reached with few negotiation steps, because we explicitly model and utilize the underlying graphical structure of complex utility functions. Consequently, our approach is applicable to domains where reaching an efficient outcome in a limited amount of time is important. Furthermore, unlike other solutions for highdimensional negotiations, the proposed approach does not require a mediator. <br> - </details>

<details> <summary> <a href="https://deepblue.lib.umich.edu/bitstream/handle/2027.42/50434/proof-dexter-dss.pdf?sequence=1"> Exploring bidding strategies for market-based scheduling </a>by Daniel M. Reeves*, Michael P. Wellman, Jeffrey K. MacKie-Mason, Anna Osepayshvili. Decision Support Systems, 2005. <a href="link">  </a> </summary> A market-based scheduling mechanism allocates resources indexed by time to alternative uses based on the bids of participating agents. Agents are typically interested in multiple time slots of the schedulable resource, with value determined by the earliest deadline by which they can complete their corresponding tasks. Despite the strong complementarities among slots induced by such preferences, it is often infeasible to deploy a mechanism that coordinates allocation across all time slots. We explore the case of separate, simultaneous markets for individual time slots, and the strategic problem it poses for bidding agents. Investigation of the straightforward bidding policy and its variants indicates that the efficacy of particular strategies depends critically on preferences and strategies of other agents, and that the strategy space is far too complex to yield to general game-theoretic analysis. For particular environments, however, it is often possible to derive constrained equilibria through evolutionary search methods <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/263435/1/fulltext.pdf"> Efficient Methods for Automated Multi-Issue Negotiation: Negotiating over a Two-Part Tariff </a>by D.J.A. Somefun, E.H. Gerding, J.A. La Poutre. International Journal of Intelligent Systems, 2006. <a href="link">  </a> </summary> In this article, we consider the novel approach of a seller and customer negotiating bilaterally about a two-part tariff, using autonomous software agents. An advantage of this approach is that win–win opportunities can be generated while keeping the problem of preference elicitation as simple as possible. We develop bargaining strategies that software agents can use to conduct the actual bilateral negotiation on behalf of their owners. We present a decomposition of bargaining strategies into concession strategies and Pareto-efficient-search methods: Concession and Paretosearch strategies focus on the conceding and win–win aspect of bargaining, respectively. An important technical contribution of this article lies in the development of two Pareto-search methods. Computer experiments show, for various concession strategies, that the respective use of these two Pareto-search methods by the two negotiators results in very efficient bargaining outcomes while negotiators concede the amount specified by their concession strategy <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/264473/1/toit-06.pdf"> A Heuristic Bidding Strategy for Buying Multiple Goods in Multiple English Auctions </a>by MINGHUA HE, NICHOLAS R. JENNINGS, ADAM PRUGEL-BENNETT. ACM Transactions on Internet Technology, 2006. <a href="link">  </a> </summary> This paper presents the design, implementation, and evaluation of a novel bidding algorithm that a software agent can use to obtain multiple goods from multiple overlapping English auctions. Specifically, an Earliest Closest First heuristic algorithm is proposed that uses neurofuzzy techniques to predict the expected closing prices of the auctions and to adapt the agent’s bidding strategy to reflect the type of environment in which it is situated. This algorithm first identifies the set of auctions that are most likely to give the agent the best return and then, according to its attitude to risk, it bids in some other auctions that have approximately similar expected returns, but which finish earlier than those in the best return set. We show through empirical evaluation against a number of methods proposed in the multiple auction literature that our bidding strategy performs effectively and robustly in a wide range of scenarios. <br> - </details>

<br/>

### Bounded Rationality

<details> <summary> <a href="https://dl.acm.org/doi/pdf/10.1145/195058.195445"> On complexity as bounded rationality  </a>by C.H. Papadimitriou, M. Yannakakis. STOC, 1994. <a href="link">  </a> </summary> It has been hoped that computational approaches can help resolve some well-known paradoxes in game theory. We prove that tf the repeated prisoner’s dilemma M played by finite automata with less than exponentially (in the number of rounds) many states, then cooperation can be achieved an equilibrium (while with exponentially many states, defection is the only equilibrium). We furthermore prove a generalization to arbitrary games and Pareto optimal points. Finally, we present a general model of polynomially computable games, and characterize in terms of fami!iar complexity classes ranging from NP to NEXP the natural problems that arise in relation with such games. <br> - </details>

<details> <summary> <a href="http://sfi-edu.s3.amazonaws.com/sfi-edu/production/uploads/sfi-com/dev/uploads/filer/eb/8d/eb8d7d4d-8bc0-49ac-8feb-e73c5a141a12/94-03-014.pdf"> Inductive Reasoning, Bounded Rationality and the Bar Problem </a>by W. Brian Arthur. American Economic Review, 1994. <a href="link">  </a> </summary> This paper draws on modem psychology to argue that as humans, in economic decision contexts that are complicated or ill-defined, we use not deductive, but inductive reasoning. That is, in such contexts we induce a variety of working hypotheses or mental models, act upon the most credible, and replace hypotheses with new ones if they cease to work. Inductive reasoning leads to a rich psychological world in which an agent's hypotheses or mental models compete for survival against each other, in an environment formed by other other agents' hypotheses or mental models-a world that is both evolutionary and complex. Inductive reasoning can be modeled in a variety of ways. The main body of the paper introduces and models a coordination problem-"the bar problem"-in which agents' expectations are forced to be subjective and to differ. It shows that while agents' beliefs never settle down, collectively they form an "ecology" that does converge to an equilibrium pattern. <br> - </details>

<details> <summary> <a href="https://mitpress.mit.edu/9780262681001/modeling-bounded-rationality/"> Modeling Bounded Rationality </a>by Ariel Rubinstein. MIT Press,
1998. <a href="link">  </a> </summary> The notion of bounded rationality was initiated in the 1950s by Herbert Simon; only recently has it influenced mainstream economics. In this book, Ariel Rubinstein defines models of bounded rationality as those in which elements of the process of choice are explicitly embedded. The book focuses on the challenges of modeling bounded rationality, rather than on substantial economic implications. In the first part of the book, the author considers the modeling of choice. After discussing some psychological findings, he proceeds to the modeling of procedural rationality, knowledge, memory, the choice of what to know, and group decisions.In the second part, he discusses the fundamental difficulties of modeling bounded rationality in games. He begins with the modeling of a game with procedural rational players and then surveys repeated games with complexity considerations. He ends with a discussion of computability constraints in games. The final chapter includes a critique by Herbert Simon of the author's methodology and the author's response. <br> - </details>

<details> <summary> <a href="https://www.pnas.org/doi/pdf/10.1073/pnas.211534898"> On the impossibility of predicting the behavior of rational agents </a>by Dean P. Foster, H. Peyton Young. PNAS, 2001. <a href="link">  </a> </summary> A foundational assumption in economics is that people are rational: they choose optimal plans of action given their predictions about future states of the world. In games of strategy this means that each player’s strategy should be optimal given his or her prediction of the opponents’ strategies. We demonstrate that there is an inherent tension between rationality and prediction when players are uncertain about their opponents’ payoff functions. Specifically, there are games in which it is impossible for perfectly rational players to learn to predict the future behavior of their opponents (even approximately) no matter what learning rule they use. The reason is that in trying to predict the next-period behavior of an opponent, a rational player must take an action this period that the opponent can observe. This observation may cause the opponent to alter his next-period behavior, thus invalidating the first player’s prediction. The resulting feedback loop has the property that, a positive fraction of the time, the predicted probability of some action next period differs substantially from the actual probability with which the action is going to occur. We conclude that there are strategic situations in which it is impossible in principle for perfectly rational agents to learn to predict the future behavior of other perfectly rational agents based solely on their observed actions. <br> - </details>

<br/>

### Collective Intelligence

<details> <summary> <a href="https://proceedings.neurips.cc/paper/1998/file/5129a5ddcd0dcd755232baa04c231698-Paper.pdf"> USING COLLECTIVE INTELLIGENCE TO ROUTE INTERNET TRAFFIC </a>by David H. Wolpert, Kagan Turner, Jeremy Frank. NeurIPS, 1998. <a href="link">  </a> </summary> A COllective INtelligence (COIN) is a set of interacting reinforcement learning (RL) algorithms designed in an automated fashion so that their collective behavior optimizes a global utility function. We summarize the theory of COINs, then present experiments using that theory to design COINs to control internet traffic routing. These experiments indicate that COINs outperform all previously investigated RL-based, shortest path routing algorithms.  <br> - </details>

<details> <summary> <a href="link"> GENERAL PRINCIPLES OF LEARNING-BASED MULTI-AGENT SYSTEMS </a>by David H. Wolpert, Kevin R. Wheeler, Kagan Turner. International Conference on Autonomous Agents, 1999. <a href="link">  </a> </summary> We consider the problem of how to design large decentralized multi-agent systems (MAS’s) in an automated fashion, with little or no hand-tuning. Our approach has each agent run a reinforcement learning algorithm. This converts the problem into one of how to automatically set/update the reward functions for each of the agents so that the global goal is achieved. In particular we don not want the agents to “work at cross-purposes” as far as the global goal is concerned. We use the term artificial COllective INtelligence (COIN) to refer to systems that embody solutions to this problem. In this paper we present a summary of a mathematical framework for COINS. We then investigate the real-world applicability of the core concepts of that framework via two computer experiments: we show that our COINS perform near optimally in a difficult variant of Arthur’s bar problem [l] (and in psrtitular avoid the tragedy of the commons for that problem), and we also illustrate optimal performance for our COINS in the leader-follower problem.  <br> - </details>

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/wolpert01a.pdf"> Optimal Payoff Functions for Members of Collectives </a>by David H. Wolpert, Kagan Tumer. Advances in Complex Systems, 2001. <a href="link">  </a> </summary> We consider the problem of designing (perhaps massively distributed) collectives of computational processes to maximize a provided "world utility" function. We consider this problem when the behavior of each process in the collective can be cast as striving to maximize its own payoff utility function. For such cases the central design issue is how to initialize/update those payoff utility functions of the individual processes so as to induce behavior of the entire collective having good values of the world utility. Traditional "team game" approaches to this problem simply assign to each process the world utility as its payoff utility function. In previous work we used the "Collective Intelligence" (COIN) framework to derive a better choice of payoff utility functions, one that results in world utility performance up to orders of magnitude superior to that ensuing from the use of the team game utility. In this paper, we extend these results using a novel mathematical framework. Under that new framework we review the derivation of the general class of payoff utility functions that both (i) are easy for the individual processes to try to maximize, and (ii) have the property that if good values of them are achieved, then we are assured a high value of world utility. These are the "Aristocrat Utility" and a new variant of the "Wonderful Life Utility" that was introduced in the previous COIN work. We demonstrate experimentally that using these new utility functions can result in significantly improved performance over that of previously investigated COIN payoff utilities, over and above those previous utilities' superiority to the conventional team game utility. These results also illustrate the substantial superiority of these payoff functions to perhaps the most natural version of the economics technique of "endogenizing externalities." <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/3-540-45656-2_30.pdf"> Phe-Q: A Pheromone Based Q-Learning </a>by Ndedi Monekosso, Paolo Remagnino. LNAI, 2001. <a href="link">  </a> </summary> Biological systems have often provided inspiration for the design of artificial systems. On such example of a natural system that has inspired researchers is the ant colony. In this paper an algorithm for multi-agent reinforcement learning, a modified Q-learning, is proposed. The algorithm is inspired by the natural behaviour of ants, which deposit pheromones in the environment to communicate. The benefit besides simulating ant behaviour in a colony is to design complex multiagent systems. Complex behaviour can emerge from relatively simple interacting agents. The proposed Q-learning update equation includes a belief factor. The belief factor reflects the confidence the agent has in the pheromone detected in its environment. Agents communicate implicitly to co-operate in learning to solve a path-planning problem. The results indicate that combining synthetic pheromone with standard Q-learning speeds up the learning process. It will be shown that the agents can be biased towards a preferred solution by adjusting the pheromone deposit and evaporation rates. <br> - </details>

<details> <summary> <a href="http://faculty.washington.edu/paymana/swarm/bonabeau03-etcon.pdf"> Swarm Intelligence </a>by Eric Bonabeau. Emergent technology conference, 2003. <a href="link">  </a> </summary> Ants, Bees or Termites - all social insects - show impressive collective problem-solving capabilities. Properties associated with their group behaviour like self-organisation, robustness and flexibility are seen as characteristics that artificial systems for optimisation, control or task execution should exhibit. In the last decade, diverse efforts have been made to take social insects as an example and develop algorithms inspired by their strictly self-organised behaviour. These approaches can be subsumed under the concept of "Swarm Intelligence". <br> - </details>

<details> <summary> <a href="http://nozdr.ru/data/media/biblio/kolxoz/Cs/CsLn/L/Learning%20and%20Adaption%20in%20Multi-Agent%20Systems,%201%20conf.,%20LAMAS%202005(LNCS3898,%20Springer,%202006)(ISBN%203540330534)(224s).pdf#page=215"> Multi-type ACO for Light Path Protection </a>by Peter Vrancx, Ann Nowe, Kris Steenhaut. LAMAS, 2005. <a href="link">  </a> </summary> Backup trees (BTs) are a promising approach to network protection in optical networks. BTs allow us to protect a group of working paths against single network failures, while reserving only a minimum amount of network capacity for backup purposes. The process of constructing a set of working paths together with a backup tree is computationally very expensive, however. In this paper we propose a multi-agent approach based on ant colony optimization (ACO) for solving this problem. ACO algorithms use a set of relatively simple agents that model the behavior of real ants. In our algorithm multiple types of ants are used. Ants of the same type collaborate, but are in competition with the ants of other types. The idea is to let each type find a path in the network that is disjoint with that of other types. We also demonstrate a preliminary version of this algorithm in a series of simple experiments. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-030-77584-1.pdf"> A Primer on Multiple Intelligences </a>by Matthew N. O. Sadiku, Sarhan M. Musa. Springer, 2021. <a href="link">  </a> </summary> The concept of intelligence is interesting and is discussed every day. It has been central to the feld of psychology and remains hotly debated. It was frst introduced by Francis Galton in 1885. Intelligence or cognitive development is a biopsychological potential to process information that can be activated to solve problems. It is the capacity of the individual to act purposefully, to think rationally, and to deal effectively with his environment. The characteristic of intelligence is usually attributed to humans. Traditionally, intelligence is often regarded as a person’s intellectual capacity; something one is born with and that cannot be changed. It is the ability to learn, to proft from experience, the ability to adapt to new situations, and the ability to solve problems. It was generally believed that intelligence was a single entity that was inherited. It is fxed and can be measured. Historically, intelligence has been measured using the IQ test which is a general measure of one’s cognitive function. Other views of intelligence have emerged in recent years. Today, an increasing number of researchers, believe that there exists a multiple of intelligences, quite independent of each other. People have a unique blend of intelligences. The concept of multiple intelligences represents an effort to reframe the traditional conception of intelligence. Professor Howard Gardner at Harvard’s Graduate School of Education argued that there are better or alternative ways to measure intelligence than standard IQ tests. He frst proposed nine intelligences and insisted that all people are born with one or more intelligences. He believed human intelligence was multidimensional. This spectrum of intelligence indicates that people can be smart in a number of different ways. It implies that we are all intelligent in different ways, but there is always a primary, or more dominant, intelligence in each of us. Multiple intelligences theory states that everyone has all intelligences at varying degrees of profciency, while most will experience more dominant intelligences that impact the way they learn and interact with others. Each type of intelligence presents a distinct component of our total competence. There is no clear consensus about how many intelligences there are. In this book, we consider 19 multiple intelligences. The intelligences are possessed by everyone and vary in degree of development within each individual. <br> - </details>

<br/>

### Communication

<details> <summary> <a href="https://www.researchgate.net/profile/Michael-Littman/publication/228786266_Altruism_in_the_evolution_of_communication/links/02e7e52c564728c081000000/Altruism-in-the-evolution-of-communication.pdf"> Altruism in the Evolution of Communication </a>by David H. Ackley, Michael L. Littman. Workshop on the Synthesis and Simulation of Living Systems, 1994. <a href="link">  </a> </summary> Computer models of evolutionary phenomena often assume that the fitness of an individual can be evaluated in isolation, but effective communication requires that individuals interact. Existing models directly reward speakers for improved behavior on the part of the listeners so that, essentially, effective communication is fitness. We present new models in which, even though "speaking truthfully" provides no tangible benefit to the speaker, effective communication nonetheless evolves. A large population is spatially distributed so that "communication range" approximately correlates with "breeding range" so that most of the time "you'll be talking to family" allowing kin selection to encourage the emergence of communication. However, the emergence of altruistic communication also creates niches that can be exploited by "information parasites." The new models display complex and subtle long-tenD dynamics as the global implications of such social dilemmas are played out <br> - </details>

<details> <summary> <a href="https://digital.csic.es/bitstream/10261/127969/1/Spatial%20Vocabulary.pdf"> A self-organizing spatial vocabulary </a>by L. Steels. Artificial Life, 2(3):319–332, 1995. <a href="link">  </a> </summary> Language is a shared set of conventions for mapping meanings to utterances. This paper explores self-organization as the primary mechanism for the formation of a vocabulary. It reports on a computational experiment in which a group of distributed agents develop ways to identify each other using names or spatial descriptions. It is also shown that the proposed mechanism copes with the acquisition of an existing vocabulary by new agents entering the community and with an expansion of the set of meanings. <br> - </details>

<details> <summary> <a href="https://digital.csic.es/bitstream/10261/127973/1/alife96.pdf"> Self-organising vocabularies </a>by L. Steels. In Proceedings of Artificial Life V, 1996. <a href="link">  </a> </summary> The paper investigates a mechanism by which dis-tributed agents spontaneously and autonomously develop a common vocabulary. The vocabulary is open in the sense that new agents and new meaning may be added at any time. Self-organisation plays a critical role for achieving coherence. <br> - </details>

<details> <summary> <a href="https://digital.csic.es/bitstream/10261/128041/1/Emergent%20Adaptive%20Lexicons.pdf"> Emergent adaptive lexicons </a>by L. Steels. In P. Maes, editor, Proceedings of the Simulation of Adaptive Behavior Conference. MIT Press, 1996. <a href="link">  </a> </summary> The paper reports experiments to test the hy­ pothesis that language is an autonomous evolving adaptive system maintained by a group of dis­ tributed agents without central control. The ex­ periments show how a coherent lexicon may spon­ taneously emerge in a group of agents engaged in language games and how a lexicon may adapt to cope with new meanings that arise or new agents that enter the group. The lexicon has several characteristics of natural language lexicons, such as polysemy, synonymy and ambiguity. <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/30820556/steels98h-libre.pdf?1363290051=&response-content-disposition=inline%3B+filename%3DSynthesising_the_origins_of_language_and.pdf&Expires=1670758666&Signature=g2SXJ0F3WtIevMDqz~kIsx-xQkF6rcLGozNSMXU2-oMPvPNADLyHxoyucGxvnbefh-q7CI8Ct1kzjl8S8epdR2a0kjrZNgKiyVhS~-QZvUAHNpMIiX~tNhP2qFrmAqNCORoFYM0CsijO4KftefM57LKMkPglqaM5nKnKmXa-LWVqmWp4EpmiPjVxmHIPU1fpY00lDmbp3YUXpJwrM6XqJ6gB1tabyRL-pY4XVIp0zSSZzftbLvsI-MtRbSD3q6GpyAETh5zGmUK5v3DwLYVFY-CWtpR-O~53P9scOcHG3GJMc9lWSkiPh-516DQHnj75oWIrEV~2Z2cq6SoCYklZbA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> Synthesising the origins of language and meaning using co-evolution, self-organisation and level formation. </a>by L. Steels. Edinburgh University Press, 1997. <a href="link">  </a> </summary> The paper reports on experiments in which robotic agents and software agents are set up to originate language and meaning. The experiments test the hypothesis that mechanisms for generating complexity commonly found in biosystems, in particular self-organisation, co-evolution, and level formation, also may explain the spontaneous formation, adaptation, and growth in complexity of language. <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/30896575/10.1.1.80.2329-libre.pdf?1392074672=&response-content-disposition=inline%3B+filename%3DCollective_learning_and_semiotic_dynamic.pdf&Expires=1670759430&Signature=DMxT9Bps4TKHj0YbXPP~PvA1Eokadr2Pmkt-n3BUgtKshtc-4qUEb5VKWJYbKTrmH9sT~dh0nm6NedQXrPKehHN0tZLwxf6tlgQdWGrdSW0qU~o2Lje-kOk0z7JRER-ZeKZcJFLM~QO53HuTKtgBZ81ADu4zRbOXPvjvpCXeNDmwNcfXW3mnGw4a5qO9xqWhR6Pf7Tw5OHIOj5P0T~NLoULy044uOQxrguimzmDfuGDmZvn6vFRiYVimF2DhQJcYaOEVtZzF7bT7V7~Q-qlFSP7b1b5OgWHb6qmnzJKdMUP1eoriDhqxz~d9nGMbPdoHJeU6022-AaXqrjnY-XtQLw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> Collective learning and semiotic dynamics </a>by L. Steels. In Proceedings of the European Conference on Artificial Life, pages 679–688, 1999. <a href="link">  </a> </summary> We report on a case study in the emergence of a lexicon in a group of autonomous distributed agents situated and grounded in an open environment. Because the agents are autonomous, grounded, and situated, the possible words and possible meanings are not fixed but continuously change as the agents autonomously evolve their communication system and adapt it to novel situations. The case study shows that a complex semiotic dynamics unfolds and that generalisations present in the language are due to processes outside the agent. <br> - </details>

<details> <summary> <a href="https://digital.csic.es/bitstream/10261/128130/1/puzzle%20of%20language.pdf"> The puzzle of language evolution </a>by L. Steels. Kognitionswissenschaft, 8(4):143–150, 2000. <a href="link">  </a> </summary> Linguistics must again concentrate on the evolutionary nature of language, so that language models are more realistic with respect to human natural languages and have a greater explanatory force. Multi-agent systems are proposed as a possible route to develop such evolutionary models and an example is given of a concrete experiment in the origins and evolution of word-meaning based on a multi-agent approach. <br> - </details>

<details> <summary> <a href="https://langev.com/pdf/quinn01evolvingCommunication.pdf"> Evolving Communication without Dedicated Communication Channels </a>by Matt Quinn. ECAL, 2001. <a href="link">  </a> </summary> Artificial Life models have consistently implemented communication as an exchange of signals over dedicated and functionally isolated channels. I argue that such a feature prevents models from providing a satisfactory account of the origins of communication and present a model in which there are no dedicated channels. Agents controlled by neural networks and equipped with proximity sensors and wheels are presented with a co-ordinated movement task. It is observed that functional, but non-communicative, behaviours which evolve in the early stages of the simulation both make possible, and form the basis of, the communicative behaviour which subsequently evolves. <br> - </details>

<br/>

### Convergent Learning

<details> <summary> <a href="http://www.dklevine.com/archive/refs4415.pdf"> Learning Mixed Equilibria </a> by Drew Fundenberg, David M. Kreps. Journal of Games and Economic Behvaior, 1993.  </summary> We study learning processes for finite strategic-form games, in which players use the history of past play to forecast play in the current period. In a generalization of fictitious play, we assume only that players asymptotically choose best responses to the historical frequencies of opponents′ past play. This implies that if the stage-game strategies converge, the limit is a Nash equilibrium. In the basic model, plays seems unlikely to converge to a mixed-strategy equilibrium, but such convergence is natural when the stage game is perturbed in the manner of Harsanyi′s purification theorem.  <br> - </details>

<details> <summary> <a href="http://www.eecs.harvard.edu/cs286r/courses/spring06/papers/kalailehrer93.pdf"> Rational learning leads to nash equilibrium </a>by Ehud Kalai, Ehud Lehrer. Econometrica, 1993. <a href="link">  </a> </summary> Each of n players, in an infinitely repeated game, starts with subjective beliefs about his opponents' strategies. If the individual beliefs are compatible with the true strategies chosen, then Bayesian updating will lead in the long run to accurate prediction of the future play of the game. It follows that individual players, who know their own payoff matrices and choose strategies to maximize their expected utility, must eventually play according to a Nash equilibrium of the repeated game. An immediate corollary is that, when playing a Harsanyi-Nash equilibrium of a repeated game of incomplete information about opponents' payoff matrices, players will eventually play a Nash equilibrium of the real game, as if they had complete information <br> - </details>

<details> <summary> <a href="https://cs.brown.edu/~mlittman/papers/ml96-generalized.pdf"> A generalized reinforcement-learning model: Convergence and applications </a>by Michael L. Littman, C. Szepesvari. ICML, 1996. <a href="link">  </a> </summary> Reinforcement learning is the process by which an autonomous agent uses its experience interacting with an environment to improve its behavior. The Markov decision process (MDP) model is a popular way of formalizing the reinforcement learning problem but it is by no means the only way. In this paper we show how many of the important theoretical results concerning reinforcement learning in MDPs extend to a generalized MDP model that includes MDPs two-player games and MDPs under a worst-case optimality criterion as special cases. The basis of this extension is a stochastic approximation theorem that reduces asynchronous convergence to synchronous con <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf"> The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems </a> by Caroline Claus, Craig Boutilier. AAAI, 1998.  </summary> Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.  <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/A:1007514623589.pdf"> Conjectural Equilibrium in Multiagent Learning </a>by MICHAEL P. WELLMAN, JUNLING HU. Machine Learning, 1998. <a href="link">  </a> </summary> Learning in a multiagent environment is complicated by the fact that as other agents learn, the environment effectively changes. Moreover, other agents’ actions are often not directly observable, and the actions taken by the learning agent can strongly bias which range of behaviors are encountered. We define the concept of a conjectural equilibrium, where all agents’ expectations are realized, and each agent responds optimally to its expectations. We present a generic multiagent exchange situation, in which competitive behavior constitutes a conjectural equilibrium. We then introduce an agent that executes a more sophisticated strategic learning strategy, building a model of the response of other agents. We find that the system reliably converges to a conjectural equilibrium, but that the final result achieved is highly sensitive to initial belief. In essence, the strategic learner’s actions tend to fulfill its expectations. Depending on the starting point, the agent may be better or worse off than had it not attempted to learn a model of the other agents at all. <br> - </details>

<details> <summary> <a href="https://econweb.ucsd.edu/~jandreon/Econ264/papers/Erev%20Roth%20AER%201998.pdf"> Predicting how people play games: reinforcement leaning in experimental games with unique, mixed strategy equilibria </a> by Ido Erev, Alvin E. Roth. The American Economic Review, 1998. </summary> We examine learning in all experiments we could locate involving 100 periods or more of games with a unique equilibrium in mixed strategies, and in a new experiment. We study both the ex post ("best fit") descriptive power of learning models, and their ex ante predictive power, by simulating each experiment using parameters estimated from the other experiments. Even a one-parameter reinforcement learning model robustly outperforms the equilibrium predictions. Predictive power is improved by adding "forgetting" and "experimentation," or by allowing greater rationality as in probabilistic fictitious play. Implications for developing a low-rationality, cognitive game theory are discussed. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Satinder-Singh-3/publication/234140138_Nash_Convergence_of_Gradient_Dynamics_in_Iterated_General-Sum_Games/links/55ad059508ae98e661a2ade1/Nash-Convergence-of-Gradient-Dynamics-in-Iterated-General-Sum-Games.pdf"> Nash Convergence of Gradient Dynamics in General-Sum Games </a>by Satinder Singh, Michael Kearns, Yishay Mansour. UAI, 2000. <a href="link">  </a> </summary> Multi-agent games are becoming an increasingly prevalent formalism for the study of electronic commerce and auctions. The speed at which transactions can take place and the growing complexity of electronic marketplaces makes the study of computationally simple agents an appealing direction. In this work, we analyze the behavior of agents that incrementally adapt their strategy through gradient ascent on expected payoff, in the simple setting of two-player, two-action, iterated general-sum games, and present a surprising result. We show that either the agents will converge to a Nash equilibrium, or if the strategies themselves do not converge, then their average payoffs will nevertheless converge to the payoffs of a Nash equilibrium <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~mmv/papers/01ijcai-mike.pdf"> Rational and Convergent Learning in Stochastic Games </a>by Michael Bowling, Manuela Veloso. 2001. </summary> This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we briefly overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm, WoLF policy hillclimbing, that is based on a simple principle: “learn quickly while losing, slowly while winning.” The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.  <br> - </details>

<details> <summary> <a href="https://dspace.mit.edu/bitstream/handle/1721.1/3688/CS023.pdf?sequence=2&isAllowed=y"> Playing is believing: the role of beliefs in multi-agent learning </a> by Yu-Han Chang, Leslie Pack Kaelbling. NeurIPS, 2001. </summary> We propose a new classification for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classification, we review the optimality of existing algorithms and discuss some insights that can be gained. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the long-run against fair opponents. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/3-540-44795-4_33.pdf"> Social agents playing a periodical policy </a>by Ann Now´e, Johan Parent, and Katja Verbeeck. ECML, 2001. <a href="link">  </a> </summary> Coordination is an important issue in multiagent systems. Within the stochastic game framework this problem translates to policy learning in a joint action space. This technique however suffers some important drawbacks like the assumption of the existence of a unique Nash equilibrium and synchronicity, the need for central control, the cost of communication, etc. Moreover in general sum games it is not always clear which policies should be learned. Playing pure Nash equilibria is often unfair to at least one of the players, while playing a mixed strategy doesn’t give any guarantee for coordination and usually results in a sub-optimal payoff for all agents. In this work we show the usefulness of periodical policies, which arise as a side effect of the fairness conditions used by the agents. We are interested in games which assume competition between the players, but where the overall performance can only be as good as the performance of the poorest player. Players are social distributed reinforcement learners, who have to learn to equalize their payoff. Our approach is illustrated on synchronous one-step games as well as on asynchronous job scheduling games. <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/2002/file/f8e59f4b2fe7c5705bf878bbd494ccdf-Paper.pdf"> Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games </a>by Xiaofeng Wang, Tuomas Sandholm. NeurIPS, 2002. <a href="link">  </a> </summary> Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conflicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the first algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm’s parameters are easy to set to meet the convergence conditions. <br> - </details>

<details> <summary> <a href="https://www.sciencedirect.com/science/article/pii/S0004370202001212/pdf?crasolve=1&r=76e929ed589406db&ts=1669198737508&rtype=https&vrr=UKN&redir=UKN&redir_fr=UKN&redir_arc=UKN&vhash=UKN&host=d3d3LnNjaWVuY2VkaXJlY3QuY29t&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&iv=7f135f0e0222bf00fdd4304ee2a1ae9f&token=36303334373233626432363262646163646435326538633139323566646439336462623432323537373965623737383261653335393965373534323332333532616365373333346337623830666338373735616139396537333962383a333465363031303533343635303962346333666262613232&text=9e36f3db92abeb589d5da8a52095e27f9670f9d838b2ad4862938be7f725337583a19749fa0cc538c097a5baf504157a6cefe332eae16f6f272bc064381f4524219bd0363a96bdf58d6e26627fa730ff6cec4c9ad6d794be01e48ff4e223857fa5f9011bcb6a38bcfb560984c7bef5f934b93e74793f2a1547243e01e692b606979614b88b57ba5f4833e30854615a2d02124eae78f47ac7001997788775368d12924e437912cf828d5431f7a3e769024e7459bbd1997839f8b098ba49c013f6876d4d9d1a2b78a1705aeadb49566b37ec1f3f3a816efba12093a43af51508f89b60bbbc41bb6d3c2ca20e5dd140af278c1436b5ea7c507e8053624890df9b6f12f39a3dbf7cac42ab4dca5b8356300411cf58a5c91c81d91bd4811d65cd14aa4b3959746ebaac377a8115b4801ee33a&original=3f6d64353d3236333032333763363663366136323039316333326433373234316337393064267069643d312d73322e302d53303030343337303230323030313231322d6d61696e2e706466"> Multiagent learning using a variable learning rate </a>by Michael Bowling, Manuela Veloso. Artificial Intelligence, 2002. <a href="link">  </a> </summary> Learning to act in a multiagent environment is a difficult problem since the normal definition of an optimal policy no longer applies. The optimal policy at any moment depends on the policies of the other agents. This creates a situation of learning a moving target. Previous learning algorithms have one of two shortcomings depending on their approach. They either converge to a policy that may not be optimal against the specific opponents’ policies, or they may not converge at all. In this article we examine this learning problem in the framework of stochastic games. We look at a number of previous learning algorithms showing how they fail at one of the above criteria. We then contribute a new reinforcement learning technique using a variable learning rate to overcome these shortcomings. Specifically, we introduce the WoLF principle, “Win or Learn Fast”, for varying the learning rate. We examine this technique theoretically, proving convergence in self-play on a restricted class of iterated matrix games. We also present empirical results on a variety of more general stochastic games, in situations of self-play and otherwise, demonstrating the wide applicability of this method <br> - </details>

<details> <summary> <a href="https://papers.nips.cc/paper/2002/file/0d73a25092e5c1c9769a9f3255caa65a-Paper.pdf"> Efficient learning equilibrium </a>by Ronen I. Brafman, Moshe Tennenholtz. NeurIPS, 2002. <a href="link">  </a> </summary> We introduce efficient learning equilibrium (ELE), a normative approach to learning in noncooperative settings. In ELE, the learning algorithms themselves are required to be in equilibrium. In addition, the learning algorithms must arrive at a desired value after polynomial time, and a deviation from the prescribed ELE becomes irrational after polynomial time. We prove the existence of an ELE (where the desired value is the expected payoff in a Nash equilibrium) and of a Pareto-ELE (where the objective is the maximization of social surplus) in repeated games with perfect monitoring. We also show that an ELE does not always exist in the imperfect monitoring case. Finally, we discuss the extension of these results to general-sum stochastic games <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/2003/file/e71e5cd119bbc5797164fb0cd7fd94a4-Paper.pdf"> Extending Q-Learning to General Adaptive Multi-Agent Systems </a>by Gerald Tesauro. NeurIPS, 2003. <a href="link">  </a> </summary> Recent multi-agent extensions of Q-Learning require knowledge of other agents’ payoffs and Q-functions, and assume game-theoretic play at all times by all other agents. This paper proposes a fundamentally different approach, dubbed “Hyper-Q” Learning, in which values of mixed strategies rather than base actions are learned, and in which other agents’ strategies are estimated from observed actions via Bayesian inference. Hyper-Q may be effective against many different types of adaptive agents, even if they are persistently dynamic. Against certain broad categories of adaptation, it is argued that Hyper-Q may converge to exact optimal time-varying policies. In tests using Rock-Paper-Scissors, Hyper-Q learns to significantly exploit an Infinitesimal Gradient Ascent (IGA) player, as well as a Policy Hill Climber (PHC) player. Preliminary analysis of Hyper-Q against itself is also presented. <br> - </details>

<details> <summary> <a href="https://www.jair.org/index.php/jair/article/view/10393/24892"> Existence of Multiagent Equilibria with Limited Agents </a>by Michael Bowling, Manuela Veloso. JAIR, 2004. <a href="link">  </a> </summary> Multiagent learning is a necessary yet challenging problem as multiagent systems become moreprevalent and environments become more dynamic. Much of the groundbreaking work in this areadraws on notable results from game theory, in particular, the concept of Nash equilibria.  Learnersthat directly learn an equilibrium obviously rely on their existence.  Learners that instead seek toplay optimally with respect to the other players also depend upon equilibria since equilibria arefixed points for learning.  From another perspective, agents with limitations are real and common.These may be undesired physical limitations as well as self-imposed rational limitations, such asabstraction and approximation techniques, used to make learning tractable. This article explores theinteractions of these two important concepts:  equilibria and limitations in learning.  We introducethe question of whether equilibria continue to exist when agents have limitations.  We look at thegeneral effects limitations can have on agent behavior, and define a natural extension of equilibriathat accounts for these limitations. Using this formalization, we make three major contributions: (i)a counterexample for the general existence of equilibria with limitations, (ii) sufficient conditionson limitations that preserve their existence, (iii) three general classes of games and limitations thatsatisfy  these  conditions.   We  then  present  empirical  results  from  a  specific  multiagent  learningalgorithm applied to a specific instance of limited agents.  These results demonstrate that learningwith limitations is feasible, when the conditions outlined by our theoretical analysis hold. <br> - </details>

<details> <summary> <a href="https://ww2.odu.edu/~jsokolow/projects/files/Best-Response%20Multiagent%20Learning%20in%20Non-Stationary%20Environments.pdf"> Best-Response Multiagent Learning in Non-Stationary Environments </a>by Michael Weinberg, Jeffrey S. Rosenschein. AAMAS, 2004. <a href="link">  </a> </summary>
This paper investigates a relatively new direction in Multiagent Reinforcement Learning. Most multiagent learning techniques focus on Nash equilibria as elements of both the learning algorithm and its evaluation criteria. In contrast, we propose a multiagent learning algorithm that is optimal in the sense of finding a best-response policy, rather than in reaching an equilibrium. We present the first learning algorithm that is provably optimal against restricted classes of non-stationary opponents. The algorithm infers an accurate model of the opponent’s non-stationary strategy, and simultaneously creates a best-response policy against that strategy. Our learning algorithm works within the very general framework of N-player, general-sum stochastic games, and learns both the game structure and its associated optimal policy <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/s10994-006-0143-1.pdf"> AWESOME: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents </a>by Vincent Conitzer, Tuomas Sandholm. Machine Learning, 2007. <a href="link">  </a> </summary> Two minimal requirements for a satisfactory multiagent learning algorithm are that it 1. learns to play optimally against stationary opponents and 2. converges to a Nash equilibrium in self-play. The previous algorithm that has come closest, WoLF-IGA, has been proven to have these two properties in 2-player 2-action (repeated) games—assuming that the opponent’s mixed strategy is observable. Another algorithm, ReDVaLeR (which was introduced after the algorithm described in this paper), achieves the two properties in games with arbitrary numbers of actions and players, but still requires that the opponents’ mixed strategies are observable. In this paper we present AWESOME, the first algorithm that is guaranteed to have the two properties in games with arbitrary numbers of actions and players. It is still the only algorithm that does so while only relying on observing the other players’ actual actions (not their mixed strategies). It also learns to play optimally against opponents that eventually become stationary. The basic idea behind AWESOME (Adapt When Everybody is Stationary, Otherwise Move to Equilibrium) is to try to adapt to the others’ strategies when they appear stationary, but otherwise to retreat to a precomputed equilibrium strategy. We provide experimental results that suggest that AWESOME converges fast in practice. The techniques used to prove the properties of AWESOME are fundamentally different from those used for previous algorithms, and may help in analyzing future multiagent learning algorithms as well. <br> - </details>

<br/>

### Coordination

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/1994/AAAI94-065.pdf"> Learning to coordinate without sharing information </a>by Sandip Sen, Mahendra Sekaran, John Hale. National Conference on Artificial Intelligence, 1994. <a href="link">  </a> </summary> Researchers in the field of Distributed Artificial Intelligence (DAI) have been developing efficient mechanisms to coordinate the activities of multiple autonomous agents. The need for coordination arises because agents have to share resources and expertise required to achieve their goals. Previous work in the area includes using sophisticated information exchange protocols, investigating heuristics for negotiation, and developing formal models of possibilities of conflict and cooperation among agent interests. In order to handle the changing requirements of continuous and dynamic environments, we propose learning as a means to provide additional possibilities for effective coordination. We use reinforcement learning techniques on a block pushing problem to show that agents can learn complimentary policies to follow a desired path without any knowledge about each other. We theoretically analyze and experimentally verify the effects of learning rate on system convergence, and demonstrate benefits of using learned coordination knowledge on similar problems. Reinforcement learning based coordination can be achieved in both cooperative and non-cooperative domains, and in domains with noisy communication channels and other stochastic characteristics that present a formidable challenge to using other coordination schemes. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/ICMAS/1995/ICMAS95-049.pdf"> Recursive agent and agent-group tracking in a real-time dynamic environment </a>by M. Tambe. AAAI Press, 1995. <a href="link">  </a> </summary> Agent tracking is an important capability an intelligent agent requires for interacting with other
agents. It involves monitoring the observable actions of other agents as well as inferring their unobserved actions or high-level goals and behaviors. This paper focuses on a key challenge for agent tracking: recursive tracking of individuals or groups of agents. The paper first introduces an approach for tracking recursive agent models. To tame the resultant growth in the tracking effort and aid real-time performance, the paper then presents model sharing, an optimization that involves sharing the effort of tracking multiple models. Such shared models are dynamically unshared as needed -- in effect, a model is selectively tracked if it is dissimilar enough to require unsharing. The paper also discusses the application of recursive modeling in service of deception, and the impact of sensor imperfections. This investigation is based on our on-going effort to build intelligent pilot agents for a real-world synthetic air-combat environment. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/1302.3561.pdf"> Learning Conventions in Multiagent Stochastic Domains using Likelihood Estimates  </a>by Craig Boutilier. UAI, 1996. <a href="link">  </a> </summary> Fully cooperative multiagent systems-those in which agents share a joint utility model-is of special interest in AI. A key problem is that of ensuring that the actions of individual agents are coordinated, especially in settings where the agents are autonomous decision makers. We investigate approaches to learning coordinated strategies in stochastic domains where an agent's actions are not directly observable by others. Much recent work in game theory has adopted a Bayesian learning perspective to the more general problem of equilibrium selection, but tends to assume that actions can be observed. We discuss the special problems that arise when actions are not observable, including effects on rates of convergence, and the effect of action failure probabilities and asymmetries. We also use likelihood estimates as a means of generalizing fictitious play learning models in our setting. Finally, we propose the use of maximum likelihood as a means of removing strategies from consideration, with the aim of convergence to a conventional equilibrium, at which point learning and deliberation can cease. <br> - </details>

<details> <summary> <a href="https://www.cs.toronto.edu/~cebly/Papers/_download_/tark96.pdf"> Planning, Learning and Coordination in Multiagent Decision Processes </a>by Craig Boutilier. TARK, 1996. <a href="link">  </a> </summary> There has been a growing interest in AI in the design of multiagent systems, especially in multiagent cooperative planning. In this paper, we investigate the extent to which methods from single-agent planning and learning can be applied in multiagent settings. We survey a number of different techniques from decision-theoretic planning and reinforcement learning and describe a number of interesting issues that arise with regard to coordinating the policies of individual agents. To this end, we describe multiagent Markov decision processes as a general model in which to frame this discussion. These are special n-person cooperative games in which agents share the same utility function. We discuss coordination mechanisms based on imposed conventions (or social laws) as well as learning methods for coordination. Our focus is on the decomposition of sequential decision processes so that coordination can be learned (or imposed) locally, at the level of individual states. We also discuss the use of structured problem representations and their role in the generalization of learned conventions and in approximation. <br> - </details>

<details> <summary> <a href="http://www.ens.utulsa.edu/~sandip/research/web/papers/CoordinationLearner1995Sen.pdf"> Multiagent coordination with learning classifier systems </a>by Sandip Sen, Mahendra Sekaran. IJCAI, 1996. <a href="link">  </a> </summary> Researchers in the field of Distributed Artificial Intelligence (DAI) [Bond and Gasser, 1988] have developed a variety of agent coordination schemes under different assumptions about agent capabilities and relationships. Most of these schemes rely on shared knowledge or authority relationships between agents. These kinds of information may not be available or may be manipulated by malevolent agents. We have used reinforcement learning [Barto et al., 1989] as a coordination mechanism that imposes little cognitive burden on agents and does not suffer from the above-mentioned shortcomings [Sekaran and Sen, 1994; Sen et al., 1994]. In this paper, we evaluate a particular reinforcement learning methodology, a genetic algorithm based machine learning mechanism known as classifier systems [Holland, 1986] for developing action policies to optimize environmental feedback. Action policies that provide a mapping between perceptions and actions can be used by multiple agents to learn coordination strategies without having to rely on shared information. These agents are unaware of the capabilities of other agents and may or may not be cognizant of goals to achieve. We show that through repeated problem-solving experience, these agents can develop policies to maximize environmental feedback that can be interpreted as goal achievement from the viewpoint of an external observer. Experimental results from a couple of multiagent domains show that classifier systems can be more effective than the more widely used Q-learning scheme for multiagent coordination <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/publication/226319923_A_Distributed_Approach_for_Coordination_of_Traffic_Signal_Agents"> A game-theoretic approach to coordination of traffic signal agents </a>by Bazzan A. L. C. PhD thesis, 1997. <a href="link">  </a> </summary> Innovative control strategies are needed to cope with the increasing urban traffic chaos. In most cases, the currently used strategies are based on a central traffic-responsive control system which can be demanding to implement and maintain. Therefore, a functional and spatial decentralization is desired. For this purpose, distributed artificial intelligence and multi-agent systems have come out with a series of techniques which allow coordination and cooperation. However, in many cases these are reached by means of communication and centrally controlled coordination processes, giving little room for decentralized management. Consequently, there is a lack of decision-support tools at managerial level (traffic control centers) capable of dealing with decentralized policies of control and actually profiting from them. In the present work a coordination concept is used, which overcomes some disadvantages of the existing methods. This concept makes use of techniques of evolutionary game theory: intersections in an arterial are modeled as individually-motivated agents or players taking part in a dynamic process in which not only their own local goals but also a global one has to be taken into account. The role of the traffic manager is facilitated since s/he has to deal only with tactical ones, leaving the operational issues to the agents. Thus the system ultimately provides support for the traffic manager to decide on traffic control policies. Some application in traffic scenarios are discussed in order to evaluate the feasibility of transferring the responsibility of traffic signal coordination to agents. The results show different performances of the decentralized coordination process in different scenarios (e.g. the flow of vehicles is nearly equal in both opposing directions, one direction has a clearly higher flow, etc.). Therefore, the task of the manager is facilitate once s/he recognizes the scenario and acts accordingly. <br> - </details>

<details> <summary> <a href="http://www.ens.utulsa.edu/~sandip/jetai.pdf"> Individual learning of coordination knowledge </a>by Sandip Sen, Mahendra Sekaran. Journal of Experimental and Theoretical Artificial Intelligence, 1998. <a href="link">  </a> </summary> Social agents, both human and computational, inhabiting a world containing multiple active agents, need to coordinate their activities. This is because agents share resources, and without proper coordination or “rules of the road”, everybody will be interfering with the plans of others. As such, we need coordination schemes that allow agents to effectively achieve local goals without adversely affecting the problem-solving capabilities of other agents. Researchers in the field of Distributed Artificial Intelligence (DAI) have developed a variety of coordination schemes under different assumptions about agent capabilities and relationships. Whereas some of these research have been motivated by human cognitive biases, others have approached it as an engineering problem of designing the most effective coordination architecture or protocol. We evaluate individual and concurrent learning by multiple, autonomous agents as a means for acquiring coordination knowledge. We show that a uniform reinforcement learning algorithm suffices as a coordination mechanism in both cooperative and adversarial situations. Using a number of multiagent learning scenarios with both tight and loose coupling between agents and with immediate as well as delayed feedback, we demonstrate that agents can consistently develop effective policies to coordinate their actions without explicit information sharing. We demonstrate the viability of using both the Q-learning algorithm and genetic algorithm based classifier systems with different payoff schemes, namely the bucket brigade algorithm (BBA) and the profit sharing plan (PSP), for developing agent coordination on two different multi-agent domains. In addition, we show that a semi-random scheme for action selection is preferable to the more traditional fitness proportionate selection scheme used in classifier systems. <br> - </details>

<details> <summary> <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=29893127a377d37f16662dc482e012ff900bd55a"> Coordinated Reinforcement Learning </a>by Carlos Guestrin, Michail Lagoudakis, Ronald Parr. AAAI, 2002. <a href="link">  </a> </summary> We present several new algorithms for multiagent reinforcement learning. A common feature of these algorithms is a parameterized, structured representation of a policy or value function. This structure is leveraged in an approach we call coordinated reinforcement learning, by which agents coordinate both their action selection activities and their parameter updates. Within the limits of our parametric representations, the agents will determine a jointly optimal action without explicitly considering every possible action in their exponentially large joint action space. Our methods differ from many previous reinforcement learning approaches to multiagent coordination in that structured communication and coordination between agents appears at the core of both the learning algorithm and the execution architecture. Our experimental results, comparing our approach to other RL methods, illustrate both the quality of the policies obtained and the additional benefits of coordination <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2002/AAAI02-050.pdf"> Reinforcement Learning of Coordination in Cooperative Multiagent Systems </a>by Kapetanakis S. and Kudenko D. AAAI, 2002. <a href="link">  </a> </summary> We report on an investigation of reinforcement learning techniques for the learning of coordination in cooperative multiagent systems. Specifically, we focus on a novel action selection strategy for Q-learning (Watkins 1989). The new technique is applicable to scenarios where mutual observation of actions is not possible. To date, reinforcement learning approaches for such independent agents did not guarantee convergence to the optimal joint action in scenarios with high miscoordination costs. We improve on previous results (Claus & Boutilier 1998) by demonstrating empirically that our extension causes the agents to converge almost always to the optimal joint action even in these difficult cases. <br> - </details>

<details> <summary> <a href="https://link.springer.com/chapter/10.1007/3-540-36187-1_36"> Learning to Reach the Pareto Optimal Nash Equilibrium as a Team </a>by Katja Verbeeck, Ann Nowe, Tom Lenaerts, Johan Parent.  LNAI, 2002. <a href="link">  </a> </summary> Coordination is an important issue in multi-agent systems when agents want to maximize their revenue. Often coordination is achieved through communication, however communication has its price. We are interested in finding an approach where the communication between the agents is kept low, and a global optimal behavior can still be found. In this paper we report on an efficient approach that allows independent reinforcement learning agents to reach a Pareto optimal Nash equilibrium with limited communication. The communication happens at regular time steps and is basicallya signal for the agents to start an exploration phase. During each exploration phase, some agents exclude their current best action so as to give the team the opportunityto look for a possiblyb etter Nash equilibrium. This technique of reducing the action space byexclusions was onlyrecen tlyin troduced for finding periodical policies in games of conflicting interests. Here, we explore this technique in repeated common interest games with deterministic or stochastic outcomes. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Georgios-Chalkiadakis/publication/2553318_Coordination_in_Multiagent_Reinforcement_Learning_A_Bayesian_Approach/links/09e4151363949e7f80000000/Coordination-in-Multiagent-Reinforcement-Learning-A-Bayesian-Approach.pdf"> Coordination in Multiagent Reinforcement Learning: A Bayesian Approach </a>by Georgios Chalkiadakis, Craig Boutilier. AAMAS, 2003. <a href="link">  </a> </summary> Much emphasis in multiagent reinforcement learning (MARL) research is placed on ensuring that MARL algorithms (eventually) converge to desirable equilibria. As in standard reinforcement learning, convergence generally requires sufficient exploration of strategy space. However, exploration often comes at a price in the form of penalties or foregone opportunities. In multiagent settings, the problem is exacerbated by the need for agents to “coordinate” their policies on equilibria. We propose a Bayesian model for optimal exploration in MARL problems that allows these exploration costs to be weighed against their expected benefits using the notion of value of information. Unlike standard RL models, this model requires reasoning about how one’s actions will influence the behavior of other agents. We develop tractable approximations to optimal Bayesian exploration, and report on experiments illustrating the benefits of this approach in identical interest games. <br> - </details>

<details> <summary> <a href="https://core.ac.uk/download/pdf/54045748.pdf"> Coordination of independent learners in cooperative Markov games </a>by La¨etitia Matignon, Guillaume J. Laurent, Nadine Le Fort-Piat. Technical Report, 2009. <a href="link">  </a> </summary> In the framework of fully cooperative multi-agent systems, independent agents learning by reinforcement must overcome several difficulties as the coordination or the impact of exploration. The study of these issues allows first to synthesize the characteristics of existing reinforcement learning decentralized methods for independent learners in cooperative Markov games. Then, given the difficulties encountered by these approaches, we focus on two main skills: optimistic agents, which manage the coordination in deterministic environments, and the detection of the stochasticity of a game. Indeed, the key difficulty in stochastic environment is to distinguish between various causes of noise. The SOoN algorithm is so introduced, standing for “Swing between Optimistic or Neutral”, in which independent learners can adapt automatically to the environment stochasticity. Empirical results on various cooperative Markov games notably show that SOoN overcomes the main factors of non-coordination and is robust face to the exploration of other agents <br> - </details>

<br/>

### Cooperation

<details> <summary> <a href="https://www.aaai.org/Papers/Workshops/1994/WS-94-02/WS94-02-022.pdf"> Application of distributed AI and cooperative problem solving to telecommunications </a>by R. Weihmayer and H. Velthuijsen. In J. Liebowitz and D. Prereau, editors, AI Approaches to Telecommunications and Network Management. IOS Press, 1994. <a href="link">  </a> </summary> A cursory survey of applications in the DAI literature suggests that a primary area of using DAI technology for real-world systems can be found in telecommunications. This paper explores the current state-of-the-art of DAI and cooperative problem solving in this domain.Telecommunication networks have proven in the last five years to be a fertile ground for problems involving the coordination of distributed intelligence. A wide range of problems from distributed traffic management to resolution of service interactions in intelligent networks have led to conceptual studies and prototype developments involving systems of cooperative agents. Although there are currently no fielded systems that can be said to be DAI-based, we believe these will begin to appear within the next five years. We give an overview of the full range of potential applications found in the literature and additionally consider four applications in more detail, including the authors contributions to the field. <br> - </details>

<details> <summary> <a href="https://www.rand.org/content/dam/rand/pubs/reports/2009/R2728.pdf"> Distributed intelligence for air fleet control </a>by R. Steeb, S. Cammarata, F. Hayes-Roth, P. Thorndyke, and R. Wesson. Morgan Kaufmann Publishers, 1988. <a href="link">  </a> </summary> This report summarizes the results of a nine-month investigation of distributed intelligence for air fleet control, conducted for the Information Processing Techniques Office, Defense Advanced Research Projects Agency.<br> - </details>

<details> <summary> <a href="https://trs.jpl.nasa.gov/bitstream/handle/2014/35036/93-0553.pdf?sequence=1"> Enhancing performance of cooperating agents in realtime diagnostic systems </a>by U. M. Schwuttke and A. G. Quan. In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, 1993. <a href="link">  </a> </summary> We present a data-driven protocol and a supporting adlitecturc for communication among cooperating intelligent agents in real-time diagnostic systems. The system archilcctuc and the exchange of information among agents arc based on simplicity of agents, hierarchical organization of agents, and modular non-overlapping division of the problem domain. These featurcs combine 10 enable efficient diagnosis of complcex system failures in real-time environments with high dala volumes and moderate Pdilurc rates. Preliminary results of the real-world application of this work to the monitoring and diagnosis of complex systems are discussed in the context of NASA’s interplanetary mission operations. <br> - </details>

<details> <summary> <a href="https://web.media.mit.edu/~cynthiab/Readings/tan-MAS-reinfLearn.pdf"> Multi-Agent Reinforcement Learning Independent vs Cooperative Agents </a>by Ming Tan. ICML, 1993. <a href="link">  </a> </summary> Intelligent human agents exist in a cooperative social environment that facilitates learning. They learn not only by trial-and-error but also through cooperation by sharing instantaneous information episodic experience and learned knowledge. The key investigations of this paper are, "Given the same number of reinforcement learning agents will cooperative agents outperform independent agents who do not communicate during learning?" and "What is the price for such cooperation?" Using independent agents as a benchmark cooperative agents are studied in following ways: (1) sharing sensation, (2) sharing episodes and (3) sharing learned policies. This paper shows that as (a) additional sensation from another agent is beneficial if it can be used efficiently, (b) sharing learned policies or episodes among agents speeds up learning at the cost of communication and (c) for joint tasks agents engaging in partnership can significantly outperform independent agents although they may learn slowly in the beginning. These tradeoffs are not just limited to multi-agent reinforcement learning. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/252093/1/ES-WITH-APPL-7-4.pdf"> Integrating intelligent systems into a cooperating community for electricity distribution management </a>by L. Z. Varga, N. R. Jennings, and D. Cockburn. International Journal of Expert Systems with Applications, 7(4):563–579, 1994. <a href="link">  </a> </summary> Systems in which semi-autonomous problem solving agents communicate and cooperate with one another represent an exciting vision of future computing environments. However, if this vision is ever going to result in commercially viable systems then consideration must be given to the large software base which exists within many organisations. Success requires the ability to incorporate pre-existing systems alongside purpose built agents in a cooperating community. This requirement is vital because the former represent a substantial resource investment which companies cannot afford to consign to the scrap heap.<br> - </details>

<details> <summary> <a href="https://www.ai.rug.nl/~mwiering/GROUP/ARTICLES/TR-29-97-soccer.pdf"> Learning team strategies with multiple policy-sharing agents: A soccer case study </a>by R. Salustowicz, M. Wiering, and J. Schmidhuber. Technical report, 1997. <a href="link">  </a> </summary> We use simulated soccer to study multiagent learning. Each team's players (agents) share action set and policy, but may behave differently due to position-dependent inputs. All agents making up a team are rewarded or punished collectively in case of goals. We conduct simulations with varying team sizes, and compare several learning algorithms: TD-Q learning with linear neural networks (TD-Q), Probabilistic Incremental Program Evolution (PIPE), and a PIPE version that learns by coevolution (CO-PIPE). TD-Q is based on learning evaluation functions (EFs) mapping input/action pairs to expected reward. PIPE and CO-PIPE search policy space directly. They use adaptive probability distributions to synthesize programs that calculate action probabilities from current inputs. Our results show that linear TD-Q encounters several difficulties in learning appropriate shared EFs. PIPE and CO-PIPE, however, do not depend on EFs and find good policies faster and more reliably. This suggests that in some multiagent learning scenarios direct search in policy space can offer advantages over EF-based approaches. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/3-540-39963-1.pdf?pdf=button"> Evolving Behaviors for Cooperating Agents </a>by Jeffrey K. Bassett, Kenneth A. De Jong. Symposium on Methodologies for Intelligent Systems, 2000. <a href="link">  </a> </summary> A good deal of progress has been made in the past few years in the design and implementation of control programs for autonomous agents. A natural extension of this work is to consider solving difficult tasks with teams of cooperating agents. Our interest in this area is motivated in part by our involvement in a Navy-sponsored micro air vehicle (MAV) project in which the goal is to solve difficult surveillance tasks using a large team of small inexpensive autonomous air vehicles rather than a few expensive piloted vehicles. Our approach to developing control programs for these MAVs is to use evolutionary computation techniques to evolve behavioral rule sets. In this paper we describe our architecture for achieving this, and we present some of our initial results. <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839146&casa_token=Qb2aIlZRh9IAAAAA:1BrtA4ESZr_iJJJ4vhG6HZstOeQ-qfuqfs3iIGu2HutZb5bXpR9KyJTslQNF4YeYJ9l-Jw-mGac5BDk&tag=1"> Advantages of Cooperation Between Reinforcement Learning Agents in Difficult Stochastic Problems </a>by Hamid R. Berenji, David Vengerov. International Conference on Fuzzy Systems, 2000. <a href="link">  </a> </summary> This paper presents the first results in understanding the reasons for cooperative advantage between reinforcement learning agents. We consider a cooporation method which consists of using and updating a common policy. We tested this method on a complex fuzzy reinforcement learning problem and found that cooperation brings larger than expected benefits. More precisely, we found that K cooperative agents oach learning for N time steps outperform K independent agents each learning in a separate world for K*N time steps. In this paper, we explain the observed phenomenon and determine the necessary conditions for its presence in a wide class of reinforccment learning problems.  <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=839146&casa_token=Y-3elWT3fZUAAAAA:YAI-tJ6dnLkc6QZ7rGlVy0sOeKOEw3PRxqCdi2igfTJWnFcMbn6Zl5Y9WN-Iq9RlcNyP__7dPKsQJg&tag=1"> Advantages of Cooperation Between Reinforcement Learning Agents in Difficult Stochastic Problems  </a>by Hamid R. Berenji, David Vengerov. Fuzzy Systems, 2000. <a href="link">  </a> </summary> This paper presents the first results in understanding the reasons for cooperative advantage betwccn reinforcement learning agents. We consider a cooporation method which consists of using and updating a common policy. We tested this method on a complex fumy reinforcement learning problem and found that cooperation brings larger than expected benefits. More precisely, we found that K cooperative agents oach learning for N time steps outperform K independent agents oach learning in a separate world for K*N time steps. In this paper we explain the observed phenomenon and determine the necessary conditions for its presence in a wide class of reinforccment learning problems. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/cs/0105032.pdf"> Learning to Cooperate via Policy Search </a>by Leonid Peshkin, Kee-Eung Kim, Nicolas Meuleau, Leslie Pack Kaelbling. UAI, 2000. <a href="link">  </a> </summary> Cooperative games are those in which both agents share the same payoff structure. Valuebased reinforcement-learning algorithms, such as variants of Q-learning, have been applied to learning cooperative games, but they only apply when the game state is completely observable to both agents. Policy search methods are a reasonable alternative to value-based methods for partially observable environments. In this paper, we provide a gradient-based distributed policysearch method for cooperative games and compare the notion of local optimum to that of Nash equilibrium. We demonstrate the effectiveness of this method experimentally in a small, partially observable simulated soccer domain. <br> - </details>

<details> <summary> <a href="https://staff.fmi.uvt.ro/~daniela.zaharie/ma2016/projects/techniques/coevolution/CooperativeCoevolution1.pdf"> An empirical analysis of collaboration methods in cooperative coevolutionary algorithms </a>by R. P. Wiegand, W. Liles, and K. De Jong. In E. Cantu-Paz et al, editor, Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), pages 1235–1242, 2001. <a href="link">  </a> </summary> Although a variety of coevolutionary methods have been explored over the years, it has only been recently that a general architecture for cooperative coevolution has been proposed. Since that time, the flexibility and success of this cooperative coevolutionary architecture (CCA) has been shown in an array of different kinds of problems. However, many questions about the dynamics of this model, as well as the efficacy of various CCA-specific choices remain unanswered. One such choice surrounds the issue of how the algorithm selects collaborators for evaluation. This paper offers an empirical analysis of various types of collaboration mechanisms and presents some basic advice about how to choose a mechanism which is appropriate for a particular problem. <br> - </details>

<details> <summary> <a href="https://www.cse.unr.edu/~sushil/class/gas/papers/cea/cec02-egt.pdf"> Analyzing cooperative coevolution with evolutionary game theory </a>by R. P. Wiegand, W. Liles, and K. De Jong. In D. Fogel, editor, Proceedings of Congress on Evolutionary Computation (CEC-02), pages 1600–1605. IEEE Press, 2002. <a href="link">  </a> </summary> The task of understanding coevolutionary algorithms is a very difficult one. These algorithms search landscapes which are in some sense adaptive. As a result, the dynamical behaviors of coevolutionary systems can frequently be even more complex than traditional evolutionary algorithms (EAs). Moreover, traditional EA theory tells us little about coevolutionary algorithms. One major question that has yet to be clearly addressed is whether or not coevolutionary algorithms are well–suited for optimization tasks. Although this question is equally applicable to competitive, as well as cooperative approaches, answering the question for cooperative coevolutionary algorithms is perhaps more attainable. Recently, evolutionary game theoretic (EGT) models have begun to be used to help analyze the dynamical behaviors of coevolutionary algorithms. One type of EGT model which is already reasonably well understood are multi–population symmetric games. We believe these games can be used to analytically model cooperative coevolutionary algorithms. This paper introduces our analysis framework, explaining how and why such models may be generated. It includes some examples illustrating specific theoretical and empirical analyses. We demonstrate that using our framework, a better understanding for the degree to which cooperative coevolutionary algorithms can be used for optimization can be achieved. <br> - </details>

<details> <summary> <a href="http://eecs.ucf.edu/~wiegand/papers/Wiegand2002foga.pdf"> Modeling variation in cooperative coevolution using evolutionary game theory </a>by R. P. Wiegand, W. Liles, and K. De Jong. FOGA, pages 1600–1605, 2002. <a href="link">  </a> </summary> Though coevolutionary algorithms are currently used for optimization purposes, practitioners are often plagued with difficulties due to the fact that such systems frequently behave in counter intuitive ways that are not well understood. This paper seeks to extend work which uses evolutionary game theory (EGT) as a form of dynamical systems modeling of coevolutionary algorithms in order to begin to answer questions regarding how these systems work. It does this by concentrating on a particular subclass of cooperative coevolutionary algorithms, for which multi-population symmetric evolutionary game theoretic models are known to apply. We examine dynamical behaviors of this model in the context of static function optimization, by both formal analysis, as well as model validation study. Finally, we begin looking at the effects of variation by extending traditional EGT, offering some introductory analysis, as well as model validation. In the course of this study, we investigate the effects of parameterized uniform crossover and bit-flip mutation. <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/2003/file/c8067ad1937f728f51288b3eb986afaa-Paper.pdf"> All learning is local: Multi-agent learning in global reward games </a>by Yu-Han Chang, Tracey Ho, Leslie Pack Kaelbling. NeurIPS, 2003. <a href="link">  </a> </summary> In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efficient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman filtering to allow an agent to construct a good training signal and learn an effective policy. <br> - </details>

<details> <summary> <a href="http://www.tesseract.org/paul/papers/WiegandSarma2004pps-final.pdf"> Spatial embedding and loss of gradient in cooperative coevolutionary algorithms. </a>by R. P. Wiegand and J. Sarma. Springer, 2004. <a href="link">  </a> </summary> Coevolutionary algorithms offer great promise as adaptive problem
solvers but suffer from several known pathologies. Historically, spatially embedded coevolutionary algorithms seem to have succeeded where other coevolutionary approaches fail; however, explanations for this have been largely unexplored. We examine this idea more closely by looking at spatial models in the context of a particular coevolutionary pathology: loss of gradient. We believe that loss of gradient in cooperative coevolution is caused by asymmetries in the problem or initial conditions between populations, driving one population to convergence before another. Spatial models seem to lock populations together in terms of evolutionary change, helping establish a type of dynamic balance to thwart loss of gradient. We construct a tunably asymmetric function optimization problem domain and conduct an empirical study to justify this assertion. We find that spatial restrictions for collaboration and selection can help keep population changes balanced when presented with severe asymmetries in the problem. <br> - </details>

<br/>

### Competing

<details> <summary> <a href="https://data.exppad.com/public/papers/Machine%20Learning/MMARL/Tesauro%2C%20Kephart%20(2002)%3A%20Princing%20in%20Agent%20Economies%20Using%20Multi-Agent%20Q-Learning.pdf"> Pricing in agent economies using multi-agent Q-learning. Autonomous Agents and Multi-Agent Systems </a>by G. Tesauro and J. O. Kephart. Springer, 8:289--304, 2002. <a href="link">  </a> </summary> This paper investigates how adaptive software agents may utilize reinforcement learning algorithms such as Q-learning to make economic decisions such as setting prices in a competitive marketplace. For a single adaptive agent facing fixed-strategy opponents, ordinary Q-learning is guaranteed to find the optimal policy. However, for a population of agents each trying to adapt in the presence of other adaptive agents, the problem becomes non-stationary and history dependent, and it is not known whether any global convergence will be obtained, and if so, whether such solutions will be optimal. In this paper, we study simultaneous Q-learning by two competing seller agents in three moderately realistic economic models. This is the simplest case in which interesting multi-agent phenomena can occur, and the state space is small enough so that lookup tables can be used to represent the Q-functions. We find that, despite the lack of theoretical guarantees, simultaneous convergence to self-consistent optimal solutions is obtained in each model, at least for small values of the discount parameter. In some cases, exact or approximate convergence is also found even at large discount parameters. We show how the Q-derived policies increase profitability and damp out or eliminate cyclic price “wars” compared to simpler policies based on zero lookahead or short-term lookahead. In one of the models (the "Shopbot" model) where the sellers’ profit functions are symmetric, we find that Q-learning can produce either symmetric or broken-symmetry policies, depending on the discount parameter and on initial conditions. <br> - </details>

<br/>

### Decentralised Partially Observable MDPs

<details> <summary> <a href="https://www.researchgate.net/profile/Stacy-Marsella-2/publication/2570317_Taming_Decentralized_POMDPs_Towards_Efficient_Policy_Computation_for_Multiagent_Settings/links/573085d508aee022975c43b3/Taming-Decentralized-POMDPs-Towards-Efficient-Policy-Computation-for-Multiagent-Settings.pdf"> Taming Decentralized POMDPs: Towards Efficient Policy Computation for Multiagent Settings </a>by R. Nair, M. Tambe, M. Yokoo, D. Pynadath, S. Marsella. IJCAI, 2003. <a href="link">  </a> </summary> The problem of deriving joint policies for a group of agents that maximize some joint reward function can be modeled as a decentralized partially observable Markov decision process (POMDP). Yet, despite the growing importance and applications of decentralized POMDP models in the multiagents arena, few algorithms have been developed for efficiently deriving joint policies for these models. This paper presents a new class of locally optimal algorithms called “Joint Equilibrium based search for policies (JESP)”. We first describe an exhaustive version of JESP and subsequently a novel dynamic programming approach to JESP. Our complexity analysis reveals the potential for exponential speedups due to the dynamic programming approach. These theoretical results are verified via empirical comparisons of the two JESP versions with each other and with a globally optimal brute-force search algorithm. Finally, we prove piece-wise linear and convexity (PWLC) properties, thus taking steps towards developing algorithms for continuous belief states <br> - </details>

<br/>

### Decision Theory

<details> <summary> <a href="https://www.ijcai.org/Proceedings/91-1/Papers/011.pdf"> A decision-theoretic approach to coordinating multiagent interactions </a>by Piotr J. Gmytrasiewicz, Edmund H. Durfee, David K. Weh. IJCAI, 1991. <a href="https://www.Summary.so/instadeep/Multiagent-Learning-Basics-Challenges-and-Prospect-21cb7b4294b84a4188cafd184a3deed8">  </a> </summary> We describe a decision-theoretic method that an autonomous agent can use to model multiagent situations and behave rationally based on its model. Our approach, which we call the Recursive Modeling Method, explicitly accounts for the recursive nature of multiagent reasoning. Our method lets an agent recursively model another agent's decisions based on probabilistic views of how that agent perceives the multiagent situation, which in turn are derived from hypothesizing how that other agent perceives the initial agent's possible decisions, and so on. Further, we show how the possibility of multiple interactions can affect the decisions of agents, allowing cooperative behavior to emerge as a rational choice of selfish agents that otherwise might behave uncooperatively <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/1301.3836.pdf"> The Complexity of Decentralized Control of Markov Decision Processes </a>by Daniel S. Bernstein, Shlomo Zilberstein, Neil Immerman. UAI, 2000. <a href="link">  </a> </summary> Planning for distributed agents with partial state information is considered from a decision theoretic perspective. We describe generalizations of both the MDP and POMDP models that allow for decentralized control. For even a small number of agents, the finite-horizon problems corresponding to both of our models are complete for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov processes. In contrast to the MDP and POMDP problems, the problems we consider provably do not admit polynomialtime algorithms and most likely require doubly exponential time to solve in the worst case. We have thus provided mathematical evidence corresponding to the intuition that decentralized planning problems cannot easily be reduced to centralized problems and solved exactly using established techniques.  <br> - </details>

<details> <summary> <a href="https://www.jair.org/index.php/jair/article/view/10339/24717"> Decision-Theoretic Bidding Based on Learned Density Models in Simultaneous, Interacting Auctions </a>by P. Stone, R. S. P., M. L. Littman, J. A. Csirik, and D. McAlleste. JAIR, 2003. <a href="link">  </a> </summary> Auctions are becoming an increasingly popular method for transacting business, especially over the Internet. This article presents a general approach to building autonomous bidding agents to bid in multiple simultaneous auctions for interacting goods. A core component of our approach learns a model of the empirical price dynamics based on past data and uses the model to analytically calculate, to the greatest extent possible, optimal bids. We introduce a new and general boosting-based algorithm for conditional density estimation problems of this kind, i.e., supervised learning problems in which the goal is to estimate the entire conditional distribution of the real-valued label. This approach is fully implemented as ATTac-2001, a top-scoring agent in the second Trading Agent Competition (TAC-01). We present experiments demonstrating the effectiveness of our boosting-based price predictor relative to several reasonable alternatives. <br> - </details>

<br/>

### Dispersion Games

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2002/AAAI02-061.pdf"> Dispersion games: general definitions and some specific learning results </a>by T. Grenager, R. Powers, Y. Shoham. AAAI, 2002. <a href="link">  </a> </summary> Dispersion games are the generalization of the anticoordination game to arbitrary numbers of agents and actions. In these games agents prefer outcomes in which the agents are maximally dispersed over the set of possible actions. This class of games models a large number of natural problems, including load balancing in computer science, niche selection in economics, and division of roles within a team in robotics. Our work consists of two main contributions. First, we formally define and characterize some interesting classes of dispersion games. Second, we present several learning strategies that agents can use in these games, including traditional learning rules from game theory and artificial intelligence, as well as some special purpose strategies. We then evaluate analytically and empirically the performance of each of these strategies. <br> - </details>

<br/>

### Evaluation

<details> <summary> <a href="https://www.aaai.org/Papers/Workshops/2002/WS-02-06/WS02-06-013.pdf"> Analyzing Complex Strategic Interactions in Multi-Agent Systems </a>by William E. Walsh, Rajarshi Das, Gerald Tesauro, Jeffrey O. Kephart. AAAI, 2002. <a href="link">  </a> </summary>  We develop a model for analyzing complex games with repeated interactions, for which a full game-theoretic analysis is intractable. Our approach treats exogenously specified, heuristic strategies, rather than the atomic actions, as primitive, and computes a heuristic-payoff table specifying the expected payoffs of the joint heuristic strategy space. We analyze two games based on (i) automated dynamic pricing and (ii) continuous double auction. For each game we compute Nash equilibria of previously published heuristic strategies. To determine the most plausible equilibria, we study the replicator dynamics of a large population playing the strategies. In order to account for errors in estimation of payoffs or improvements in strategies, we also analyze the dynamics and equilibria based on perturbed payoff <br> - </details>

<details> <summary> <a href="https://apps.dtic.mil/sti/pdfs/ADA436200.pdf#page=104"> Run the GAMUT: A Comprehensive Approach to Evaluating Game-Theoretic Algorithms </a>by Eugene Nudelman, Jennifer Wortman, Yoav Shoham. AAMAS, 2004. <a href="link">  </a> </summary> We present GAMUT, a suite of game generators designed for testing game-theoretic algorithms. We explain why such a generator is necessary, offer a way of visualizing relationships between the sets of games supported by GAMUT, and give an overview of GAMUT’s architecture. We highlight the importance of using comprehensive test data by benchmarking existing algorithms. We show surprisingly large variation in algorithm performance across different sets of games for two widely-studied problems: computing Nash equilibria and multiagent learning in repeated games. <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/2004/file/220a7f49d42406598587a66f02584ac3-Paper.pdf"> New Criteria and a New Algorithm for Learning in Multi-Agent Systems </a>by Rob Powers, Yoav Shoham. NeurIPS, 2004. <a href="link">  </a> </summary> We propose a new set of criteria for learning algorithms in multi-agent systems, one that is more stringent and (we argue) better justified than previous proposed criteria. Our criteria, which apply most straightforwardly in repeated games with average rewards, consist of three requirements: (a) against a specified class of opponents (this class is a parameter of the criterion) the algorithm yield a payoff that approaches the payoff of the best response, (b) against other opponents the algorithm’s payoff at least approach (and possibly exceed) the security level payoff (or maximin value), and (c) subject to these requirements, the algorithm achieve a close to optimal payoff in self-play. We furthermore require that these average payoffs be achieved quickly. We then present a novel algorithm, and show that it meets these new criteria for a particular parameter class, the class of stationary opponents. Finally, we show that the algorithm is effective not only in theory, but also empirically. Using a recently introduced comprehensive game theoretic test suite, we show that the algorithm almost universally outperforms previous learning algorithms <br> - </details>

<details> <summary> <a href="https://www.emse.fr/~boissier/enseignement/sma05/exposes/PowersShoham_LearningAgainstBoundedMemory.pdf"> Learning against opponents with bounded memory </a>by Rob Powers, Yoav Shoham. IJCAI, 2005. <a href="link">  </a> </summary> Recently, a number of authors have proposed criteria for evaluating learning algorithms in multiagent systems. While well-justified, each of these has generally given little attention to one of the main challenges of a multi-agent setting: the capability of the other agents to adapt and learn as well. We propose extending existing criteria to apply to a class of adaptive opponents with bounded memory which we describe. We then show an algorithm that provably achieves an epsilon-best response against this richer class of opponents while simultaneously guaranteeing a minimum payoff against any opponent and performing well in self-play. This new algorithm also demonstrates strong performance in empirical tests against a variety of opponents in a wide range of environments <br> - </details>

<br/>

### Evolutionary Game Theory and Strategies

<details> <summary> <a href="http://etherplan.com/the-logic-of-animal-conflict.pdf"> The logic of animal conflict </a>by Maynard Smith, J., Price, G.R. Nature, 1973. <a href="link">  </a> </summary> Conflicts between animals of the same species usually are of “limited war” type, not causing serious injury. This is often explained as due to group or species selection for behaviour benefiting the species rather than individuals. Game theory and computer simulation analyses show, however, that a “limited war” strategy benefits individual animals as well as the species. <br> - </details>

<details> <summary> <a href="https://www.reed.edu/biology/courses/BIO342/2012_syllabus/2012_readings/smith_1976_games.pdf"> Evolution and the Theory of Games </a>by J. Maynard Smith. American Scientist, 1976. <a href="link">  </a> </summary> n situations characterized by conflict of interest, the best strategy to adopt depends on what others are doing. <br> - </details>

<details> <summary> <a href="http://math.uchicago.edu/~shmuel/Modeling/Axelrod%20and%20Hamilton.pdf"> The Evolution of Cooperation </a>by Robert Axelrod, William D. Hamilton. Science, 1981. <a href="link">  </a> </summary> Cooperation in organisms, whether bacteria or primates, has been a difficulty for evolutionary theory since Darwin. On the assumption that interactions between pairs of individuals occur on a probabilistic basis, a model is developed based on the concept of an evolutionarily stable strategy in the context of the Prisoner's Dilemma game. Deductions from the model, and the results of a computer tournament show how cooperation based on reciprocity can get started in an asocial world, can thrive while interacting with a wide range of other strategies, and can resist invasion once fully established. Potential applications include specific aspects of territoriality, mating, and disease. <br> - </details>

<details> <summary> <a href="https://www.econstor.eu/bitstream/10419/95028/1/wp347.pdf"> An Introduction to Evolutionary Game Theory </a>by Weibull, Jörgen W. IFN, 1992. <a href="link">  </a> </summary> Please note that these lecture notes are incomplete and may contain typos and errors. I hope to have a more full-fledged version in a few months, and appreciate in the meantime comments and suggestions.  <br> - </details>

<details> <summary> <a href="http://bobby.cs-i.brandeis.edu/papers/icga5.pdf"> Competitive Environments Evolve Better Solutions for Complex Tasks </a>by Peter J. Angeline, Jordan B. Pollack. ICGA, 1993. <a href="link">  </a> </summary> In the typical genetic algorithm experiment, the fitness function is constructed to be independent of the contents of the population to provide a consistent objective measure. Such objectivity entails significant knowledge about the environment which suggests either the problem has previously been solved or other non-evolutionary techniques may be more efficient. Furthermore, for many complex tasks an independent fitness function is either impractical or impossible to provide. In this paper, we demonstrate that competitive fitness functions, i.e. fitness functions that are dependent on the constituents of the population, can provide a more robust training environment than independent fitness functions. We describe three differing methods for competitive fitness, and discuss their respective advantages <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/1994/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf"> Learning to play the game of chess </a>by S. Thrun. The MIT Press, Cambridge, MA, 1995. <a href="link">  </a> </summary> This paper presents NeuroChess, a program which learns to play chess from the final outcome of games. NeuroChess learns chess board evaluation functions, represented by artificial neural networks. It integrates inductive neural network learning, temporal differencing, and a variant of explanation-based learning. Performance results illustrate some of the strengths and weaknesses of this approach. 
<br> - </details>

<details> <summary> <a href="https://www.econstor.eu/bitstream/10419/94705/1/wp407.pdf"> Nash Equilibrium and Evolution by Imitation </a>by Bjornerstedt J., and Weibull, J. The Rational Foundations of Economic Behavior, 1995. <a href="link">  </a> </summary> Nash's "mass action" interpretation of his equilibrium concept does not presume that the players know the game or are capable of sophisticated calculations. Instead, players are repeatedly and randomly drawn from large populations to play the game, one population for each player position, and base their strategy choice on observed payoffs. The present paper examines in some detail such an interpretation in a dass of population dynamics based on adaptation by way of imitation of successful behaviors. Drawing from results in evolutionary game theory, implications of dynamic stability for aggregate Nash equilibrium play are discussed.  <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Forrest-Bennett-Iii/publication/2345524_Discovery_by_Genetic_Programming_of_a_Cellular_Automata_Rule_that_is_Better_than_any_Known_Rule_for_the_Majority_Classification_Problem/links/0912f50a3c6bd0ccd2000000/Discovery-by-Genetic-Programming-of-a-Cellular-Automata-Rule-that-is-Better-than-any-Known-Rule-for-the-Majority-Classification-Problem.pdf"> Discovery by Genetic Programming of a Cellular Automata Rule that is Better than any Known Rule for the Majority Classification Problem </a>by David Andre, Forrest H Bennett, John R. Koza. Genetic programming, 1996. <a href="link">  </a> </summary> It is difficult to program cellular automata. This is especially true when the desired computation requires global communication and global integration of information across great distances in the cellular space. Various human-written algorithms have appeared in the past two decades for the vexatious majority classification task for one-dimensional two-state cellular automata. This paper describes how genetic programming with automatically defined functions evolved a rule for this task with an accuracy of 82.326%. This level of accuracy exceeds that of the original 1978 Gacs-Kurdyumov-Levin (GKL) rule, all other known human-written rules, and all other known rules produced by automated methods. The rule evolved by genetic programming is qualitatively different from all previous rules in that it employs a larger and more intricate repertoire of domains and particles to represent and communicate information across the cellular space. <br> - </details>

<details> <summary> <a href="http://www.cs.unibo.it/babaoglu/courses/cas06-07/resources/tutorials/Evolving_Cellular_Automata.pdf"> Evolving Cellular Automata with Genetic Algorithms: A Review of Recent Work </a>by Melanie Mitchell, James P. Crutchfield, Rajarshi Das. EvCA, 1996. <a href="link">  </a> </summary> We review recent work done by our group on applying genetic algorithms (GAs) to the design of cellular automata (CAs) that can perform computations requiring global coordination. A GA was used to evolve CAs for two computational tasks: density classification and synchronization. In both cases, the GA discovered rules that gave rise to sophisticated emergent computational strategies. These strategies can be analyzed using a "computational mechanics" framework in which "particles" carry information and interactions between particles effects information processing. This framework can also be used to explain the process by which the strategies were designed by the GA. The work described here is a first step in employing GAs to engineer useful emergent computation in decentralized multi-processor systems. It is also a first step in understanding how an evolutionary process can produce complex systems with sophisticated collective computational abilities. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Spring/1996/SS-96-01/SS96-01-009.pdf"> Methods for Competitive and Cooperative Co-evolution </a>by John Grefenstette, Robert Daley. AAAI, 1996. <a href="link">  </a> </summary> We have been investigating evolutionary methods to design behavioral strategies for intelligent robots in multi-agent environments. Such ~nvironments resemble an ecological system in which species evolve and adapt in a complex interaction with other evolving and adapting species. This paper will report on our investigations of alternative co-evolutionary approaches in the context of a simulated multi-agent environment <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Thomas-Haynes/publication/2751435_Co--adaptation_in_a_Team/links/573b730208ae298602e45732/Co--adaptation-in-a-Team.pdf"> Co-adaptation in a Team </a>by Thomas D. Haynes, Sandip Sen. IJCIO, 1997. <a href="link">  </a> </summary> We introduce a cooperative co-evolutionary system to facilitate the development of teams of heterogeneous agents. We believe that k different behavioral strategies for controlling the actions of a group of k agents can combine to form a cooperation strategy which efficiently achieves global goals. We both examine the on-line adaption of behavioral strategies utilizing genetic programming and demonstrate the successful co-evolution of cooperative individuals. We present a new crossover mechanism for genetic programming systems in order to facilitate the evolution of more than one member in the team during each crossover operation. Our goal is to reduce the time needed to evolve an effective team. <br> - </details>

<details> <summary> <a href="https://www.econstor.eu/bitstream/10419/94851/1/wp487.pdf"> What have we learned from Evolutionary Game Theory so far? </a>by Weibull, Jörgen W. IFN, 1997. <a href="link">  </a> </summary> Evolutionary theorizing has a long tradition in economics. Only recently has this approach been brought into the framework of noncooperative game theory. Evolutionary game theory studies the robustness of strategic behavior with respect to evolutionary forces in the context of games played many times in large populations of boundedly rational agents. This new strand in economic theory has lead to new predictions and opened up doors to other social sciences. The discussion will be focused on the following questions: What distinguishes the evolutionary approach from the rationalistic? What are the most important ndings in evolutionary game theory so far? What are the next challenges for evolutionary game theory in economics? <br> - </details>

<details> <summary> <a href="https://core.ac.uk/download/pdf/6518937.pdf"> Learning through Reinforcement and Replicator Dynamics </a>by Borgers, T., Sarin, R. Journal of Economic Theory, 1997. <a href="link">  </a> </summary> This paper considers a version of R. R. Bush and F. Mosteller's stochastic learning theory in the context of games. We show that in a continuous time limit the learning model converges to the replicator dynamics of evolutionary game theory. Thus we provide a non-biological interpretation of evolutionary game theory. <br> - </details>

<details> <summary> <a href="https://web.unbc.ca/~russellt/swarm/hitoshiiba1.pdf"> Evolutionary learning of communicating agents </a>by Hitoshi Iba. Journal of Information Sciences, 1998. <a href="link">  </a> </summary> This paper presents the emergence of the cooperative behavior for communicating agents by means of Genetic Programming (GP). Our experimental domains are the pursuit game and the robot navigation task. We conduct experiments with the evolution of the communicating agents and show the effectiveness of the emergent communication in terms of the robustness of generated GP programs. The performance of GP-based multi-agent learning is discussed with comparative experiments by using different breeding strategies, i.e., homogenous breeding and heterogeneous breeding <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/44626510/white98-gpc-libre.pdf?1460385545=&response-content-disposition=inline%3B+filename%3DASGA_Improving_the_ant_system_by_integra.pdf&Expires=1670780595&Signature=b3lMVuKe1NoXEV2ASdlYm6A6DevE58J1ljc~H9XV8CwCiKTPh-s1iuJOclPizMdq16fVJPtcxbKvJAiljZotnftKLgovb2-JOV2LBFKRLT~hfLny30KqTL8xAQBvllc3OZQOEA0AhJx1NWM2H9DrBeji9O9S8zkIekQPIzZFwUuoYaj6Tg2~QA1dsOh8OiR6YHbiQDOtPgz02b8Is6oPlV-p48BUuDP6GVgjTO3Qg4So9p0XvnTFjdCS7ug00JglxUqKJJiN1h9zHuLCYLImacmRzwREnb3VhNIqpjbPTaTwujvnsW~yh3R3gXwUY1die-3fJIxhuLr1Mwu4QJyQLw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> ASGA: Improving the ant system by integration with genetic algorithms </a>by T. White, B. Pagurek, and F. Oppacher. In J. R. Koza, W. Banzhaf, K. Chellapilla, K. Deb, M. Dorigo, D. B. Fogel, M. H. Garzon, D. E. Goldberg, H. Iba, and R. Riolo, editors, Genetic Programming, 1998. <a href="link">  </a> </summary> This paper describes how the Ant System can be improved by selfadaptation of its controlling parameters. Adaptation is achieved by integrating a genetic algorithm with the ant system and maintaining a population of agents (ants) that have been used to generate solutions. These agents have behavior that is inspired by the foraging activities of ants, with each agent capable of simple actions. Problem solving is inherently distributed and arises as a consequence of the self-organization of a collection of agents, or swarm system. This paper applies the Ant System with Genetic Algorithm (ASGA) system to the problem of path finding in networks, demonstrating by experimentation that the hybrid algorithm exhibits improved performance when compared to the basic Ant System. <br> - </details>

<details> <summary> <a href="http://www.fulviofrisone.com/attachments/article/412/Hofbauer%20Evolutionary%20Games%20and%20Population%20Dynamics.pdf"> Evolutionary Games and Population Dynamics </a>by Hofbauer, J., Sigmund, K. Cambridge University Press, 1998. <a href="link">  </a> </summary> Every form of behaviour is shaped by trial and error. Such stepwise adaptation can occur through individual learning or through natural selection, the basis of evolution. Since the work of Maynard Smith and others, it has been realised how game theory can model this process. Evolutionary game theory replaces the static solutions of classical game theory by a dynamical approach centred not on the concept of rational players but on the population dynamics of behavioural programmes. In this book the authors investigate the nonlinear dynamics of the self-regulation of social and economic behaviour, and of the closely related interactions between species in ecological communities. Replicator equations describe how successful strategies spread and thereby create new conditions which can alter the basis of their success, i.e. to enable us to understand the strategic and genetic foundations of the endless chronicle of invasions and extinctions which punctuate evolution. In short, evolutionary game theory describes when to escalate a conflict, how to elicit cooperation, why to expect a balance of the sexes, and how to understand natural selection in mathematical terms. <br> - </details>

<details> <summary> <a href="http://gpbib.cs.ucl.ac.uk/cache/cache/.hidden_13-jun_2087063365/http___www.cs.ucl.ac.uk_staff_W.Langdon_aigp3_ch19.pdf"> Evolving Multiple Agents by Genetic Programming </a>by Hitoshi Iba. MIT press, 1999. <a href="link">  </a> </summary> On the emergence of the cooperative behaviour for multiple agents by means of Genetic Programming (GP). Our experimental domains are multi-agent test beds, i.e., the robot navigation task and the Tile World. The world consists of a simulated robot agent and a simulated environment which is both dynamic and unpredictable. In our previous paper, we proposed three types of strategies, i.e, homogeneous breeding, heterogeneous breeding, and co-evolutionary breeding, for the purpose of evolving the cooperative behavior. We use the heterogeneous breeding in this paper. The previous Q-learning approach commonly used for the multi-agent task has the difficulty with the combinatorial explosion for many agents. This is because the state space for Q-table is so huge for the practical computer resources. We show how successfully GP-based multi-agent learning is applied to multi-agent tasks and compare the performance with Q-learning by experiments. Thereafter, we conduct experiments with the evolution of the communicating agents. The communication is an essential factor for the emergence of cooperation. This is because a collaborative agent must be able to handle situations in which conflicts arise and must be capable of negotiating with other agents to reach an agreement. The effectiveness of the emergent communication is empirically shown in terms of the robustness of generated GP programs. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Herbert-Gintis/publication/235734033_Game_Theory_Evolving/links/0c96052f7a69760977000000/Game-Theory-Evolving.pdf"> Game Theory Evolving </a>by Herbert Gintis. Princeton University Press, 2000. <a href="link">  </a> </summary> Since its original publication Game Theory Evolving has been considered the best textbook on evolutionary game theory. This completely revised and updated second edition of Game Theory Evolving contains new material and shows students how to apply game theory to model human behavior in ways that reflect the special nature of sociality and individuality. The textbook continues its in-depth look at cooperation in teams, agent-based simulations, experimental economics, the evolution and diffusion of preferences, and the connection between biology and economics. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/adap-org/9907007.pdf"> Resource sharing and coevolution in evolving cellular automata </a>by J. Werfel, M. Mitchell, and J. P. Crutchfield. IEEE Transactions on Evolutionary Computation, 4(4):388, November 2000. <a href="link">  </a> </summary> Evolving one-dimensional cellular automata (CAs) with genetic algorithms has provided insight into how improved performance on a task requiring global coordination emerges when only local interactions are possible. Two approaches that can affect the search efficiency of the genetic algorithm are coevolution, in which a population of problems—in our case, initial configurations of the CA lattice—evolves along with the population of CAs; and resource sharing, in which a greater proportion of a limited fitness resource is assigned to those CAs which correctly solve problems that fewer other CAs in the population can solve. Here we present evidence that, in contrast to what has been suggested elsewhere, the improvements observed when both techniques are used together depend largely on resource sharing alone. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Kam-Chuen-Jim/publication/12106374_Talking_Helps_Evolving_Communicating_Agents_for_the_Predator-Prey_Pursuit_Problem/links/564cadcf08aedda4c13435ff/Talking-Helps-Evolving-Communicating-Agents-for-the-Predator-Prey-Pursuit-Problem.pdf"> Talking Helps: Evolving Communicating Agents for the Predator-Prey Pursuit Problem </a>by Kam-Chuen Jim, C. Lee Giles. Artifical life, 2000. <a href="link">  </a> </summary> We analyze a general model of multi-agent communication in which all agents communicate simultaneously to a message board. A genetic algorithm is used to evolve multi-agent languages for the predator agents in a version of the predator-prey pursuit problem. We show that the resulting behavior of the communicating multi-agent system is equivalent to that of a Mealy finite state machine whose states are determined by the agents’ usage of the evolved language. Simulations show that the evolution of a communication language improves the performance of the predators. Increasing the language size (and thus increasing the number of possible states in the Mealy machine) improves the performance even further. Furthermore, the evolved communicating predators perform significantly better than all previous work on similar preys. We introduce a method for incrementally increasing the language size which results in an effective coarse-to-fine search that significantly reduces the evolution time required to find a solution. We present some observations on the effects of language size, experimental setup, and prey difficulty on the evolved Mealy machines. In particular, we observe that the start state is often revisited, and incrementally increasing the language size results in smaller Mealy machines. Finally, a simple rule is derived that provides a pessimistic estimate on the minimum language size that should be used for any multi-agent problem. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/3-540-45356-3.pdf"> A Game-Theoretic Approach to the Simple Coevolutionary Algorithm </a>by Sevan G. Ficici, Jordan B. Pollack. LNCS, 2000. <a href="link">  </a> </summary> The fundamental distinction between ordinary evolutionary algorithms (EA) and co-evolutionary algorithms lies in the interaction between coevolving entities. We believe that this property is essentially game-theoretic in nature. Using game theory, we describe extensions that allow familiar mixing-matrix and Markov-chain models of EAs to address coevolutionary algorithm dynamics. We then employ concepts from evolutionary game theory to examine design aspects of conventional coevolutionary algorithms that are poorly understood. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/AAAI/2000/AAAI00-031.pdf?ref=https://githubhelp.com"> On pruning techniques for multi-player games </a>by N. Sturtevant and R. Korf. In Proceedings of National Conference on Artificial Intelligence (AAAI), pages 201–207, 2000. <a href="link">  </a> </summary> Max-n (Luckhardt and Irani, 1986) is the extension of the minimax backup rule to multi-player games. We have shown that only a limited version of alpha-beta pruning, shallow pruning, can be applied to a max-n search tree. We extend this work by calculating the exact bounds needed to use this pruning technique. In addition, we show that branch-and-bound pruning, using a monotonic heuristic, has the same limitations as alpha-beta pruning in a max-n tree. We present a hybrid of these algorithms, alpha-beta branch-and-bound pruning, which combines a monotonic heuristic and backed-up values to prune even more effectively. We also briefly discuss the reduction of a n-player game to a "paranoid" 2-player game. In Sergeant Major, a 3-player card game, we averaged node expansions over 200 height 15 trees. Shallow pruning and branch-and-bound each reduced node expansions by a factor of about 100. Alpha-beta branch-and-bound reduced the expansions by an additional factor of 19. The 2-player reduction was a factor of 3 better than alpha-beta branchand-bound. Using heuristic bounds in the 2-player reduction reduced node expansions another factor of 12. <br> - </details>

<details> <summary> <a href="https://watermark.silverchair.com/282794.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsYwggLCBgkqhkiG9w0BBwagggKzMIICrwIBADCCAqgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMBRcVXHaCrhh9DPsJAgEQgIICecvIjI-AyHPqDEtoqxG45etk8Kr3qZnjlgYcF9HmV5Zgk3mbn6FJ-OzHZWLMlN-KdRwXGkIjDVCEOB8A-qFzdZz484BYJeQqM-d1usHshJkVd8nuz2mGVsBsS1PT9YPlw68yuUWF1UzLK41IyXZmZk4Pu58zMpQSken2DXuBZzC5R0btcbkHVhC_PuRR2Empi2m-xjW8dxWV5R3sbNiDnQQFLjm7BZZXTEE1qV1GvUCW0DdIDhcWJpa-LP6NZ1G0Evo4nlcVC4h1ZNzgizSe5oKTU3hJjeltMD35akait35q5EiPh6xMFqhykSJAAu-krMio05VNfHqAZ-n9vrW5g2W6z38eSBMCPmOx4VaLh9-vCyxHTIosjdbrsKISJ1F2t1AZitrUPniIOLr07aPZBVQHT1lxegrDKEQ-CzvB5Zg7cyVgyBRHzYKswHkCNij_3teAAB7Wi8DSmJWVKOovllkEuXayDtwEy8cktVZsgCGunE9Mz1wZKB0Pj6UGiccWtbGTS63nTNBgFh4ZntPAChV6x3olsXbzqcn0snIVvDtUzS_ziOyIGSD1WrHcZCQUFcWiZXPg059UQHKxunkIJCoZTqEhq5Aicg0MBOG7RPfBMIMv4GZtEPtNRuo3Ax1kFbdmlN2_FwnbSAtMYnh5an1Z3SjVeWkKCko2FBP6yigjm2vOIJBs0Q1s3sCNAFIoa7fSIH2jgmykzWsFQdRe-0ke7GHPs0UhzzDIYogT569IVE7FUeyY3lAHMFTcH3k8XlSCtnCu84J3nXoghqfHArjcDNZnTEg9mxu5W17_tnJzHhuU-viP7hzQu0gFaej7yaaCLRKkClTkuQ"> Evolution of biological information </a>by Thomas D. Schneider. Nucleic Acids Research, 2000. <a href="link">  </a> </summary> How do genetic systems gain information by evolutionary processes? Answering this question precisely requires a robust, quantitative measure of information. Fortunately, 50 years ago Claude Shannon defined information as a decrease in the uncertainty of a receiver. For molecular systems, uncertainty is closely related to entropy and hence has clear connections to the Second Law of Thermodynamics. These aspects of information theory have allowed the development of a straightforward and practical method of measuring information in genetic control systems. Here this method is used to observe information gain in the binding sites for an artificial ‘protein’ in a computer simulation of evolution. The simulation begins with zero information and, as in naturally occurring genetic systems, the information measured in the fully evolved binding sites is close to that needed to locate the sites in the genome. The transition is rapid, demonstrating that information gain can occur by punctuated equilibrium. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/John-Sauter-2/publication/2555126_Tuning_Synthetic_Pheromones_With_Evolutionary_Computing/links/0a85e533ecde553ef1000000/Tuning-Synthetic-Pheromones-With-Evolutionary-Computing.pdf"> Tuning Synthetic Pheromones With Evolutionary Computing </a>by J Sauter, HVD Parunak and S Brueckner. ECOMAS, 2001. <a href="link">  </a> </summary> Agents guided by synthetic pheromones can imitate the dynamics of insects. These systems are well suited to problems such as the control of unmanned robotic vehicles. We have developed a model for controlling robotic vehicles in air combat missions using synthetic pheromones. In the course of our experimentation, we have identified the need for proper tuning of the algorithms to get acceptable performance. We describe pheromones in natural andsynthetic systems, and describe the mechanisms we have developed. The role of evolutionary computing in offlineand online tuning is discussed. <br> - </details>

<details> <summary> <a href="https://eprints.soton.ac.uk/262011/1/watson_cdms_gecco_2001.pdf"> Coevolutionary dynamics in a minimal substrate </a>by R. Watson and J. Pollack. Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), 2001. <a href="link">  </a> </summary> One of the central difficulties of coevolutionary methods arises from "intransitive superiority" -- in a two-player game, for example, the fact that A beats B, and B beats C, does not exclude the possibility that C beats A. Such cyclic superiority in a coevolutionary substrate is hypothesized to cause cycles in the dynamics of the population such that it "chases its own tail" - traveling through some part of strategy space more than once despite apparent improvement with each step. It is often difficult to know whether an application domain contains such difficulties and to verify this hypothesis in the failure of a given coevolutionary set-up. In this paper we wish to elucidate some of the issues and concepts in an abstract domain where the dynamics of coevolution can be studied simply and directly. We define three simple "number games" that illustrate intransitive superiority and resultant oscillatory dynamics, as well as some other relevant concepts. These include the distinction between a player's perceived performance and performance with respect to an external metric, and the significance of strategies with a multidimensional nature. These features alone can also cause oscillatory behavior and coevolutionary failure. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Lee-Spector-2/publication/2549787_Evolutionary_Dynamics_Discovered_via_Visualization_in_the_BREVE_Simulation_Environment/links/604ce174a6fdcccfee7cefd2/Evolutionary-Dynamics-Discovered-via-Visualization-in-the-BREVE-Simulation-Environment.pdf"> Evolutionary dynamics discovered via visualization in the breve simulation environment </a>by L. Spector and J. Klein. In Workshop Proceedings of the 8th International Conference on the Simulation and Synthesis of Living Systems, pages 163–170, 2002. <a href="link">  </a> </summary> We report how breve, a simulation environment with rich 3d graphics, was used to discover significant patterns in the dynamics of a system that evolves controllers for swarms of goal-directed agents. These patterns were discovered via visualization in the sense that we had not considered their relevance or thought to look for them initially, but they became obvious upon visually observing the behavior of the system. In this paper we briefly describe breve and the system of evolving swarms that we implemented within it. We then describe two discovered properties of the evolutionary dynamics of the system: transitions to/from genetic drift regimes and the emergence of collective or multicellular organization. We comment more generally on the utility of 3d visualization for the discovery of biologically significant phenomena and briefly describe our ongoing work in this area. Pointers are provided to on-line resources including source code and animations that demonstrate several of the described effects. Associated links are available on-line at http://hampshire.edu/lspector/alife8-visualization.html. <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/30801544/Tuybnaic02-with-cover-page-v2.pdf?Expires=1668008453&Signature=D2CzKtWwSHhSIlxHOl7NcfNf3Be7IomezobJzZPLDBT2~3nA28zKf7xPQkQq13XFBfGgEIhx3IvzL9OHu4abVVSf9TxdFZwCaNg7JODf81a8~bBg2y9CITtTYBtmpw8gxQw9mXc4dpHBEc9dKwjLi18zC47x2e9gr4ZX3uYeRu6JflBxR6FmqwvlNzR4VxPvTv0DwgKdnALkVedwDLaGUlE7iQEd5VQgNhy8ZF-76bZ8qhGWv4FNdrFY5bjVAbJ2nz4vYcM2AAc6qNE~if9VjBARd1hkg0-3U7WLUDk2UnRzBz2rn9Z7ra75pN2MQB0VtpXAQHuh8gG5Od~MBOIfuA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> Towards a relation between learning agents and evolutionary dynamics </a>by Karl Tuyls, Tom Lenaerts, Katja Verbeeck, Sam Maes. BNAIC, 2002. <a href="link">  </a> </summary> Modeling learning agents in the context of Multi-agent Systems requires insight in the type and form of interactions with the environment and other agents in the system. Usually, these agents are modeled similar to the different players in a standard game theoretical model. In this paper we examine whether evolutionary game theory, and more specifically the replicator dynamics, is an adequate theoretical model for the study of the dynamics of reinforcement learning agents in a multi-agent system. As a first step in this direction we extend the results of [1, 9] to a more general reinforcement learning framework, i.e. Learning Automata. <br> - </details>

<details> <summary> <a href="https://www.ijcai.org/Proceedings/03/Papers/095.pdf"> When Evolving Populations is Better than Coevolving Individuals: The Blind Mice Problem </a>by Thomas Miconi. IJCAI, 2003. <a href="link">  </a> </summary> This paper is about the evolutionary design of multi-agent systems. An important part of recent research in this domain has been focusing on collaborative revolutionary methods. We expose possible drawbacks of these methods, and show that for a non-trivial problem called the "blind mice" problem, a classical GA approach in which whole populations are evaluated, selected and crossed together (with a few tweaks) finds an elegant and non-intuitive solution more efficiently than cooperative coevolution. The difference in efficiency grows with the number of agents within the simulation. We propose an explanation for this poorer performance of cooperative coevolution, based on the intrinsic fragility of the evaluation process. This explanation is supported by theoretical and experimental arguments.  <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/publication/225679709_Emergence_of_Collective_Behavior_in_Evolving_Populations_of_Flying_Agents"> Emergence of Collective Behavior in Evolving Populations of Flying Agents </a>by L. Spector, J. Klein, C. Perry, and M. Feinstein. Genetic and Evolutionary Computation Conference, Chicago, IL, USA, July 12-16, 2003. <a href="link">  </a> </summary> We demonstrate the emergence of collective behavior in two evolutionary computation systems, one an evolutionary extension of a classic (highly constrained) flocking algorithm and the other a relatively un-constrained system in which the behavior of agents is governed by evolved computer programs. We describe the systems in detail, document the emergence of collective behavior, and argue that these systems present new opportunities for the study of group dynamics in an evolutionary context.  <br> - </details>

<details> <summary> <a href="http://www.tesseract.org/paul/papers/gecco03-ccea.pdf"> Exploring the Explorative Advantage of the Cooperative Coevolutionary (1+1) EA </a>by Thomas Jansen, R. Paul Wiegand. GECCO, 2003. <a href="link">  </a> </summary> Using a well-known cooperative coevolutionary function optimization framework, a very simple cooperative coevolutionary (1+1) EA is defined. This algorithm is investigated in the context of expected optimization time. The focus is on the impact the cooperative coevolutionary approach has and on the possible advantage it may have over more traditional evolutionary approaches. Therefore, a systematic comparison between the expected optimization times of this coevolutionary algorithm and the ordinary (1+1) EA is presented. The main result is that separability of the objective function alone is is not sufficient to make the cooperative coevolutionary approach beneficial. By presenting a clear structured example function and analyzing the algorithms’ performance, it is shown that the cooperative coevolutionary approach comes with new explorative possibilities. This can lead to an immense speed-up of the optimization. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Karl-Tuyls/publication/221471451_On_a_Dynamical_Analysis_of_Reinforcement_Learning_in_Games_Emergence_of_Occam%27s_Razor/links/0c9605203e5ba30157000000/On-a-Dynamical-Analysis-of-Reinforcement-Learning-in-Games-Emergence-of-Occams-Razor.pdf"> On a Dynamical Analysis of Reinforcement Learning in Games: Emergence of Occam’s Razor </a>by Karl Tuyls, Katja Verbeeck, Sam Maes. Lecture Notes in Computer Science, 2003. <a href="link">  </a> </summary> Modeling learning agents in the context of Multi-agent Systems requires an adequate understanding of their dynamic behaviour. Usually, these agents are modeled similar to the different players in a standard game theoretical model. Unfortunately traditional Game Theory is static and limited in its usefelness. Evolutionary Game Theory improves on this by providing a dynamics which describes how strategies evolve over time. In this paper, we discuss three learning models whose dynamics are related to the Replicator Dynamics(RD). We show how a classical Reinforcement Learning(RL) technique, i.e. Qlearning relates to the RD. This allows to better understand the learning process and it allows to determine how complex a RL model should be. More precisely, Occam’s Razor applies in the framework of games, i.e. the simplest model (Cross) suffices for learning equilibria. An experimental verification in all three models is presented. <br> - </details>

<details> <summary> <a href="https://ochicken.top/Library/Mathematics/Dynamical_Systems/Nonlinear_and_Chaos_%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%92%8C%E6%B7%B7%E6%B2%8C/smale-def-eq.pdf"> Differential Equations, Dynamical Systems, and an Introduction to Chaos </a>by MW Hirsch, S Smale, RL Devaney. Elsevier, 2003. <a href="link">  </a> </summary> Hirsch, Devaney, and Smale's classic "Differential Equations, Dynamical Systems, and an Introduction to Chaos" has been used by professors as the primary text for undergraduate and graduate level courses covering differential equations. It provides a theoretical approach to dynamical systems and chaos written for a diverse student population among the fields of mathematics, science, and engineering. Prominent experts provide everything students need to know about dynamical systems as students seek to develop sufficient mathematical skills to analyze the types of differential equations that arise in their area of study. The authors provide rigorous exercises and examples clearly and easily by slowly introducing linear systems of differential equations. Calculus is required as specialized advanced topics not usually found in elementary differential equations courses are included, such as exploring the world of discrete dynamical systems and describing chaotic systems. This is a classic text by three of the world's most prominent mathematicians. It continues the tradition of expository excellence. It contains updated material and expanded applications for use in applied studies. <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/30888627/p693-with-cover-page-v2.pdf?Expires=1668008714&Signature=c6qzlgxHi0g~03AfAJcY0D1i7Pi0TbjmgXXrMbaE-mYV52qJJLFcEMYIjkqj59wHIJyC69sEI6dMenm8neQP9IORV-tpCNigyRWlRS5b8WL4gIFLqBodbIlr10KlLaY6~zNRHY-shOzVixDraeDdIT3qCqh-Kjb~S3uSnoIRRgKSV8p5XzeW1SAnJEcXRnM1ZZY6VTWiVejZoH02f-g9Tx1LiDmvp8XI2FJK0FuMrR-iFpcFcafc44q8bSu9HxhPBW3OPll4~vT4H3U~dMyv2h-lo-vp6gmuoQLPpym~Gc1lqBQsQb8ngo0ZDEdESlUz-ayTFD8CS6cgWm17HHO~IQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> A selection-mutation model for Q-learning in multi-agent systems </a>by Karl Tuyls, Katja Verbeeck, Tom Lenaerts. AAMAS, 2003. <a href="link">  </a> </summary> Although well understood in the single-agent framework, the use of traditional reinforcement learning (RL) algorithms in multi-agent systems (MAS) is not always justified. The feedback an agent experiences in a MAS, is usually influenced by the other agents present in the system. Multi agent environments are therefore non-stationary and convergence and optimality guarantees of RL algorithms are lost. To better understand the dynamics of traditional RL algorithms we analyze the learning process in terms of evolutionary dynamics. More specifically we show how the Replicator Dynamics (RD) can be used as a model for Q-learning in games. The dynamical equations of Q-learning are derived and illustrated by some well chosen experiments. Both reveal an interesting connection between the exploitationexploration scheme from RL and the selection-mutation mechanisms from evolutionary game theory. <br> - </details>

<details> <summary> <a href="https://cs.gmu.edu/~lpanait/papers/panait03improving.pdf"> Improving Coevolutionary Search for Optimal Multiagent Behaviors </a>by Liviu Panait, R. Paul Wiegand, Sean Luke. IJCAI, 2003. <a href="link">  </a> </summary> Evolutionary computation is a useful technique for learning behaviors in multiagent systems. Among the several types of evolutionary computation, one natural and popular method is to coevolve multiagent behaviors in multiple, cooperating populations. Recent research has suggested that coevolutionary systems may favor stability rather than performance in some domains. In order to improve upon existing methods, this paper examines the idea of modifying traditional coevolution, biasing it to search for maximal rewards. We introduce a theoretical justification of the improved method and present experiments in three problem domains. We conclude that biasing can help coevolution find better results in some multiagent problem domains. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-540-39857-8_38.pdf"> Extended Replicator Dynamics as a Key to Reinforcement Learning in Multi-agent Systems </a>by Karl Tuyls, Dries Heytens, Ann Nowe, Bernard Manderick.  ECML, 2003. <a href="link">  </a> </summary> Modeling learning agents in the context of Multi-agent Systems requires an adequate understanding of their dynamic behaviour. Evolutionary Game Theory provides a dynamics which describes how strategies evolve over time. B¨orgers et al. [1] and Tuyls et al. [11] have shown how classical Reinforcement Learning (RL) techniques such as Cross-learning and Q-learning relate to the Replicator Dynamics (RD). This provides a better understanding of the learning process. In this paper, we introduce an extension of the Replicator Dynamics from Evolutionary Game Theory. Based on this new dynamics, a Reinforcement Learning algorithm is developed that attains a stable Nash equilibrium for all types of games. Such an algorithm is lacking for the moment. This kind of dynamics opens an interesting perspective for introducing new Reinforcement Learning algorithms in multi-state games and MultiAgent Systems. <br> - </details>

<details> <summary> <a href="http://www.tesseract.org/paul/papers/Panait2004ppsn-final.pdf"> A Visual Demonstration of Convergence Properties of Cooperative Coevolution </a>by Liviu Panait, R. Paul Wiegand, Sean Luke. PPSN, 2004. <a href="link">  </a> </summary> We introduce a model for cooperative coevolutionary algorithms (CCEAs) using partial mixing, which allows us to compute the expected long-run convergence of such algorithms when individuals’ fitness is based on the maximum payoff of some N evaluations with partners chosen at random from the other population. Using this model, we devise novel visualization mechanisms to attempt to qualitatively explain a difficult-to-conceptualize pathology in CCEAs: the tendency for them to converge to suboptimal Nash equilibria. We further demonstrate visually how increasing the size of N, or biasing the fitness to include an ideal-collaboration factor, both improve the likelihood of optimal convergence, and under which initial population configurations they are not much help. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-3-540-30115-8_18.pdf"> Analyzing Multi-agent Reinforcement Learning Using Evolutionary Dynamics </a>by ’t Hoen, P.J., Tuyls, K. ECML, 2004. <a href="link">  </a> </summary> In this paper, we show how the dynamics of Q-learning can be visualized and analyzed from a perspective of Evolutionary Dynamics (ED). More specifically, we show how ED can be used as a model for Qlearning in stochastic games. Analysis of the evolutionary stable strategies and attractors of the derived ED from the Reinforcement Learning (RL) application then predict the desired parameters for RL in MultiAgent Systems (MASs) to achieve Nash equilibriums with high utility. Secondly, we show how the derived fine tuning of parameter settings from the ED can support application of the COllective INtelligence (COIN) framework. COIN is a proved engineering approach for learning of cooperative tasks in MASs. We show that the derived link between ED and RL predicts performance of the COIN framework and visualizes the incentives provided in COIN toward cooperative behavior. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/978-1-4419-8909-3.pdf"> Selection in Coevolutionary Algorithms and the Inverse Problem </a>by Sevan Ficici, Ofer Melnik, Jordan Pollack. Springer, 2004. <a href="link">  </a> </summary> The inverse problem in the collective intelligence framework concerns how the private utility functions of agents can be engineered so that their selfish behaviors collectively give rise to a desired world state. In this chapter we examine several selection and fitnesssharing methods used in coevolution and consider their operation with respect to the inverse problem. The methods we test are truncation and linear-rank selection and competitive and similarity-based fitness sharing. Using evolutionary game theory to establish the desired world state, our analyses show that variable-sum games with polymorphic Nash are problematic for these methods. Rather than converge to polymorphic Nash, the methods we test produce cyclic behavior, chaos, or attractors that lack game-theoretic justification and therefore fail to solve the inverse problem. The private utilities of the evolving agents may thus be viewed as poorly factored—improved private utility does not correspond to improved world utility. <br> - </details>

<details> <summary> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1555000&casa_token=CmWmYz9F4AUAAAAA:7bw2f3NZZdUZlt1kFoEIWEVVD__xdLeBOvnp6Ebemfmkgj4wbhX5ntu2JREjL0cMdb0EmQ7gYpdW9qI&tag=1"> On Social Learning and Robust Evolutionary Algorithm Design in Economic Games </a>by Floortje Alkemade, Han La Poutre, Hans Amman. Congress on Evolutionary Computation, 2005. <a href="link">  </a> </summary> Agent-based computational economics (ACE) combines elements from economics and computer science. In this paper, we focus on the relation between the evolutionary technique that is used and the economic problem that is modeled. In the field of ACE, economic simulations often derive parameter settings for the genetic algorithm directly from the values of the economic model parameters. In this paper we compare two important approaches that are dominating in ACE and show that the above practice may hinder the performance of the GA and thereby hinder agent learning. More specifically, we show that economic model parameters and evolutionary algorithm parameters should be treated separately by comparing the two widely used approaches to social learning with respect to their convergence properties and robustness. This leads to new considerations for the methodological aspects of evolutionary algorithm design within the field of ACE. We also present improved social (ACE) simulation results for the Cournot oligopoly game, yielding (higher profit) Cournot-Nash equilibria instead of the competitive equilibria. <br> - </details>

<details> <summary> <a href="http://nozdr.ru/data/media/biblio/kolxoz/Cs/CsLn/L/Learning%20and%20Adaption%20in%20Multi-Agent%20Systems,%201%20conf.,%20LAMAS%202005(LNCS3898,%20Springer,%202006)(ISBN%203540330534)(224s).pdf#page=163"> The Success and Failure of Tag-Mediated Evolution of Cooperation </a>by Austin McDonald, Sandip Sen. LAMAS, 2005. <a href="link">  </a> </summary> Use of tags to limit partner selection for playing has been shown to produce stable cooperation in agent populations playing the Prisoner’s Dilemma game. There is, however, a lack of understanding of how and why tags facilitate such cooperation. We start with an empirical investigation that identifies the key dynamics that result in sustainable cooperation in PD. Sufficiently long tags are needed to achieve this effect. A theoretical analysis shows that multiple simulation parameters including tag length, mutation rate and population size will have significant effect on sustaining cooperation. Experiments partially validate these observations. Additionally, we claim that tags only promote mimicking and not coordinated behavior in general, i.e., tags can promote cooperation only if cooperation requires identical actions from all group members. We illustrate the failure of the tag model to sustain cooperation by experimenting with domains where agents need to take complementary actions to maximize payoff <br> - </details>


<br/>

### Exploration

<details> <summary> <a href="http://www0.cs.ucl.ac.uk/staff/Rob.Smith/othgray.pdf"> Co-adaptive genetic algorithms: An example in othello strategy </a>by R. Smith and B. Gray. University of Alabama, 1993. <a href="link">  </a> </summary> This paper focuses on co-adaptive GAs, where population members are independent, and adaptation depends on the evolving population context. Recent research on co-adaptive GAs is reviewed. As an example of co-adaptation, a system of Othelo strategy acquisition is presented. Preliminary results illustrate how a co-adaptive GA can continually explore the space of Orthelo strategies. This exploration yields insight into strategy interactions in Orthelo. Avenues for future analysis and experimentation with co-adaptive GAs are discussed. <br> - </details>

<details> <summary> <a href="http://nozdr.ru/data/media/biblio/kolxoz/Cs/CsLn/L/Learning%20and%20Adaption%20in%20Multi-Agent%20Systems,%201%20conf.,%20LAMAS%202005(LNCS3898,%20Springer,%202006)(ISBN%203540330534)(224s).pdf#page=173"> An Adaptive Approach for the Exploration-Exploitation Dilemma and Its Application to Economic Systems </a>by Lilia Rejeb, Zahia Guessoum, Rym MHallah. LAMAS, 2005. <a href="link">  </a> </summary> Learning agents have to deal with the exploration-exploitation dilemma. The choice between exploration and exploitation is very difficult in dynamic systems; in particular in large scale ones such as economic systems. Recent research shows that there is neither an optimal nor a unique solution for this problem. In this paper, we propose an adaptive approach based on metarules to adapt the choice between exploration and exploitation. This new adaptive approach relies on the variations of the performance of the agents. To validate the approach, we apply it to economic systems and compare it to two adaptive methods originally proposed by Wilson: one local and one global. Moreover, we compare different exploration strategies and focus on their influence on the performance of the agents. <br> - </details>

<br/>

### Extensive Form Games

<details> <summary> <a href="https://www.tau.ac.il/~samet/papers/learning-to-play.pdf">  Learning to play games in extensive form by valuation </a>by Phillipe Jehiel, Dov Samet. NAJ Economics, 2001. <a href="link">  </a> </summary> Game theoretic models of learning which are based on the strategic form of the game cannot explain learning in games with large extensive form. We study learning in such games by using valuation of moves. A valuation for a player is a numeric assessment of her moves that purports to reflect their desirability. We consider a myopic player, who chooses moves with the highest valuation. Each time the game is played, the player revises her valuation by assigning the payoff obtained in the play to each of the moves she has made. We show for a repeated win–lose game that if the player has a winning strategy in the stage game, there is almost surely a time after which she always wins. When a player has more than two payoffs, a more elaborate learning procedure is required. We consider one that associates with each move the average payoff in the rounds in which this move was made. When all players adopt this learning procedure, with some perturbations, then, with probability 1 there is a time after which strategies that are close to subgame perfect equilibrium are played. A single player who adopts this procedure can guarantee only her individually rational payoff <br> - </details>

<br/>

### Fictitious Play

<details> <summary> <a href="https://dash.harvard.edu/bitstream/handle/1/3198694/fudenberg_consistency.pdf?sequence=2&origin=publicationDetail"> Consistency and Cautious Fictitious Play </a>by Drew Fudenberg, David K. Levine. Economic Dynamics and Control, 1995. <a href="link">  </a> </summary> We study a variation of fictitious play, in which the probability of each action is an exponential function of that action’s utility against the historical frequency of opponents’ play. Regardless of the opponents’ strategies, the utility received by an agent using this rule is nearly the best that could be achieved against the historical frequency. Such rules are approximately optimal in i.i.d. environments, and guarantee nearly the minmax regardless of opponents’ behavior. Fictitious play shares these properties provided it switches “infrequently” between actions. We also study the long run outcomes when all players use consistent and cautious rules <br> - </details>

<br/>

### Foundational Theory

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/neumann44a.pdf"> Theory of Games and Economic Behaviour </a>by von Neumann, J., Morgenstern, O. Princeton University Press, 1944. <a href="link">  </a> </summary> This is the classic work upon which modern-day game theory is based. What began more than sixty years ago as a modest proposal that a mathematician and an economist write a short paper together blossomed, in 1944, when Princeton University Press published Theory of Games and Economic Behavior. In it, John von Neumann and Oskar Morgenstern conceived a groundbreaking mathematical theory of economic and social organization, based on a theory of games of strategy. Not only would this revolutionize economics, but the entirely new field of scientific inquiry it yielded--game theory--has since been widely used to analyze a host of real-world phenomena from arms races to optimal policy choices of presidential candidates, from vaccination policy to major league baseball salary negotiations. And it is today established throughout both the social sciences and a wide range of other sciences. <br> - </details>

<details> <summary> <a href="https://gtl.csa.iisc.ac.in/gametheory/Classics/NCG.pdf"> Non-cooperative games </a>by J. Nash. Annals of Mathematics, 1951. <a href="link">  </a> </summary> Theory of n-person games called cooperative in the absence of coalitions. <br> - </details>

<details> <summary> <a href="http://www3.ub.tu-berlin.de/ihv/001686626.pdf"> Game Theory: Analysis of Conflict </a>by Myerson, R. Harvard University Press, 1991. <a href="link">  </a> </summary> Eminently suited to classroom use as well as individual study, Roger Myerson's introductory text provides a clear and thorough examination of the models, solution concepts, results, and methodological principles of noncooperative and cooperative game theory. Myerson introduces, clarifies, and synthesizes the extraordinary advances made in the subject over the past fifteen years, presents an overview of decision theory, and comprehensively reviews the development of the fundamental models: games in extensive form and strategic form, and Bayesian games with incomplete information. Game Theory will be useful for students at the graduate level in economics, political science, operations research, and applied mathematics. Everyone who uses game theory in research will find this book essential. <br> - </details>

<details> <summary> <a href="https://arielrubinstein.tau.ac.il/books/GT.pdf"> A Course in Game Theory </a>by Martin J. Osborne and Ariel Rubinstein.  MIT Press, 1994. <a href="link">  </a> </summary> A Course in Game Theory presents the main ideas of game theory at a level suitable for graduate students and advanced undergraduates, emphasizing the theory's foundations and interpretations of its basic concepts. The authors provide precise definitions and full proofs of results, sacrificing generalities and limiting the scope of the material in order to do so. The text is organized in four parts: strategic games, extensive games with perfect information, extensive games with imperfect information, and coalitional games. It includes over 100 exercises. <br> - </details>

<details> <summary> <a href="https://www.emse.fr/~beaune/maml/sen-weiss-MAL99.pdf"> Learning in Multiagent Systems </a>by Sandip Sen, Gerhard Weiss. MIT Press, 1999. <a href="link">  </a> </summary> Chapter 6 of the book. <br> - </details>

<details> <summary> <a href="https://epdf.tips/the-theory-of-learning-in-games.html"> The Theory of Learning in Games </a>by Drew Fudenberg, David K. Levine. MIT Press, 1999. <a href="link">  </a> </summary> In economics, most noncooperative game theory has focused on equilibrium in games, especially Nash equilibrium and its refinements. The traditional explanation for when and why equilibrium arises is that it results from analysis and introspection by the players in a situation where the rules of the game, the rationality of the players, and the players' payoff functions are all common knowledge. Both conceptually and empirically, this theory has many problems. In The Theory of Learning in Games Drew Fudenberg and David Levine develop an alternative explanation that equilibrium arises as the long-run outcome of a process in which less than fully rational players grope for optimality over time. The models they explore provide a foundation for equilibrium theory and suggest useful ways for economists to evaluate and modify traditional equilibrium concepts. <br> - </details>

<details> <summary> <a href="http://www.misserpirat.dk/main/docs/00000013.pdf"> Algorithms, Games, and the Internet </a>by
Christos H. Papadimitrio. STOC, 2001. <a href="link">  </a> </summary> If the Internet is the next great subject for Theoretical Computer Science to model and illuminate mathematically, then Game Theory, and Mathematical Economics more generally, are likely to prove useful tools. In this talk I survey some opportunities and challenges in this important frontier. <br> - </details>

<br/>

### General-Sum Games

<details> <summary> <a href="https://webdocs.cs.ualberta.ca/~bowling/papers/00icml.pdf"> Convergence Problems of General-Sum Multiagent Reinforcement Learning </a>by Michael Bowling. ICML, 2000. </summary> Stochastic games are a generalization of MDPs to multiple agents, and can be used as a framework for investigating multiagent learning. Hu and Wellman (1998) recently proposed a multiagent Q-learning method for general-sum stochastic games. In addition to describing the algorithm, they provide a proof that the method will converge to a Nash equilibrium for the game under specified conditions. The convergence depends on a lemma stating that the iteration used by this method is a contraction mapping. Unfortunately the proof is incomplete. In this paper we present a counterexample and flaw to the lemma’s proof. We also introduce strengthened assumptions under which the lemma holds, and examine how this affects the classes of games to which the theoretical result can be applied  <br> - </details>

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/littman01a.pdf"> Friend-or-foe Q-learning in general-sum games </a>by Michael L. Littman. ICML, 2001. <a href="link">  </a> </summary> This paper describes an approach to reinforcement learning in multiagent general-sum games in which a learner is told to treat each other agent as either a friend" or foe". This Q-learning-style algorithm provides strong convergence guarantees compared to an existing Nash-equilibrium-based learning rule. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-02/SS02-02-012.pdf"> Correlated-Q Learning </a>by Amy Greenwald, Keith Hall, Roberto Serrano. NeurIPS workshop on multi-agent learning, 2002. <a href="link">  </a> </summary> Bowling named two desiderata for multiagent learning algorithms: rationality and convergence. This paper introduces correlated-Q learning, a natural generalization of Nash-Q and FF-Q that satisfies these criteria. Nash-Q satisfies rationality, but in general it does not converge. FF-Q satisfies convergence, but in general it is not rational. Correlated-Q satisfies rationality by construction. This papers demonstrates the empirical convergence of correlated-Q on a standard testbed of general-sum Markov games. <br> - </details>

<details> <summary> <a href="https://www.jmlr.org/papers/volume4/hu03a/hu03a.pdf"> Nash Q-Learning for General-Sum Stochastic Games </a>by Junling Hu, Michael Wellman. JMLR, 2003. <a href="link">  </a> </summary> We extend Q-learning to a noncooperative multiagent context, using the framework of general-sum stochastic games. A learning agent maintains Q-functions over joint actions, and performs updates based on assuming Nash equilibrium behavior over the current Q-values. This learning protocol provably converges given certain restrictions on the stage games (defined by Q-values) that arise during learning. Experiments with a pair of two-player grid games suggest that such restrictions on the game structure are not necessarily required. Stage games encountered during learning in both grid environments violate the conditions. However, learning consistently converges in the first grid game, which has a unique equilibrium Q-function, but sometimes fails to converge in the second, which has three different equilibrium Q-functions. In a comparison of offline learning performance in both games, we find agents are more likely to reach a joint optimal path with Nash Q-learning than with a single-agent Q-learning method. When at least one agent adopts Nash Q-learning, the performance of both agents is better than using single-agent Q-learning. We have also implemented an online version of Nash Q-learning that balances exploration with exploitation, yielding improved performance. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Sandip-Sen-2/publication/2831062_Towards_a_Pareto-optimal_solution_in_general-sum_games/links/54dbfb250cf28d3de65e2dc8/Towards-a-Pareto-optimal-solution-in-general-sum-games.pdf"> Towards a Pareto Optimal Solution in general-sum games </a>by Sandip Sen, Stephane Airiau, Rajatish Mukherjee. AAMAS, 2003. <a href="link">  </a> </summary> Multiagent learning literature has investigated iterated two-player games to develop mechanisms that allow agents to learn to converge on Nash Equilibrium strategy profiles. Such equilibrium configuration implies that there is no motivation for one player to change its strategy if the other does not. Often, in general sum games, a higher payoff can be obtained by both players if one chooses not to respond optimally to the other player. By developing mutual trust, agents can avoid iterated best responses that will lead to a lesser payoff Nash Equilibrium. In this paper we work with agents who select actions based on expected utility calculations that incorporates the observed frequencies of the actions of the opponent(s). We augment this stochastically-greedy agents with an interesting action revelation strategy that involves strategic revealing of one's action to avoid worst-case, pessimistic moves. We argue that in certain situations, such apparently risky revealing can indeed produce better payoff than a non-revealing approach. In particular, it is possible to obtain Pareto-optimal solutions that dominate Nash Equilibrium. We present results over a large number of randomly generated payoff matrices of varying sizes and compare the payoffs of strategically revealing learners to payoffs at Nash equilibrium. <br> - </details>

<br/>

### Hierachical Learning

<details> <summary> <a href="http://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/whitesonaamas03.pdf"> Concurrent layered learning </a>by S. Whiteson and P. Stone. AAMAS, 2003. <a href="link">  </a> </summary> Hierarchies are powerful tools for decomposing complex control tasks into manageable subtasks. Several hierarchical approaches have been proposed for creating agents that can execute these tasks. Layered learning is such a hierarchical paradigm that relies on learning the various subtasks necessary for achieving the complete high-level goal. Layered learning prescribes training low-level behaviors (those closer to the environmental inputs prior to high-level behaviors. In past implementations these lower-level behaviors were always frozen before advancing to the next layer. In this paper, we hypothesize that there are situations where layered learning would work better were the lower layers allowed to keep learning concurrently with the training of subsequent layers, an approach we call concurrent layered learning. We identify a situation where concurrent layered learning is beneficial and present detailed empirical results verifying our hypothesis. In particular, we use neuro-evolution to concurrently learn two layers of a layered learning approach to a simulated robotic soccer keepaway task. The main contribution of this paper is evidence that there exist situations where concurrent layered learning outperforms traditional layered learning. Thus, we establish that, when using layered learning, the concurrent training of layers can be an effective option. <br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1023/a:1022140919877.pdf"> Recent Advances in Hierarchical Reinforcement Learning </a>by Andrew G. Barto, Sridhar Mahadevan. Discrete Event Dynamic Systems, 2003. <a href="link">  </a> </summary> Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting. <br> - </details>

<details> <summary> <a href="https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1183&context=cs_faculty_pubs"> Learning to Communicate and Act using Hierarchical Reinforcement Learning </a>by Mohammad Ghavamzadeh, Sridhar Mahadevan. AAMAS, 2004. <a href="link">  </a> </summary> In this paper, we address the issue of rational communication behavior among autonomous agents. The goal is for agents to learn a policy to optimize the communication needed for proper coordination, given the communication cost. We extend our previously reported cooperative hierarchical reinforcement learning (HRL) algorithm to include communication decisions and propose a new multiagent HRL algorithm, called COM-Cooperative HRL. In this algorithm, we define cooperative subtasks to be those subtasks in which coordination among agents significantly improves the performance of the overall task. Those levels of the hierarchy which include cooperative subtasks are called cooperation levels. Coordination skills among agents are learned faster by sharing information at the cooperation levels, rather than the level of primitive actions. We add a communication level to the hierarchical decomposition of the problem below each cooperation level. Before making a decision at a cooperative subtask, agents decide if it is worthwhile to perform a communication action. A communication action has a certain cost and provides each agent at a certain cooperation level with the actions selected by the other agents at the same level. We demonstrate the efficacy of the COM-Cooperative HRL algorithm as well as the relation between the communication cost and the learned communication policy using a multiagent taxi domain. <br> - </details>

<br/>

### Incomplete Information Games

<details> <summary> <a href="http://www.ma.huji.ac.il/~zamir/papers/22_IJGT85.pdf"> Formulation of bayesian analysis for games with incomplete information </a>by J-F. Mertens, S. Zamir. International Journal of Game Theory, 1985. <a href="link">  </a> </summary> A formal model is given of Harsanyi's infinite hierarchies of beliefs. It is shown that the model closes with some Bayesian game with incomplete information, and that any such game can be approximated by one with a finite number of states of world. <br> - </details>

<br/>

### No-Regret Learning

<details> <summary> <a href="http://static.cs.brown.edu/people/amy/papers/icml.pdf"> On no-regret learning, fictitious play, and nash equilibrium </a>by Jafari, C., Greenwald, A., Gondek, D., Ercal, G. ICML, 2001. <a href="link">  </a> </summary> This paper addresses the question what is the outcome of multi-agent learning via no-regret algorithms in repeated games? Specifically, can the outcome of no-regret learning be characterized by traditional game-theoretic solution concepts, such as Nash equilibrium? The conclusion of this study is that no-regret learning is reminiscent of fictitious play: play converges to Nash equilibrium in dominancesolvable, constant-sum, and generalsum 2  2 games, but cycles exponentially in the Shapley game. Notably, however, the information required of fictitious play far exceeds that of noregret learning. <br> - </details>

<details> <summary> <a href="https://proceedings.neurips.cc/paper/2004/file/88fee0421317424e4469f33a48f50cb0-Paper.pdf"> Convergence and No-Regret in Multiagent Learning </a>by Michael Bowling. NeurIPS, 2004. <a href="link">  </a> </summary> Learning in a multiagent system is a challenging problem due to two key factors. First, if other agents are simultaneously learning then the environment is no longer stationary, thus undermining convergence guarantees. Second, learning is often susceptible to deception, where the other agents may be able to exploit a learner’s particular dynamics. In the worst case, this could result in poorer performance than if the agent was not learning at all. These challenges are identifiable in the two most common evaluation criteria for multiagent learning algorithms: convergence and regret. Algorithms focusing on convergence or regret in isolation are numerous. In this paper, we seek to address both criteria in a single algorithm by introducing GIGA-WoLF, a learning algorithm for normalform games. We prove the algorithm guarantees at most zero average regret, while demonstrating the algorithm converges in many situations of self-play. We prove convergence in a limited setting and give empirical results in a wider variety of situations. These results also suggest a third new learning criterion combining convergence and regret, which we call negative non-convergence regret (NNR). <br> - </details>

<br/>

### Opponent Modelling

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Spring/2000/SS-00-01/SS00-01-026.pdf"> Learning Models of Other Agents Using Influence Diagrams </a>by Dicky Suryadi, Piotr J. Gmytrasiewicz. AAAI, 2000. <a href="link">  </a> </summary> We adopt decision theory as a descriptive paradigm to model rational agents. We use influence diagrams as a modeling representation of agents, which is used to interact with them and to predict their behavior. In this paper, we provide a framework that an agent can use to learn the models of other agents in a multi-agent system (MAS) based on their observed behavior. Since the correct model is usually not known with certainty our agents maintain a number of possible models and assign a probability to each of them being correct. When none of the available models is likely to be correct, we modify one of them to better account for the observed behaviors. The modification refines the parameters of the influence diagram used to model the other agent’s capabilities, preferences, or beliefs. The modified model is then allowed to compete with the other models and the probability assigned to it being correct can be arrived at based on how well it predicts the behaviors of the other agent already observed. <br> - </details>

<details> <summary> <a href="https://d1wqtxts1xzle7.cloudfront.net/53841950/Learning_Mutual_Trust20170712-17959-yl7mr9-libre.pdf?1499904603=&response-content-disposition=inline%3B+filename%3DLearning_Mutual_Trust.pdf&Expires=1670310190&Signature=ZEnHSv7fmyCN0C0xXlwqMOhEAz2Sxo~KgxCxfCtExJxTl1wTjHQvYvQy~KMgANz5LFt91gTK5FWkY3s1zfFHBptW3VclE2PdBE98kSQUIKTPQZIyfLRDKzEgh8MtpXb8qApbm-JpNTkVRDREfqfbDQB~A6icH8~dAKvTLNj71xvYiUPWyljZ~zoTbC6CnMM~5o7HQQk~sOUUjREcDb1A9Eoypkqk9HXqJQNJMgqzpU1d-yBTyaF25o3fQUj4Fhz7v7jJWqrQYUvMYsMsOdap89dQ0JfLhN2MozHO1H4oHlrA5QQ4Yq3YLDjQD7RpjBIP1Wmgo2tLHiHe3gBB8rkiaA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"> Learning mutual trust </a>by Bikramjit Banerjee, Rajatish Mukherjee, Sandip Sen. Workshop on Deception, Fraud and Trust in Agent Societies, 2000. <a href="link">  </a> </summary> Multiagent learning literature has looked at iterated twoplayer games to develop mehanisms that allow agents to learn to converge on Nash Equilibrium strategy profiles. Such equilibrium configuration implies that there is no motivation for one player to hange its strategy if the other does not. Often, in general sum games, a higher payoff an be obtained by both players if one chooses not to respond optimally to the other player. By developing mutual trust, agents can avoid iterated best responses that will lead to a lesser payoff Nash Equilibrium. In this paper we consider 1-level agents (modelers) who selet ations based on expeted utility considering probability distributions over the ations of the opponent(s). We show that in certain situations, such stochastially-greedy agents an perform better (by developing mutually trusting behavior) than those that expliitly attempt to converge to Nash Equilibrium <br> - </details>

<details> <summary> <a href="http://strategicreasoning.org/wp-content/uploads/2010/03/csr01.pdf"> Learning about other agents in a dynamic multiagent system </a>by Junling Hu, Michael Wellman. Journal of Cognitive Systems Research, 2001. <a href="link">  </a> </summary> We analyze the problem of learning about other agents in a class of dynamic multiagent systems, where performance of the primary agent depends on behavior of the others. We consider an online version of the problem, where agents must learn models of the others in the course of continual interactions. Various levels of recursive models are implemented in a simulated double auction market. Our experiments show learning agents on average outperform non-learning agents who do not use information about others. Among learning agents, those with minimum recursion assumption generally perform better than the agents with more complicated, though often wrong assumptions. <br> - </details>

<br/>

### Relational Learning

<details> <summary> <a href="https://lirias.kuleuven.be/retrieve/383228"> Multi-agent Relational Reinforcement Learning </a>by Tom Croonenborghs, Karl Tuyls, Jan Ramon, and Maurice Bruynooghe. LAMAS, 2005. <a href="link">  </a> </summary> In this paper we report on using a relational state space in multi-agent reinforcement learning. There is growing evidence in the Reinforcement Learning research community that a relational representation of the state space has many benefits over a propositional one. Complex tasks as planning or information retrieval on the web can be represented more naturally in relational form. Yet, this relational structure has not been exploited for multi-agent reinforcement learning tasks and has only been studied in a single agent context so far. In this paper we explore the powerful possibilities of using Relational Reinforcement Learning (RRL) in complex multi-agent coordination tasks. More precisely, we consider an abstract multi-state coordination problem, which can be considered as a variation and extension of repeated stateless Dispersion Games. Our approach shows that RRL allows to represent a complex state space in a multi-agent environment more compactly and allows for fast convergence of learning agents. Moreover, with this technique, agents are able to make complex interactive models (in the sense of learning from an expert), to predict what other agents will do and generalize over this model. This enables to solve complex multi-agent planning tasks, in which agents need to be adaptive and learn, with more powerful tools. <br> - </details>

### Repeated Games

<details> <summary> <a href="http://www-stat.wharton.upenn.edu/~steele/Resources/Projects/SequenceProject/Hannan.pdf"> Approximation to bayes risk in repeated plays </a>by James Hannan. Contributions to the Theory of Games, 1959. <a href="link">  </a> </summary> This paper is concerned with the development of a dynamic theoryof decision under uncertainty. The results obtained are directly applicableto the development of a dynamic theory of games in which at least one play­er is, at each stage, fully informed on the joint empirical distribution ofthe past choices of strategies of the rest. Since the decision problem canbe Imbedded in a sufficiently unspecified game theoretic model, the paperis written in the language and notation of the general two person game, in which, however, player  I’s motivation is completely unspecified. <br> - </details>

<details> <summary> <a href="https://scholars.huji.ac.il/sites/default/files/abrahamn/files/bounded.pdf"> Bounded complexity justifies cooperation in finitely repeated prisoner’s dilemma </a>by Abraham Neyman. Economic Letters, 1985. <a href="link">  </a> </summary> Cooperation in the finitely repeated prisoner's dilemma is justified, without departure from strict utility maximization or complete information, but under the assumption that there are bounds (possibly very large) to the complexity of the strategies that the players may use. <br> - </details>

<details> <summary> <a href="http://www.econ.ucla.edu/workingpapers/wp735.pdf"> Noncomputable strategies and discounted repeated games </a>by John H. Nachbar, William R. Zame. Economic Theory, 1996. <a href="link">  </a> </summary> A number of authors have used formal models of computation to capture the idea of “bounded rationality” in repeated games. Most of this literature has used computability by a finite automaton as the standard. A conceptual difficulty with this standard is that the decision problem is not “closed.” That is, for every strategy implementable by an automaton, there is some best response implementable by an automaton, but there may not exist any algorithm forfinding such a best response that can be implemented by an automaton. However, such algorithms can always be implemented by a Turing machine, the most powerful formal model of computation. In this paper, we investigate whether the decision problem can be closed by adopting Turing machines as the standard of computability. The answer we offer is negative. Indeed, for a large class of discounted repeated games (including the repeated Prisoner's Dilemma) there exist strategies implementable by a Turing machine for whichno best response is implementable by a Turing machine. <br> - </details>

<details> <summary> <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fa466323f685599a7ae5b40d0ca921cb9a2c0a0d"> On Multiagent Q-Learning in a Semi-competitive Domain </a>by Tuomas W. Sandholm, Robert H. Crites. Adaptation and Learning in Multiagent Systems, 1996. <a href="link">  </a> </summary> Q-learning is a recent reinforcement learning (RL) algorithm that does not need a model of its environment and can be used on-line. Therefore it is well-suited for use in repeated games against an unknown opponent. Most RL research has been confined to single agent settings or to multiagent settings where the agents have totally positively correlated payoffs (team problems) or totally negatively correlated payoffs (zero-sum games). This paper is an empirical study of reinforcement learning in the iterated prisoner's dilemma (IPD), where the agents' payoffs are neither totally positively nor totally negatively correlated. RL is considerably more difficult in such a domain. This paper investigates the ability of a variety of Q-learning agents to play the IPD game against an unknown opponent. In some experiments, the opponent is the fixed strategy Tit-for-Tat, while in others it is another Q-learner. All the Q-learners learned to play optimally against Tit-for-Tat. Playing against another learner was more difficult because the adaptation of the other learner creates a nonstationary environment in ways that are detailed in the paper. The learners that were studied varied along three dimensions: the length of history they received as context, the type of memory they employed (lookup tables based on restricted history windows or recurrent neural networks (RNNs) that can theoretically store features from arbitrarily deep in the past), and the exploration schedule they followed. Although all the learners faced difficulties when playing against other learners, agents with longer history windows, lookup table memories, and longer exploration schedules fared best in the IPD games. <br> - </details>

<details> <summary> <a href="http://www.dklevine.com/archive/refs4576.pdf"> Prediction, Optimization, and Learning in Repeated Games </a>by John H. Nachbar. Econometrica, 1997. <a href="link">  </a> </summary> Consider a two-player discounted repeated game in which each player optimizes with respect to a prior belief about his opponent's repeated game strategy. One would like to argue that if beliefs are cautious, then each player's best response will be in the support, loosely speaking, of his opponent's belief and that, therefore, players will learn as the game unfolds to predict the continuation path of play. If this conjecture were true, a convergence result due to Kalai and Lehrer would imply that the continuation path of the repeated game would asymptotically resemble that of a Nash equilibrium. One would thus have constructed a theory in which Nash equilibrium behavior is a necessary long-run consequence of optimization by cautious players. This paper points out an obstacle to such a theory. Loosely put, in many repeated games, if players optimize with respect to beliefs that satisfy a diversity condition termed neutrality, then each player will choose a strategy that his opponent was certain would not be played. <br> - </details>

<details> <summary> <a href="http://rob.schapire.net/papers/FreundScYY.pdf"> Adaptive Game Playing Using Multiplicative Weights </a>by Yoav Freund, Robert E. Schapire. Games and Economic Behavior, 1999. <a href="link">  </a> </summary> We present a simple algorithm for playing a repeated game. We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy. Our bounds are nonasymptotic and hold for any opponent. The algorithm, which uses the multiplicative-weight methods of Littlestone and Warmuth, is analyzed using the Kullback–Liebler divergence. This analysis yields a new, simple proof of the min–max theorem, as well as a provable method of approximately solving a game. A variant of our game-playing algorithm is proved to be optimal in a very strong sense <br> - </details>

<details> <summary> <a href="https://www.cs.utexas.edu/users/pstone/Papers/bib2html-links/threats-ATAL2001.pdf"> Implicit negotiation in repeated games </a>by Peter Stone and Michael L. Littman. ATAL, 2001. <a href="link">  </a> </summary> In business-related interactions such as the on-going high-stakes FCC spectrum auctions, explicit communication among participants is regarded as collusion, and is therefore illegal. In this paper, we consider the possibility of autonomous agents engaging in implicit negotiation via their tacit interactions. In repeated general-sum games, our testbed for studying this type of interaction, an agent using a ``best response'' strategy maximizes its own payoff assuming its behavior has no effect on its opponent. This notion of best response requires some degree of learning to determine the fixed opponent behavior. Against an unchanging opponent, the best-response agent performs optimally, and can be thought of as a ``follower, '' since it adapts to its opponent. However, pairing two best-response agents in a repeated game can result in suboptimal behavior. We demonstrate this suboptimality in several different games using variants of Q-learning as an example of a best-response strategy. We then examine two ``leader'' strategies that induce better performance from opponent followers via stubbornness and threats. These tactics are forms of implicit negotiation in that they aim to achieve a mutually beneficial outcome without using explicit communication outside of the game. <br> - </details>

<details> <summary> <a href="https://www.cs.utexas.edu/~pstone/Courses/394Rfall16/resources/week10-threats.pdf"> Leading Best-Response Strategies in Repeated Games </a>by Peter Stone, Michael L. Littman. IJCAI, 2001. <a href="link">  </a> </summary> First steps toward agents that can reason this way: Negotiation without explicit communication! <br> - </details>

<details> <summary> <a href="https://www.cs.cmu.edu/~mmv/papers/01ijcai-mike.pdf"> Sophisticated EWA Learning and Strategic Teaching in Repeated Games </a>by Colin F. Camerer, Teck-Hua Ho, Juin-Kuan Chong. Journal of Economic Theory, 2002. <a href="https://www.Summary.so/instadeep/Multiagent-Learning-Basics-Challenges-and-Prospect-21cb7b4294b84a4188cafd184a3deed8">   </a> </summary> Most learning models assume players are adaptive (i.e., they respond only to their own previous experience and ignore others' payo® information) and behavior is not sensitive to the way in which players are matched. Empirical evidence suggests otherwise. In this paper, we extend our adaptive experienceweighted attraction (EWA) learning model to capture sophisticated learning and strategic teaching in repeated games. The generalized model assumes there is a mixture of adaptive learners and sophisticated players. An adaptive learner adjusts his behavior the EWA way. A sophisticated player rationally best-responds to her forecasts of all other behaviors. A sophisticated player can be either myopic or farsighted. A farsighted player develops multiple-period rather than single-period forecasts of others' behaviors and chooses to `teach' the other players by choosing a strategy scenario that gives her the highest discounted net present value. We estimate the model using data from p-beauty contests and repeated trust games with incomplete information. The generalized model is better than the adaptive EWA model in describing and predicting behavior. Including teaching also allows an empirical learning-based approach to reputation formation which predicts better than a quantal-response extension of the standard typebased approach. <br> - </details>

<details> <summary> <a href="https://jmvidal.cse.sc.edu/library/littman03a.pdf"> A Polynomial-time Nash Equilibrium Algorithm for Repeated Games </a>by Michael L. Littman, Peter Stone. ACM conference on Electronic commerce, 2003. <a href="link">  </a> </summary> With the increasing reliance on game theory as a foundation for auctions and electronic commerce, efficient algorithms for computing equilibria in multiplayer general-sum games are of great theoretical and practical interest. The computational complexity of finding a Nash equilibrium for a one-shot bimatrix game is a well known open problem. This paper treats a closely related problem, that of finding a Nash equilibrium for an average-payoff repeated bimatrix game, and presents a polynomial-time algorithm. Our approach draws on the “folk theorem” from game theory and shows how finite-state equilibrium strategies can be found efficiently and expressed succinctly <br> - </details>

<details> <summary> <a href="https://digitalcommons.montclair.edu/cgi/viewcontent.cgi?article=1586&context=compusci-facpubs"> The Role of Reactivity in Multiagent Learning </a>by Bikramjit Banerjee, Jing Peng. AAMAS, 2004. <a href="link">  </a> </summary> In this paper we take a closer look at a recently proposed classification scheme for multiagent learning algorithms. Based on this scheme an exploitation mechanism (we call it the Exploiter) was developed that could beat various Policy Hill Climbers (PHC) and other fair opponents in some repeated matrix games. We show on the contrary that some fair opponents may actually beat the Exploiter in repeated games. This clearly indicates a deficiency in the original classification scheme which we address. Specifically, we introduce a new measure called Reactivity that measures how fast a learner can adapt to an unexpected hypothetical change in the opponent’s policy. We show that in some games, this new measure can approximately predict the performance of a player, and based on this measure we explain the behaviors of various algorithms in the Matching Pennies game, which was inexplicable by the original scheme. Finally we show that under certain restrictions, a player that consciously tries to avoid exploitation may be unable to do so. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Fall/2004/FS-04-02/FS04-02-012.pdf"> On the agenda(s) of research on multi-agent learning </a>by Y. Shoham, R. Powers, and T. Grenager. AAAI Fall Symposium. Technical Report FS-04-02, 2004. <a href="link">  </a> </summary> We survey the recent work in AI on multi-agent reinforcement learning (that is, learning in stochastic games). After tracing a representative sample of the recent literature, we argue that, while exciting, much of this work suffers from a fundamental lack of clarity about the problem or problems being addressed. We then propose five well-defined problems in multi-agent reinforcement learning and single out one that in our view is both well-suited for AI and has not yet been adequately addressed. We conclude with some remarks about how we believe progress is to be made on this problem. <br> - </details>

<br/>

### Robotic Teams

<details> <summary> <a href="https://www.cs.uml.edu/~holly/papers/sab92.pdf"> An adaptive communication protocol for cooperating mobile robots </a>by H. Yanco and L. Stein. International Conference on Simulation of Adaptive Behavior, pages 478--485, 1993. <a href="">  </a> </summary> We describe mobile robots engaged in a cooperative task that requires communication. The robots are initially given a fixed but uninterpreted vocabulary for communication. In attempting to perform their task, the robots learn a private communication language. Different meanings for vocabulary elements are learned in different runs of the experiment. As circumstances change, the robots adapt their lan-guage to allow continued success at their task. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Alan-Schultz-3/publication/2782918_Robo-Shepherd_Learning_Complex_Robotic_Behaviors/links/0046353417dd828b5f000000/Robo-Shepherd-Learning-Complex-Robotic-Behaviors.pdf">  Robo-shepherd: Learning complex robotic behaviors </a>by A. Schultz, J.Grefenstette, and W. Adams. ASME Press, 1996. <a href="">  </a> </summary> This paper reports on recent results using genetic algorithms to learn decision rules for complex robot behaviors. The method involves evaluating hypothetical rule sets on a simulator and applying simulated evolution to evolve more effective rules. The main contributions of this paper are (1) the task learned is a complex behavior involving multiple mobile robots, and (2) the learned rules are verified through experiments on operational mobile robots. The case study involves a shepherding task in which one mobile robot attempts to guide another robot to a specified area. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Workshops/1997/WS-97-03/WS97-03-002.pdf"> Learning Roles: Behavioral Diversity in Robot Teams </a>by Tucker Balch. AAAI, 1997. <a href="">  </a> </summary> This paper describes research investigating behavioral specialization in learning robot teams. Each agent is provided a common set of skills (motor schema-based behavioral assemblages) from which it builds a taskachieving strategy using reinforcement learning. The agents learn individually to activate particular behavioral assemblages given their current situation and a reward signal. The experiments, conducted in robot soccer simulations, evaluate the agents in terms of performance, policy convergence, and behavioral diversity. The results show that in many cases, robots will autorustically diversify by choosing heterogeneous behaviors. The degree of diversification and the performance of the team depend on the reward structure. When the entire team is jointly rewarded or penalized (global reinforcement), teams tend towards heterogeneous behavior. When agents are provided feedback individually (local reinforcement), they converge to identical policies. <br> - </details>

<details> <summary> <a href="https://data.exppad.com/public/papers/Machine%20Learning/MMARL/Mataric%20(1997)%3A%20Reinforcement%20Learning%20in%20the%20Multi-Robot%20Domain.pdf"> Reinforcement Learning in the Multi-Robot Domain </a>by MAJA J. MATARIC. Autonomous Robots, 1997. <a href="link">  </a> </summary> This paper describes a formulation of reinforcement learning that enables learning in noisy, dynamic environments such as in the complex concurrent multi-robot learning domain. The methodology involves minimizing the learning space through the use of behaviors and conditions, and dealing with the credit assignment problem through shaped reinforcement in the form of heterogeneous reinforcement functions and progress estimators. We experimentally validate the approach on a group of four mobile robots learning a foraging task <br> - </details>

<details> <summary> <a href="https://web.unbc.ca/~russellt/swarm/robocupgp98.pdf"> Genetic Programming Produced Competitive Soccer Softbot Teams for RoboCup97 </a>by Sean Luke. Genetic Programming, 1998. <a href="link">  </a> </summary> At RoboCup, teams of autonomous robots or software softbots compete in simulated soccer matches to demonstrate cooperative robotics techniques in a very difficult, real-time, noisy environment. At the IJCAI/RoboCup97 softbot competition, all entries but ours used human-crafted cooperative decision-making behaviors. We instead entered a softbot team whose high-level decision making behaviors had been entirely evolved using genetic programming. Our team won its first two games against human-crafted opponent teams, and received the RoboCup Scientific Challenge Award. This report discusses the issues we faced and the approach we took to use GP to evolve our robot soccer team for this difficult environment. <br> - </details>

<details> <summary> <a href="https://dspace.library.uu.nl/bitstream/handle/1874/20821/wiering_99_reinforcement.pdf?sequence=1"> Reinforcement learning soccer teams with incomplete world models </a>by M. Wiering, R. Salustowicz, and J. Schmidhuber. Journal of Autonomous Robots, 1999. <a href="link">  </a> </summary> We use reinforcement learning (RL) to compute strategies for multiagent soccer teams. RL may profit significantly from world models (WMs) estimating state transition probabilities and rewards. In high-dimensional, continuous input spaces, however, learning accurate WMs is intractable. Here we show that incomplete WMs can help to quickly find good action selection policies. Our approach is based on a novel combination of CMACs and prioritized sweeping-like algorithms. Variants thereof outperform both Q(lambda)-learning with CMACs and the evolutionary method Probabilistic Incremental Program Evolution (PIPE) which performed best in previous comparisons.
<br> - </details>

<details> <summary> <a href="http://www.cs.cmu.edu/afs/.cs.cmu.edu/Web/People/motionplanning/papers/sbp_papers/integrated1/wu_microairveh.pdf"> Evolving control for distributed micro air vehicles </a>by A. Wu, A. Schultz, and A. Agah. In IEEE Computational Intelligence in Robotics and Automation Engineers Conference, 1999. <a href="link">  </a> </summary> In this paper, we focus on the task of large area surveillance. Given an area to be surveilled and a team of MAVs with appropriate sensors, the task is to dynamically distribute the micro air vehicles (MAVs) appropriately in the surveillance area for maximum coverage based on features present on the ground, and to adjust this distribution over time as changes in the team or on the ground occur. We have developed a system that will learn rule sets for controlling the individual MAVs in a distributed surveillance team. Since each rule set gov- erns an individual MAV, control of the overall behavior of the entire team is distributed; there is no single entity controlling the actions of the entire team. Currently, all members of the MAV team utilize the same rule set; specialization of individual MAVs through the evolution of unique rule sets is a logical extension to this work.
<br> - </details>

<details> <summary> <a href="https://www.researchgate.net/publication/2449273_Exploiting_Embodiment_in_Multi-Robot_Teams"> Exploiting embodiment in multi-robot teams </a>by B. B. Werger and M. Mataric. Technical Report IRIS-99-378, University of Southern California, Institute for Robotics and Intelligent Systems, 1999. <a href="link">  </a> </summary> This paper describes multi-robot experiments and systems which exploit their embodied nature to reduce needs for sensory, effector, and computational resources. Many natural phenomena, such as territorial markings or ant pheromone trails, take great advantage of the ability mark the environment, and of the natural dissipative processes which cause these markings to decay. In this way, globally complex behavior can result from simple local rules. The information invariants ([Donald 1995], [Donald et al 1994]) literature raises the issue of robots similarly recording information, or even "programs," into the physical environment. This paper provides example systems that dynamically encode information and "programs" into the physical environment, and by so doing, increase their own robustness and reduce their resource requirements and computational complexity. The main experimental system that we present, a robot "chain" used for foraging, is modeled after the natural phenomenon of ant ph... <br> - </details>

<details> <summary> <a href="https://smartech.gatech.edu/bitstream/handle/1853/21573/alaa99.pdf"> Reward and Diversity in Multirobot Foraging </a>by Tucker Balch. IJCAI, 1999. <a href="link">  </a> </summary> This research seeks to quantify the impact of the choice of reward function on behavioral diversity in learning robot teams. The methodology developed for this work has been applied to multirobot foraging soccer and cooperative movement. This paper focuses specifically on results in multirobot foraging. In these experiments three types of reward are used with Qlearning to train a multirobot team to forageing a local performancebased reward a global performancebased reward and a heuristic strategy referred to as shaped reinforcement. Local strategies provide each agent a specific reward according to its own behavior while global rewards provide all the agents on the team the same reward simultaneously. Shaped reinforcement provides a heuristic reward for an agent's action given its situation. The experiments indicate that local performance based rewards and shaped reinforcement generate statistically similar results, they both provide the best performance and the least diversity. Finally learned policies are demonstrated on a team of Nomadic Technologies' Nomad-150 robots. <br> - </details>

<details> <summary> <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e10cabac2e5e90dafd9ccea24578bf0cd3f987d0"> Heterogeneity in the Coevolved Behaviors of Mobile Robots: The Emergence of Specialists </a>by Mitchell A. Potter, Lisa A. Meeden, Alan C. Schult. IJCAI, 2001. <a href="link">  </a> </summary> Many mobile robot tasks can be most efficiently solved when a group of robots is utilized. The type of organization, and the level of coordination and communication within a team of robots affects the type of tasks that can be solved. This paper examines the tradeoff of homogeneity versus heterogeneity in the controlsystems by allowing a team of robots to coevolve their high-level controllers given differentlevels of difficulty of the task. Our hypothesis is that simply increasing the difficulty of a task is not enough to induce a team of robots to create specialists. The key factor is not difficulty per se, but the number of skill sets necessary to successfully solve the task. As the number of skills needed increases, the more beneficial and necessary heterogeneity becomes. We demonstrate this in the task domain of herding, where one or more robots must herd another robot into a confined space. <br> - </details>

<details> <summary> <a href="https://kilthub.cmu.edu/articles/journal_contribution/The_Necessity_of_Average_Rewards_in_Cooperative_Multirobot_Learning/6561245/files/12043550.pdf"> The necessity of average rewards in cooperative multirobot learning </a>by P. Tangamchit, J. Dolan, and P. Khosla. In Proceedings of IEEE Conference on Robotics and Automation, 2002. <a href="link">  </a> </summary> Learning can be an effective way for robot systems to deal with dynamic environments and changing task conditions. However, popular single-robot learning algorithms based on discounted rewards, such as Q learning, do not achieve cooperation (i.e., purposeful division of labor) when applied to task-level multirobot systems. A task-level system is defined as one performing a mission that is decomposed into subtasks shared among robots. We demonstrate the superiority of average-reward-based learning such as the Monte Carlo algorithm for task-level multirobot systems, and suggest an explanation for this superiority. <br> - </details>

<details> <summary> <a href="http://idm-lab.org/bib/abstracts/papers/icra03a.pdf"> Trail-laying robots for robust terrain coverage </a>by J. Svennebring and S. Koenig. In Proceedings of the International Conference on Robotics and Automation (ICRA-03), 2003. <a href="link">  </a> </summary> Robotics researchers have studied robots that can follow the trails laid by other robots. We, on the other hand, study robots that leave trails in the terrain to cove closed terrain once or repeatedly. How to design such ant robots has so far been studied only theoretically for gross robot simplifications. In this paper, we describe for the first time how to build physical ant robots that cover terrain. We show that a modified version of node counting can model the behavior of the ant robots and report on first experiments that we performed to understand their behavior better. These experiments confirm that our ant robots indeed cover terrain robustly even if the trails are of uneven quality, the ant robots are moved without realizing this, or some trails are destroyed. Finally, we report the results of a large-scale experiment where ten simulated ant robots covered a factory floor of 25 by 25 meters repeatedly over 85 hours without any ant robots getting stuck. <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Symposia/Fall/2004/FS-04-02/FS04-02-003.pdf"> Co-Evolving Team Capture Strategies for Dissimilar Robots </a>by H. Joseph Blumenthal, Gary B. Parker. AAAI, 2004. <a href="link">  </a> </summary> Evolving team members to act cohesively is a complex and challenging problem. To allow the greatest range of solutions in team problem solving, heterogeneous agents are desirable. To produce highly specialized agents, team members should be evolved in separate populations. Co-evolution in separate populations requires a system for selecting suitable partners for evaluation at trial time. Selecting too many partners for evaluation drives computation time to unreasonable levels, while selecting too few partners blinds the GA from recognizing highly fit individuals. In previous work, we employed a method based on punctuated anytime learning which periodically tests a number of partner combinations to select a single individual from each population to be used at trail time. We began testing our method in simulation using a two-agent box pushing task. We then expanded our research by simulating a predator-prey scenario in which all the agents had the exact same capabilities. In this paper, we report the expansion of our work by applying this method of team learning to five dissimilar robots. <br> - </details>

<details> <summary> <a href="http://nozdr.ru/data/media/biblio/kolxoz/Cs/CsLn/L/Learning%20and%20Adaption%20in%20Multi-Agent%20Systems,%201%20conf.,%20LAMAS%202005(LNCS3898,%20Springer,%202006)(ISBN%203540330534)(224s).pdf#page=185"> Efficient Reward Functions for Adaptive Multi-rover Systems </a>by Kagan Tumer, Adrian Agogino. LAMAS, 2005. <a href="link">  </a> </summary> This chapter focuses on deriving reward functions that allow multiple agents to co-evolve efficient control policies that maximize a system level reward in noisy and dynamic environments. The solution we present is based on agent rewards satisfying two crucial properties. First, the agent reward function and global reward function has to be aligned, that is, an agent maximizing its agent-specific reward should also maximize the global reward. Second, the agent has to receive sufficient “signal” from its reward, that is, an agent’s action should have a large influence over its agent-specific reward. Agents using rewards with these two properties will evolve the correct policies quickly. This hypothesis is tested in episodic and non-episodic, continuous-space multi-rover environment where rovers evolve to maximize a global reward function over all rovers. The environments are dynamic (i.e. changes over time), noisy and have restriction on communication between agents. We show that a control policy evolved using agent-specific rewards satisfying the above properties outperforms policies evolved using global rewards by up to 400%. More notably, in the presence of a larger number of rovers or rovers with noisy and communication limited sensors, the proposed method outperforms global reward by a higher percentage than in noisefree conditions with a small number of rovers. <br> - </details>

<br/>

### Stochastic Games

<details> <summary> <a href="https://apps.dtic.mil/sti/pdfs/ADA385122.pdf"> An Analysis of Stochastic Game Theory for Multiagent Reinforcement Learning </a>by Michael Bowling, Manuela Veloso. Technical Report, 2000. <a href="link">  </a> </summary> Learning behaviors in a multiagent environmentis crucial for developing and adapting multiagent systems. Reinforcement learning techniques have addressed this problem for a single agent acting in a stationary environment, which is modeled as a Markov decision process (MDP). But, multiagent environments are inherently non-stationary since the other agents are free to change their behavior as they also learn and adapt. Stochastic games, first studied in the game theory community, are a natural extension of MDPs to include multiple agents. In this paper we contribute a comprehensive presentation of the relevant techniques for solving stochastic games from both the game theory community and reinforcement learning communities. We examine the assumptions and limitations of these algorithms, and identify similarities between these algorithms, single agent reinforcement learners, and basic game theory techniques <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Nobuo-Suematsu/publication/221454885_A_multiagent_reinforcement_learning_algorithm_using_extended_optimal_response/links/00b495331616ee577e000000/A-multiagent-reinforcement-learning-algorithm-using-extended-optimal-response.pdf"> A Multiagent Reinforcement Learning Algorithm
using Extended Optimal Response </a>by Nobuo Suematsu, Akira Hayashi. AAMAS, 2002. <a href="link">  </a> </summary> Stochastic games provides a theoretical framework to multiagent reinforcement learning. Based on the framework, a multiagent reinforcement learning algorithm for zero-sum stochastic games was proposed by Littman and it was extended to general-sum games by Hu and Wellman. Given a stochastic game, if all agents learn with their algorithm, we can expect that the policies of the agents converge to a Nash equilibrium. However, agents with their algorithm always try to converge to a Nash equilibrium independent of the policies used by the other agents. In addition, in case there are multiple Nash equilibria, agents must agree on the equilibrium where they want to reach. Thus, their algorithm lacks adaptability in a sense. In this paper, we propose a multiagent reinforcement learning algorithm. The algorithm uses the extended optimal response which we introduce in this paper. It will converge to a Nash equilibrium when other agents are adaptable, otherwise it will make an optimal response. We also provide some empirical results in three simple stochastic games, which show that the algorithm can realize what we intend. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/1206.3277.pdf"> A Polynomial-time Nash Equilibrium Algorithm for Repeated Stochastic Games </a>by Enrique Munoz de Cote, Michael L. Littman. Preprint, 2012. <a href="link">  </a> </summary> We present a polynomial-time algorithm that always finds an (approximate) Nash equilibrium for repeated two-player stochastic games. The algorithm exploits the folk theorem to derive a strategy profile that forms an equilibrium by buttressing mutually beneficial behavior with threats, where possible. One component of our algorithm efficiently searches for an approximation of the egalitarian point, the fairest pareto-efficient solution. The paper concludes by applying the algorithm to a set of grid games to illustrate typical solutions the algorithm finds. These solutions compare very favorably to those found by competing algorithms, resulting in strategies with higher social welfare, as well as guaranteed computational efficiency. <br> - </details>

<br/>

### Theoretical Frameworks

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/BF00115009.pdf"> Learning to predict by the methods of temporal differences </a>by R. Sutton. Machine Learning, 3:9–44, 1988. <a href="link">  </a> </summary> This article introduces a class of incremental learning procedures specialized for prediction that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, tile new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference method~ have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-differenee methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporaldifference methods can be applied to advantage. <br> - </details>

<details> <summary> <a href="http://www.weiss-gerhard.info/publications/A12.pdf"> Distributed reinforcement learning </a>by G. Weiß. Sankt Augustin: Infix Verlag, 1995. <a href="link">  </a> </summary> Distributed reinforcement learning... <br> - </details>

<details> <summary> <a href="https://www.csd.uwo.ca/~xling/cs346a/extra/tdgammon.pdf"> Temporal difference learning and TD-gammon </a>by G. Tesauro. Communications of the ACM, 38(3):58–68, 1995. <a href="link">  </a> </summary> Ever since the days of Shannon’s proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome. This article presents a game-learning program called TD-Gammon. TD-Gammon is a neural network that trains itself to be an evaluation function for the game of backgammon by playing against itself and learning from the outcome. Although TD-Gammon has greatly surpassed all previous computer programs in its ability to play backgammon, that was not why it was developed. Rather, its purpose was to explore some exciting new ideas and approaches to traditional problems in the field of reinforcement learning. <br> - </details>

<details> <summary> <a href="https://courses.cs.duke.edu/spring07/cps296.3/littman94markov.pdf"> Markov games as a framework for multiagent reinforcement learning  </a>by Michael L. Littman. ICML, 1994. <a href="link">  </a> </summary> In the Markov decision process(MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic. <br> - </details>

<details> <summary> <a href="https://sferics.idsia.ch/pub/juergen/gerhard.ps.gz"> Realistic multi-agent reinforcement learning  </a>by J Schmidhuber. ECAI Workshop, 1996. <a href="link">  </a> </summary> A "realistic multi-agent reinforcement learning system" consist of multiple "realistic reinforcement learners". Each leads a single life lasting from birth to unknown death. In between it tries to maximize cumulative reward. Its actions and learning algorithms consume part of its life --- computational resources are limited. The expected reward for a certain behaviour may change over time, partly because of the learner's or other learner's actions. Therefore, no learner can expect to collect (and generalize from) voluminous performance statistics. For this reason, previous approaches to multi-agent reinforcement learning are either very limited or heuristic by nature. I introduce a novel, sound, simple principle for learning in such general but typical situations. The principle allows for plugging in a wide variety of learning algorithms. I mention experiments with complex, non-Markovian environments that demonstrate the principle's practical feasibility. <br> - </details>

<details> <summary> <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=3a50b97266e0b97f8fb15be95018c8473bb5dd21"> Multi-agent learning with the success-story algorithm  </a>by J Schmidhuber and J Zhao. LNAI, 1996. <a href="link">  </a> </summary> We study systems of multiple reinforcement learners. Each leads a single life lasting from birth to unknown death. In between it tries to accelerate reward intake. Its actions and learning algorithms consume part of its life  computational resources are limited. The expected reward for a certain behavior may change over time, partly because of other learners' actions and learning processes. For such reasons, previous approaches to multi-agent reinforcement learning are either limited or heuristic by nature. Using a simple backtracking method called the "success-story algorithm", however, at certain times called evaluation points each of our learners is able to establish success histories of behavior modifications: it simply undoes all those of the previous modifications that were not empirically observed to trigger lifelong reward accelerations (computation time for learning and testing is taken into account). Then it continues to act and learn until the next evaluation point. Success histories can be enforced despite interference from other learners. The principle allows for plugging in a wide variety of learning algorithms. An experiment illustrates its feasibility.            <br> - </details>

<details> <summary> <a href="https://www.emse.fr/~beaune/maml/sen-weiss-MAL99.pdf"> Learning in Multiagent Systems </a>by G. Weiß and S. Sen. Lecture Notes in Artificial Intelligence, Volume 1042. Springer-Verlag, 1996. <a href="link">  </a> </summary> Learning and intelligence are intimately related to each other. It is usually agreed that a system capable of learning deserves to be called intelligent; and conversely, a system being considered as intelligent is, among other things, usually expected to be able to learn. Learning always has to do with the self-improvement of future behavior based on past experience. More precisely, according to the standard artificial intelligence (AI) point of view learning can be informally defined as follows: <br> - </details>

<details> <summary> <a href="https://www.aaai.org/Papers/Workshops/1997/WS-97-03/WS97-03-013.pdf"> Agents learning about agents: A framework and analysis </a>by J. Vidal and E. Durfee. In Working Notes of AAAI-97 Workshop on Multiagent Learning, 1997. <a href="link">  </a> </summary> We provide a framework for the study of learning in certain types of multi-agent systems (MAS), that di- vides an agent's knowledge about others into different utypes'. We use concepts from computational learn- ing theory to calculate the relative sample complexi- ties of learning the different types of knowledge, given either a supervised or a reinforcement learning algo- rithm. These results apply only for the learning of a fixed target function, which would probably not exist if the other agents are also learning. We then show how a changing target function affects the learning behaviors of the/agents, and how to determine the advantages of having lower sample complexity. Our results can be used by a designer of a learning agent in a MAS to determine which knowledge he should put into the agent and which knowledge should be learned by the agent. <br> - </details>

<details> <summary> <a href="https://jmvidal.cse.sc.edu/papers/icmas98/icmas98l.pdf"> The moving target function problem in multiagent learning </a>by J. Vidal and E. Durfee. In Proceedings of the Third Annual Conference on Multi-Agent Systems, 1998. <a href="link">  </a> </summary> We describe a framework that can be used to model and predict the behavior of MASs with learning agents. It uses a difference equation for calculating the progression of an agent's error in its decision function, thereby telling us how the agentis expected to fare in the MAS.The equation relies on parameters which capture the agents' learning abilities (such as its change rate, learning rate and retention rate) as well as relevant aspects of the MAS (such as the impact that agents have on each other). We validate the framework with experimental results using reinforcement learning agents in a market system, as well as by other experimental results gathered from the AI literature. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Pierre-Dillenbourg/publication/2691318_What_is_multi'_in_Multi-Agent_Learning/links/56e690f008ae68afa113882e/What-is-multi-in-Multi-Agent-Learning.pdf"> What is ‘multi’ in multi-agent learning? In P. Dillenbourg, editor, Collaborative learning </a>by G. Weiß and P. Dillenbourg. In P. Dillenbourg, editor, Collaborative learning. Cognitive and computational approaches, pages 64--80. Pergamon Press, 1999. <a href="link">  </a> </summary> Learning in multi-agent environments constitutes a research and application area whose importance is broadly acknowledged in artificial intelligence. Although there is a rapidly growing body of literature on multi-agent learning, almost nothing is known about the intrinsic nature of and requirements for this kind of learning. This observation is the starting point of this chapter which aims at providing a more general characterization of multi-agent learning. This is done in an interdisciplinary way from two different perspectives: the perspective of single-agent learning (the 'machine
learning perspective') and the perspective of human-human collaborative learning (the 'psychological perspective'). The former leads to a 'positive' characterization: three types of learning mechanisms - multiplication, division, and interaction - are identified and illustrated that can occur in multi-agent but not in single-agent settings. The latter leads to a 'negative' characterization: several cognitive processes like conflict resolution, mutual regulation, and explanation are identified and discussed that are most essential to human-human collaborative learning, but have been largely ignored so far in the available multi-agent learning approaches. Misunderstanding among humans is identified as a major source of these processes, and its important role in the context of multiagent systems is stressed. This chapter also offers a brief guide to agents and multi-agent systems as studied in artificial intelligence, and suggests directions for future research on multi-agent learning. <br> - </details>

<details> <summary> <a href="https://arxiv.org/pdf/cs/0001008.pdf"> Predicting the expected behavior of agents that learn about agents: the CLRI framework </a>by J. Vidal and E. Durfee. Autonomous Agents and Multi-Agent Systems, January 2003. <a href="link">  </a> </summary> We describe a framework and equations used to model and predict the behavior of multi-agent systems (MASs) with learning agents. A difference equation is used for calculating the progression of an agent’s error in its decision function, thereby telling us how the agent is expected to fare in the MAS. The equation relies on parameters which capture the agent's learning abilities, such as its change rate, learning rate and retention rate, as well as relevant aspects of the MAS such as the impact that agents have on each other. We validate the framework with experimental results using reinforcement learning agents in a market system, as well as with other experimental results gathered from the AI literature. Finally, we use PAC-theory to show how to calculate bounds on the values of the learning parameters. <br> - </details>

<details> <summary> <a href="https://www.cin.ufpe.br/~cavmj/CMU-CS-03-107.pdf"> Adversarial reinforcement learning </a>by W. Uther and M. Veloso. Technical Report CMU-CS-03-107, School of Computer Science, Carnegie Mellon University, 2003. <a href="link">  </a> </summary> 
Reinforcement Learning has been used for a number of years in single agent environments. This article reports on our investigation of Reinforcement Learning techniques in a multi-agent and adversarial environment with continuous observable state information. We introduce a new framework, two-player hexagonal grid soccer, in which to evaluate algorithms. We then compare the performance of several single-agent Reinforcement Learning techniques in that environment. These are further compared to a previously developed adversarial Reinforcement Learning algorithm designed for Markov games. Building upon these efforts, we introduce new algorithms to handle the multi-agent, the adversarial, and the continuous-valued aspects of the domain. We introduce a technique for modelling the opponent in an adversarial game. We introduce an extension to Prioritized Sweeping that allows generalization of learnt knowledge over neighboring states in the domain; and we introduce an extension to the U Tree generalizing algorithm that allows the handling of continuous state spaces. Extensive empirical evaluation is conducted in the grid soccer domain <br> - </details>

<details> <summary> <a href="https://www.lirmm.fr/~jq/Cours/3cycle/module/HuWellman98icml.pdf"> Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm </a>by Junling Hu, Michael P. Wellman. ICML, 1998. <a href="link">  </a> </summary> In this paper, we adopt general-sum stochastic games as a framework for multiagent reinforcement learning. Our work extends previous work by Littman on zero-sum stochastic games to a broader framework. We design a multiagent Q-learning method under this framework, and prove that it converges to a Nash equilibrium under specified conditions. This algorithm is useful for finding the optimal strategy when there exists a unique Nash equilibrium in the game. When there exist multiple Nash equilibria in the game, this algorithm should be combined with other learning techniques to find optimal strategies. <br> - </details>

<details> <summary> <a href="https://www.semanticscholar.org/paper/Multiagent-Reinforcement-Learning-in-Stochastic-Hu-Wellman/7ce14dbb9add4d9656746703babd00d8f765b22a"> Multiagent reinforcement learning in stochastic games </a>by Hu, J., Wellman, M.P. Cambridge University Press, 1999. <a href="link">  </a> </summary> We adopt stochastic games as a general framework for dynamic noncooperative systems. This framework provides a way of describing the dynamic interactions of agents in terms of individuals' Markov decision processes. By studying this framework, we go beyond the common practice in the study of learning in games, which primarily focus on repeated games or extensive-form games. For stochastic games with incomplete information, we design a multiagent reinforcement learning method which allows agents to learn Nash equilibrium strategies. We show in both theory and experiments that this algorithm converges. From the viewpoint of machine learning research, our work helps to establish the theoretical foundation for applying reinforcement learning, originally deened for single-agent systems, to multiagent systems. <br> - </details>

<details> <summary> <a href="https://web.engr.oregonstate.edu/~wongwe/papers/pdf/distributed.1999.pdf"> Distributed Value functions </a>by J Schneider, WK Wong, A Moore and M Riedmiller. Carnegie Mellon University, 1999. <a href="link">  </a> </summary> Many interesting problems, such as power grids, network switches, and traffic flow, that are candidates for solving reinforcement learning (RL), also have properties that make distributed solutions desirable. We propose an algorithm for distributed reinforcement learning based on distributing the representation of the value function across nodes. Each node in the system only has the ability to sense state locally, choose actions locally, and receive reward locally (the goal of the system is to maximise the sum of rewards over all nodes and over all time). However each node is allowed to give its neighbors the current estimate of its value function for the states it passes through. We present a value function learning rule, using that information, that allows each node to learn a value function that is an estimate of a weighted sum of the future rewards for all the nodes in the network. With this representation, each node can choose actions to imporve the perfomance of the overall system. <br> - </details>

<details> <summary> <a href="https://iiia.csic.es/media/filer_public/79/19/79194eb1-def8-415d-9430-d294c58c673a/iiia-2002-571.pdf"> A bartering approach to improve multiagent learning </a>by K. Tumer, A. K. Agogino, and D. H. Wolpert. In Proceedings of First International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS02), pages 386--393, 2002. <a href="link">  </a> </summary> Multiagent systems offer a new paradigm to organize AI Applications. We focus on the application of Case-Based Reasoning to Multiagent systems. CBR offers the individual agents the capability of autonomously learn from experience. In this paper we present a framework for collaboration among agents that use CBR. We present explicit strategies for case bartering that address the issue of agents having a biased view of the data. The outcome of bartering is an improvement of individual agent performance and of overall multiagent system performance that equals the ideal situation where all agents have an unbiased view of the data. We also present empirical results illustrating the robustness of the case bartering process for several configurations of the multiagent system and for three different CBR techniques. <br> - </details>

<details> <summary> <a href="https://www.researchgate.net/profile/Andrew-Williams-36/publication/220660580_Learning_to_Share_Meaning_in_a_Multi-Agent_System/links/0046352950ad3770cb000000/Learning-to-Share-Meaning-in-a-Multi-Agent-System.pdf"> Learning to share meaning in a multi-agent system </a>by A. Williams. Autonomous Agents and Multi-Agent Systems, 8:165–193, 2004. <a href="link">  </a> </summary> The development of the semantic Web will require agents to use common domain ontologies to facilitate communication of conceptual knowledge. However, the proliferation of domain ontologies may also result in conflicts between the meanings assigned to the various terms. That is, agents with diverse ontologies may use different terms to refer to the same meaning or the same term to refer to different meanings. Agents will need a method for learning and translating similar semantic concepts between diverse ontologies. Only until recently have researchers diverged from the last decade’s ‘‘common ontology’’ paradigm to a paradigm involving agents that can share knowledge using diverse ontologies. This paper describes how we address this agent knowledge sharing problem of how agents deal with diverse ontologies by introducing a methodology and algorithms for multi-agent knowledge sharing and learning in a peer-to-peer setting. We demonstrate how this approach will enable multi-agent systems to assist groups of people in locating, translating, and sharing knowledge using our Distributed Ontology Gathering Group Integration Environment (DOGGIE) and describe our proof-of-concept experiments. DOGGIE synthesizes agent communication, machine learning, and reasoning for information sharing in the Web domain. <br> - </details>

<br/>

### Theses

<details> <summary> <a href="http://reports-archive.adm.cs.cmu.edu/anon/1998/CMU-CS-98-187.pdf"> Layered Learning in Multi-Agent Systems </a>by Peter Stone. PhD thesis, 1998. <a href="link">  </a> </summary> Multi-agent systems in complex, real-time domains require agents to act effectively both autonomously and as part of a team. This dissertation addresses multi-agent systems consisting of teams of autonomous agents acting in real-time, noisy, collaborative, and adversarial environments. Because of the inherent complexity of this type of multi-agent system, this thesis investigates the use of machine learning within multi-agent systems. The dissertation makes four main  contributions to the fields of Machine Learning and Multi-Agent Systems. First, the thesis defines a team member agent architecture within which a exible teamstructure is presented, allowing agents to decompose the task space into exible roles and allowing them to smoothly switch roles while acting. Team organization is achieved by the introduction of a locker-room agreement as a collection of conventions followed by all team members. It defines agent roles, team formations, and pre-compiled multi-agent plans. In addition, the team member agent architecture includes a communication paradigm for domains with single-channel, low-bandwidth, unreliable communication. The communication paradigm facilitates team coordination while being robust to lost messages and active interference from opponents. Second, the thesis introduces layered learning, a general-purpose machine learning paradigm for complex domains in which learning a mapping directly from agents' sensors to their actuators is intractable. Given a hierarchical task decomposition, layered learning allows for learning at each level of the hierarchy, with learning at each level directly affecting learning at the next higher level. Third, the thesis introduces a new multi-agent reinforcement learning algorithm, namely team-partitioned, opaque-transition reinforcement learning (TPOT-RL). TPOT-RL is designed for domains in which agents cannot necessarily observe the state changes when other team members act. It exploits local, action-dependent features to aggressively generalize its input representation for learning and partitions the task among the agents, allowing them to simultaneously learn collaborative policies by observing the long-term effects of their actions. Fourth, the thesis contributes a fully functioning multi-agent system that incorporates learning in a real-time, noisy domain with teammates and adversaries. Detailed algorithmic descriptions of the agents' behaviors as well as their source code are included in the thesis. Empirical results validate all four contributions within the simulated robotic soccer domain. The generality of the contributions is verified by applying them to the real robotic soccer, and network routing domains. Ultimately, this dissertation demonstrates that by learning portions of their cognitive processes, selectively communicating, and coordinating their behaviors via common knowledge, a group of independent agents can work towards a common goal in a complex, real-time, noisy, collaborative, and adversarial environment. <br> - </details>

<details> <summary> <a href="https://apps.dtic.mil/sti/pdfs/ADA461188.pdf"> Multiagent Learning in the Presence of Agents with Limitations </a>by Michael Bowling. Thesis, 2003. <a href="link">  </a> </summary> Learning to act in a multiagent environment is a challenging problem. Optimal behavior for one agent depends upon the behavior of the other agents, which are learning as well. Multiagent environments are therefore non-stationary, violating the traditional assumption underlying single-agent learning. In addition, agents in complex tasks may have limitations, such as physical constraints or designer-imposed approximations of the task that make learning tractable. Limitations prevent agents from acting optimally, which complicates the already challenging problem. A learning agent must effectively compensate for its own limitations while exploiting the limitations of the other agents. My thesis research focuses on these two challenges, namely multiagent learning and limitations, and includes four main contributions. First, the thesis introduces the novel concepts of a variable learning rate and the WoLF (Win or Learn Fast) principle to account for other learning agents. The WoLF principle is capable of making rational learning algorithms converge to optimal policies, and by doing so achieves two properties, rationality and convergence, which had not been achieved by previous techniques. The converging effect of WoLF is proven for a class of matrix games, and demonstrated empirically for a wide-range of stochastic games. Second, the thesis contributes an analysis of the effect of limitations on the game-theoretic concept of Nash equilibria. The existence of equilibria is important if multiagent learning techniques, which often depend on the concept, are to be applied to realistic problems where limitations are unavoidable. The thesis introduces a general model for the effect of limitations on agent behavior, which is used to analyze the resulting impact on equilibria. The thesis shows that equilibria do exist for a few restricted classes of games and limitations, but even well-behaved limitations do not preserve the existence of equilibria, in general. Third, the thesis introduces GraWoLF, a general-purpose, scalable, multiagent learning algorithm. GraWoLF combines policy gradient learning techniques with the WoLF variable learning rate. The effectiveness of the learning algorithm is demonstrated in both a card game with an intractably large state space, and an adversarial robot task. These two tasks are complex and agent limitations are prevalent in both. Fourth, the thesis describes the CMDragons robot soccer team strategy for adapting to an unknown opponent. The strategy uses a notion of plays as coordinated team plans. The selection of team plans is the decision point for adapting the team to its current opponent, based on the outcome of previously executed plays. The CMDragons were the first RoboCup robot team to employ online learning to autonomously alter its behavior during the course of a game. These four contributions demonstrate that it is possible to effectively learn to act in the presence of other learning agents in complex domains when agents may have limitations. The introduced learning techniques are proven effective in a class of small games, and demonstrated empirically across a wide range of settings that increase in complexity <br> - </details>

<details> <summary> <a href="http://l.academicdirect.org/Horticulture/GAs/Refs/PhD_Wiegand&Jong_2003.pdf"> An Analysis of Cooperative Coevolutionary Algorithms </a>by R. Paul Wiegand. Thesis, 2003. <a href="link">  </a> </summary> Coevolutionary algorithms behave in very complicated, often quite counterintuitive ways. Researchers and practitioners have yet to understand why this might be the case, how to change their intuition by understanding the algorithms better, and what to do about the differences. Unfortunately, there is little existing theory available to researchers to help address these issues. Further, little empirical analysis has been done at a component level to help understand intrinsic differences and similarities between coevolutionary algorithms and more traditional evolutionary algorithms. Finally, attempts to categorize coevolution and coevolutionary behaviors remain vague and poorly defined at best. The community needs directed investigations to help practitioners understand what particular coevolutionary algorithms are good at, what they are not, and why. This dissertation improves our understanding of coevolution by posing and answering the question: “Are cooperative coevolutionary algorithms (CCEAs) appropriate for static optimization tasks?” Two forms of this question are “How long do they take to reach the global optimum” and “How likely are they to get there?” The first form of the question is addressed by analyzing their performance as optimizers, both theoretically and empirically. This analysis includes investigations into the effects of coevolution-specific parameters on optimization performance in the context of particular properties of potential problem domains. The second leg of this dissertation considers the second form of the question by looking at the dynamical properties of these algorithms, analyzing their limiting behaviors again from theoretical and empirical points of view. Two common cooperative coevolutionary pathologies are explored and illustrated, in both formal and practical settings. The result is a better understanding of, and appreciation for, the fact that CCEAs are not generally appropriate for the task of static, single-objective optimization. In the end a new view of the CCEA is offered that includes analysis-guided suggestions for how a traditional CCEA might be modified to be better suited for optimization tasks, or might be applied to more appropriate tasks, given the nature of its dynamics. <br> - </details>

<details> <summary> <a href="https://aaltodoc.aalto.fi/bitstream/handle/123456789/2486/isbn9512273594.pdf?sequence=1&isAllowed=y"> Multiagent Reinforcement Learning in Markov Games: Asymmetric and Symmetric approaches </a>by Ville Kononen. PhD dissertation, 2004. <a href="link">  </a> </summary> Modern computing systems are distributed, large, and heterogeneous. Computers, other information processing devices and humans are very tightly connected with each other and therefore it would be preferable to handle these entities more as agents than stand-alone systems. One of the goals of artificial intelligence is to understand interactions between entities, whether they are artificial or natural, and to suggest how to make good decisions while taking other decision makers into account. In this thesis, these interactions between intelligent and rational agents are modeled with Markov games and the emphasis is on adaptation and learning in multiagent systems. Markov games are a general mathematical tool for modeling interactions between multiple agents. The model is very general, for example common board games are special instances of Markov games, and particularly interesting because it forms an intersection of two distinct research disciplines: machine learning and game theory. Markov games extend Markov decision processes, a well-known tool for modeling single-agent problems, to multiagent domains. On the other hand, Markov games can be seen as a dynamic extension to strategic form games, which are standard models in traditional game theory. From the computer science perspective, Markov games provide a flexible and efficient way to describe different social interactions between intelligent agents. This thesis studies different aspects of learning in Markov games. From the machine learning perspective, the focus is on a very general learning model, i.e. reinforcement learning, in which the goal is to maximize the long-time performance of the learning agent. The thesis introduces an asymmetric learning model that is computationally efficient in multiagent systems and enables the construction of different agent hierarchies. In multiagent reinforcement learning systems based on Markov games, the space and computational requirements grow very quickly with the number of learning agents and the size of the problem instance. Therefore, it is necessary to use function approximators, such as neural networks, to model agents in many real-world applications. In this thesis, various numeric learning methods are proposed for multiagent learning problems. The proposed methods are tested with small but non-trivial example problems from different research areas including artificial robot navigation, simplified soccer game, and automated pricing models for intelligent agents. The thesis also contains an extensive literature survey on multiagent reinforcement learning and various methods based on Markov games. Additionally, game-theoretic methods and methods originated from computer science for multiagent learning and decision making are compared. <br> - </details>

<br/>

### Traffic Control

<details> <summary> <a href="https://scholarship.rice.edu/bitstream/handle/1911/96455/TR96-259.pdf?sequence=1&isAllowed=y"> Ants and reinforcement learning: A case study in routing in dynamic networks </a>by D. Subramanian, P. Druschel, and J. Chen. In Proceedings of Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-97), pages 832–839, 1997. <a href="link">  </a> </summary> We investigate two new distributed routing algorithms for data networks based on simple biological" ants" that explore the network and rapidly learn good routes, using a novel variation of reinforcement learning. These two algorithms are fully adaptive to topology changes and changes in link costs in the network, and have space and computational overheads that are competitive with traditional packet routing algorithms: although they can generate more routing traffic when the rate of failures in a network is low, they perform much better under higher failure rates. Both algortihms are more resilient than traditional algorithms, in the sense that random corruption of routing has limited impact on the computation of paths. We present convergence theorems for both of our algorithms drawing up on the theory of non-stationary and stationary disctete-time Markov chains over the reals.  <br> - </details>

<details> <summary> <a href="http://nozdr.ru/data/media/biblio/kolxoz/Cs/CsLn/L/Learning%20and%20Adaption%20in%20Multi-Agent%20Systems,%201%20conf.,%20LAMAS%202005(LNCS3898,%20Springer,%202006)(ISBN%203540330534)(224s).pdf#page=137"> Multiagent Traffic Management: Opportunities for Multiagent Learning </a>by Kurt Dresner, Peter Stone. LAMAS, 2005. <a href="link">  </a> </summary> Traffic congestion is one of the leading causes of lost productivity and decreased standard of living in urban settings. In previous work published at AAMAS, we have proposed a novel reservation-based mechanism for increasing throughput and decreasing delays at intersections. In more recent work, we have provided a detailed protocol by which two different classes of agents (intersection managers and driver agents) can use this system. We believe that the domain created by this mechanism and protocol presents many opportunities for multiagent learning on the parts of both classes of agents. In this paper, we identify several of these opportunities and offer a first-cut approach to each <br> - </details>

<br/>

### Cognitive Science

<details> <summary> <a href="https://ia601006.us.archive.org/31/items/edwardo.wilson_sociobiology_the-new_synthesis/Edward%20O.%20Wilson%20-%20Sociobiology_%20The%20New%20Synthesis-Belknap%20Press%20of%20Harvard%20University%20Press%20%282000%29.pdf"> Sociobiology: The New Synthesis </a>by E. Wilson. Belknap Press, 1975. <a href="link">  </a> </summary> Sociobiology was brought together as a coherent discipline in Sociobiology: The New Synthesis (1975), the book now reprinted, but it was originally conceived in my earlier work The Insect Societies (1971) as a union between entomology and population biology. This first step was entirely logical, and in retrospect, inevitable. In the 1950s and 1960s studies of the social insects had multiplied and attained a new but still unorganized level. <br> - </details>

<details> <summary> <a href="http://sandip.ens.utulsa.edu/research/web/papers/Help1995Sekaran.pdf"> To help or not to help </a>by M. Sekaran and S. Sen. In Proceedings of the Seventeenth Annual Conference of the Cognitive Science Society, pages 736–741, Pittsburgh, PA, 1995. <a href="link">  </a> </summary> Any designer of intelligent agents in a multiagent system is faced with the choice of encoding a strategy of interaction with other agents. If the nature of other agents are known in advance, a suitable strategy may be chosen from the continuum between completely selfish behavior on one extreme and a philanthropic behavior on the other. In an open and dynamic system, however, it is unrealistic to assume that the nature of all other agents, possibly designed and used by users with very different goals and motivations, are known precisely. In the presence of this uncertainty, is it possible to build agents that adapt their behavior to interact appropriately with the particular group of agents in the current scenario? We address this question by borrowing on the simple yet powerful concept of reciprocal behavior. We propose a stochastic decision making scheme which promotes reciprocity among agents. Using a package delivery problem we show that reciprocal behavior can lead to system-wide cooperation, and hence close to optimal global performance can be achieved even though each individual agent chooses actions to benefit itself. More interestingly, we show that agents who do not help others perform worse in the long run when compared with reciprocal agents. Thus it is to the best interest of every individual agent to help other agents. <br> - </details>

<details> <summary> <a href="http://sandip.ens.utulsa.edu/research/web/papers/Milestones1997Sen.pdf"> Multiagent systems: Milestones and new horizons </a>by S. Sen. Trends in Cognitive Sciences, Elsevier, 1997. <a href="link">  </a> </summary> 
Research in multiagent systems (MAS), or Distributed AI dates back to late 70's. Initial work in the area focused on distributed interpretation of sensor data organizational structuring and generic negotiation protocols. But several recent developments have helped reshape the focus of the field. Like the rest of AI the field has matured from being largely exploratory in nature to focusing on formal theories of negotiation distributed reasoning multiagent learning and communication languages. The field is also maturing to the point of developing its first few fielded applications. The recent widespread interest in the internet the world-wide-web and intelligent agent applications have further fueled the need for techniques and mechanisms by which agents representing users can effectively interact with other agents in open dynamic environments. The development of several new international workshops and conferences have helped focus research in the area. The field is poised at a critical juncture with stimulating problems and challenges promising some very exciting developments in the next few year. <br> - </details>

<br/>

### Softwares and frameworks

<details> <summary> <a href="https://www.emse.fr/~boissier/enseignement/atelier00/jennings.pdf"> Applications of distributed artificial intelligence in industry. </a>by H. Van Dyke Parunak. In G. M. P. O’Hare and N. R. Jennings, editors, Foundations of Distributed AI. John Wiley & Sons, 1996. <a href="link">  </a> </summary> In many industrial applications, large centralized software systems are not as effective as distributed networks of relatively simpler computerized agents. For example, to compete effectively in today's markets, manufacturers must be able to design, implement, reconfigure, resize, and maintain manufacturing facilities rapidly and inexpensively. Because modern manufacturing depends heavily on computer systems, these same requirements apply to manufacturing control software, and are more easily satisfied by small modules than by large monolithic systems. This paper reviews industrial needs for Distributed Artificial Intelligence (DAI),1 giving special attention to systems for manufacturing scheduling and control. It describes a taxonomy of such systems, gives case studies of several advanced research applications and actual industrial installations, and identifies steps that need to be taken to deploy these technologies more broadly.<br> - </details>

<details> <summary> <a href="https://link.springer.com/content/pdf/10.1007/11780519_9.pdf"> Keepaway soccer: A machine learning testbed. </a>by P. Stone and R. Sutton. In A. Birk, S. Coradeschi, and S. Tadokoro, editors, RoboCup 2001: Robot Soccer World Cup V, volume 2377 of Lecture Notes in Computer Science, pages 214–223. Springer, 2002. <a href="link">  </a> </summary> Keepaway soccer has been previously put forth as a testbed for machine learning. Although multiple researchers have used it successfully for machine learning experiments, doing so has required a good deal of domain expertise. This paper introduces a set of programs, tools, and resources designed to make the domain easily usable for experimentation without any prior knowledge of RoboCup or the Soccer Server. In addition, we report on new experiments in the Keepaway domain, along with performance results designed to be directly comparable with future experimental results. Combined, the new infrastructure and our concrete demonstration of its use in comparative experiments elevate the domain to a machine learning benchmark, suitable for use by researchers across the field. <br> - </details>

</br>

<!-- BREAK -->

<!-- <details> <summary> <a href="link"> title </a>by authors. Conference, year. <a href="link">  </a> </summary> abstract <br> - </details> -->
